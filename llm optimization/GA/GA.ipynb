{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm on ANN which is trained on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_G8RiWpO6-6",
    "outputId": "f115c5a9-8539-492f-ce15-01f285c79df7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at generation 7\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Test set accuracy: 0.63\n",
      "Layer 1 weights:\n",
      "[[ 0.18058989  0.74589167  0.00303259  0.6131223   0.31756673 -0.12774694\n",
      "   0.6983913   0.89690661]\n",
      " [-0.02115007 -0.24883409  0.97533602  0.15656028  0.78910445  0.41115035\n",
      "  -0.14957299 -0.63884931]\n",
      " [ 0.13589046  0.08326843 -0.93210804 -0.48422463 -0.27961872  0.66986047\n",
      "   0.94211649  0.88853298]\n",
      " [-0.05157157 -0.45538535 -0.96321865  0.82859761  0.65783095 -0.92598473\n",
      "   0.19253976 -0.53998233]]\n",
      "\n",
      "Layer 2 weights:\n",
      "[-0.75886623 -0.8460936   0.39257755 -0.32025007  0.44953354 -0.86928732\n",
      " -0.36941932  0.07898258]\n",
      "\n",
      "Layer 3 weights:\n",
      "[[ 0.92529683 -0.36249499  0.25178275  0.7719555 ]\n",
      " [ 0.23172638 -0.53408105 -0.95119844  0.09845333]\n",
      " [-0.95746118  0.74940335 -0.83440266  0.8781354 ]\n",
      " [ 0.59756647 -0.22140877 -0.42261253  0.53437658]\n",
      " [-0.19613817 -0.04024876  0.25501093 -0.97121302]\n",
      " [ 0.96816694  0.53654683 -0.9185424  -0.57011924]\n",
      " [ 0.4751646  -0.47547191 -0.77905177 -0.89714837]\n",
      " [-0.42552202 -0.40738376 -0.5327845  -0.91581362]]\n",
      "\n",
      "Layer 4 weights:\n",
      "[-0.96425213 -0.84972444  0.45637751 -0.23134671]\n",
      "\n",
      "Layer 5 weights:\n",
      "[[ 0.35929457 -0.56349222 -0.50719593]\n",
      " [ 0.31738726  0.59883176 -0.85886251]\n",
      " [ 0.75823662  0.88946404  0.17155116]\n",
      " [ 0.88046048 -0.66593211  0.98233725]]\n",
      "\n",
      "Layer 6 weights:\n",
      "[0.57455638 0.88546355 0.2992933 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#iris - ann\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=4, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20  # Increased population size\n",
    "generations = 20  # Increased number of generations\n",
    "mutation_rate = 0.05  # Adjusted mutation rate\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to evaluate fitness (loss)\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Print the final weights of the best individual\n",
    "for i, weight_matrix in enumerate(best_weights):\n",
    "    print(f\"Layer {i+1} weights:\\n{weight_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm on CNN which is trained on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLXVShFGQlfU",
    "outputId": "3598013d-4e81-4145-b899-50d130687847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Test set accuracy: 0.56\n",
      "Layer 1 weights:\n",
      "[[ 0.71281658  0.18097507  0.2522776  ...  0.45739216 -0.35118316\n",
      "   0.42563227]\n",
      " [ 0.26282334  0.03703197  0.96925217 ...  0.35118023 -0.06573472\n",
      "   0.20589389]\n",
      " [-0.94525599 -0.42875561  0.6972702  ... -0.74727591 -0.7206741\n",
      "   0.99939235]\n",
      " ...\n",
      " [-0.69930307  0.32976026 -0.68157142 ... -0.37805061 -0.65816606\n",
      "   0.65012886]\n",
      " [-0.6011073   0.38252289 -0.4586843  ...  0.14753509 -0.09051445\n",
      "   0.50697645]\n",
      " [-0.42316076  0.48934615  0.12527095 ...  0.20445338 -0.39238096\n",
      "  -0.34842802]]\n",
      "\n",
      "Layer 2 weights:\n",
      "[[ 0.66740539 -0.7831877   0.78245429  0.28469872 -1.52049908  1.41778455\n",
      "  -0.09413383  0.10187639 -0.40188577  0.77950599  0.82991201  0.7563814\n",
      "   1.19717939  0.97378531 -0.3419992  -0.38669786]\n",
      " [-0.65905467  0.85690143 -0.39196833  0.27590061  0.56434862 -0.34818243\n",
      "   0.88772949  0.29988508 -0.01952688  0.79581645  0.57616341 -0.82426832\n",
      "  -0.62778351  0.50561405 -1.01568092 -1.24375802]\n",
      " [-0.33163507 -0.43846725 -1.15576013 -1.14878481  0.69628479  0.00533457\n",
      "  -0.29913495  0.64470071  0.00968154  0.89817378  0.7865871  -0.52326802\n",
      "   0.31736186 -0.10330096  0.71121234 -0.1912751 ]\n",
      " [ 0.12581364  0.15351193  0.35833989 -0.98809132  0.57394341  0.66627152\n",
      "  -1.03113181 -0.78967925 -0.74451669  0.0845706   0.11313353  1.33352035\n",
      "  -0.66897965  0.18518915  0.48061793 -0.51819208]\n",
      " [ 0.6843648  -0.04597116 -0.63517836 -0.8991097  -0.22329921 -0.22550918\n",
      "  -0.30525512  0.53642023  0.06430537  0.84759967  0.91525378 -0.05227193\n",
      "  -0.26765775 -0.87435275  0.29614886 -0.93881706]\n",
      " [-0.24365118 -1.43722967  0.62417503 -0.05004582 -0.04722158 -0.02253639\n",
      "   0.08325185 -0.89609438 -0.64715347 -0.81713959 -0.43427278 -0.81229957\n",
      "   0.90261642  0.35726794 -0.15804909  0.20464013]\n",
      " [-0.75416702  0.01982701 -0.31971797  0.663157   -0.3070156  -0.87900588\n",
      "  -0.30796086  0.76615611 -0.83120512 -0.15553733 -0.70050826  0.69110881\n",
      "  -0.55812954  0.16749354  0.15806318 -0.62171075]\n",
      " [-0.40578758  0.55197378 -0.7724255  -0.4319439  -0.30095825 -0.9096287\n",
      "   0.23569137  0.15727234  0.11713659 -1.41161203 -0.7159439  -0.86070751\n",
      "   0.15796544 -1.17574806 -0.34182062 -0.88239551]\n",
      " [ 0.65373108  0.84693229 -0.27910229 -0.00348418  0.34401643 -0.29098038\n",
      "   0.10215158 -0.22833196 -0.47881869 -0.90173123  0.22226244  0.24014798\n",
      "  -0.40676429  0.6246532   0.08692512  0.27390863]\n",
      " [-0.31635178  0.22439412  0.08631859 -0.35335613 -0.52115152 -0.26084607\n",
      "   0.03892806  0.31236443 -0.0770616   0.44825206  0.54756021  0.32887562\n",
      "  -0.49145941 -0.18237442  0.69563178  0.21108231]\n",
      " [ 0.25199237  0.52573033 -0.04721804 -1.42615194  0.15320314  0.47991184\n",
      "   0.30705885  0.99462248  0.12928576  0.13884221  0.04704521  0.01070765\n",
      "   0.59103727  1.80560279 -0.20083626  0.9587404 ]\n",
      " [ 0.39921882  0.19398389 -1.03656767 -0.03953729  0.85615912 -0.41690154\n",
      "   1.33409169 -0.16949865 -0.5957291   0.32824226 -0.35613816  1.40132358\n",
      "  -0.56445552  0.09255023  0.64133292 -0.84648326]\n",
      " [-0.56355092  1.12251895 -0.7012081  -0.13783085 -0.73472558 -1.00890196\n",
      "   0.45889983 -0.84720417 -0.06546768  0.340926    0.92008446  0.53093313\n",
      "  -0.57712405 -0.18837846  0.05116472  1.18851552]\n",
      " [ 0.12414805  0.97424933  1.00950894  0.39632296  0.62865591  0.27507406\n",
      "  -0.61838813  0.3805457  -0.03846397  0.07644817 -0.73775195  0.21477794\n",
      "   0.15509578 -0.87440485 -0.80169024  0.89986918]\n",
      " [ 0.9759237  -0.14101261  0.38104126 -0.96746314 -0.89014146 -0.76230947\n",
      "  -0.56375947  0.57671546 -1.51527571 -0.43433853  0.24982395  0.62499637\n",
      "  -0.80793669  0.94223433  0.64967227  0.09628348]\n",
      " [-0.11434529  0.00472073 -0.16856244  0.42183032  0.28064002 -0.91314149\n",
      "   0.35177564  0.65926066  0.58392196 -1.26103188  0.13447512  0.64231726\n",
      "  -0.02433773  0.30124381  0.52264543 -0.5768476 ]\n",
      " [ 0.32998675 -0.76025981 -0.49854399 -0.02642215 -0.62447995  0.11301059\n",
      "   0.88243475 -0.07088157  0.45765284 -0.15944183  0.86345726 -0.3046493\n",
      "  -0.03006744  1.12687417 -0.09596522  0.31707766]\n",
      " [-0.59143896  0.40504617 -1.10374968  0.76918629 -0.18759396  0.97026511\n",
      "   0.58938001  0.98393853  0.16750719  0.70675909 -0.33005622 -0.90603837\n",
      "  -1.22580781  1.10747038 -0.66465657 -0.39976051]\n",
      " [-0.01040251  0.27202028 -0.40954588  0.60290445 -0.25625918 -1.00353187\n",
      "   0.91419039 -0.34416707  0.48424919  0.48280098  0.87206513 -0.24999528\n",
      "   0.19856457 -0.25360516  0.11242887  0.7102182 ]\n",
      " [ 0.2223858  -0.24187238 -0.26340663  0.79988823 -0.19795079  0.41496501\n",
      "  -0.40794587 -0.08290758 -0.71163591  0.99323498 -0.65372884  1.11299849\n",
      "  -0.12893785 -0.3522041  -0.3341377  -0.68071332]\n",
      " [ 0.47712398  0.29636422 -0.43383354 -1.07947259  1.31180867 -0.06985328\n",
      "   0.77959708 -1.17485645  0.31562836  0.5868654   0.38470241 -1.04443517\n",
      "   0.55018658  1.09792017  1.09454314 -0.20611926]\n",
      " [ 0.47056899  0.06652351  0.56043349  0.60221217 -0.83567987  0.27848202\n",
      "   0.65818823 -0.30561571  0.73428179  0.41098375  0.05849625 -0.93101261\n",
      "  -0.2576653   0.20974393  0.39328049  0.68830206]\n",
      " [ 0.70656367 -0.02649526 -0.34907448  0.16600253 -0.23072922 -0.80315316\n",
      "   0.43192863  0.06408073  0.93442134  0.00886595 -0.99369809 -0.58999855\n",
      "  -0.83881322  0.6566659   0.07176951  0.28319822]\n",
      " [ 0.97812572 -1.07447581  0.65773082 -1.29434437 -0.80451991 -0.89846795\n",
      "  -1.2761845   0.19509398  0.14121949 -0.67195251 -0.61399221  0.2937777\n",
      "  -0.2963237  -0.54696708 -0.8014178   0.76012455]\n",
      " [-1.28309811 -0.09481588 -0.53698701 -0.80681529 -1.02572963  0.73827382\n",
      "  -0.37225826  0.95582555  0.67055316 -0.97334946 -0.65087969  1.02685489\n",
      "   0.10440487  0.0046717  -0.71227832  0.00757388]\n",
      " [-0.59033027  0.04652773 -0.16032408  0.09491536 -0.53703153  0.90658821\n",
      "  -0.21879257  0.42679874  0.49656024 -0.56526769 -0.73839584 -0.6904741\n",
      "  -0.36436963 -0.2060253   1.2500845  -0.15377986]\n",
      " [ 1.0775723  -0.33698581  0.80527929  0.61712267  0.8093483  -0.93854753\n",
      "  -0.4761596   0.32483836  0.04599734  1.25021631  0.62032928 -0.45356564\n",
      "  -1.37374239  0.78258893 -0.7697107   0.59848673]\n",
      " [-0.06908391 -0.56654352  0.0595467   0.76107578 -0.6115306  -0.19685123\n",
      "  -0.01917078  0.19868091  0.94164432  0.14709685  0.26779662 -0.73434639\n",
      "  -0.08257379 -0.48683706 -0.62085277  0.36060486]\n",
      " [-0.8186653  -0.30133739  0.42230528 -0.49999782 -0.31800524 -0.09266369\n",
      "  -0.4268498  -0.71071433  0.15133334  0.03873234  0.41604042 -0.74588888\n",
      "  -0.23086163  0.22846238 -0.9922744   0.06402619]\n",
      " [ 0.18886788  0.32520039 -0.91490767 -0.74675883  0.40604686  0.40664356\n",
      "  -0.47823201 -0.37359492 -0.66054784  0.07707862  0.54115685 -0.95593963\n",
      "  -0.56444686 -0.0963354  -0.26548622 -0.71202127]\n",
      " [-1.04545282  0.89490672  1.0909466  -0.14011472 -0.78519439  0.45437483\n",
      "   0.1914512  -0.10063205  0.14569886 -0.11530713  0.62803797 -0.7225459\n",
      "  -0.23696428  0.22619372  0.4285962   0.89591134]\n",
      " [-0.35947759 -0.54094104  0.69998096  0.77067708 -0.42320981  0.81063517\n",
      "   0.05330331 -0.75152511  0.67615842  0.19495968 -0.18642619 -0.51427842\n",
      "  -0.88277274  0.15709778 -0.63203417 -0.68209333]]\n",
      "\n",
      "Layer 3 weights:\n",
      "[ 0.22318174 -0.41046587  0.27652346 -0.24285331 -0.29866165 -0.67706045\n",
      "  0.00291499  0.12637304 -0.82118098  0.43598903  0.21510021  0.82790238\n",
      " -0.76045025 -0.05160104 -0.22611783 -0.23081794]\n",
      "\n",
      "Layer 4 weights:\n",
      "[[ 0.22468064]\n",
      " [-0.44417316]\n",
      " [-0.1740895 ]\n",
      " [-0.10164554]\n",
      " [ 0.69086173]\n",
      " [ 1.63937517]\n",
      " [-0.50849041]\n",
      " [ 0.61448694]\n",
      " [-0.11453211]\n",
      " [-0.32041439]\n",
      " [ 0.43084775]\n",
      " [ 0.12037521]\n",
      " [-0.18691686]\n",
      " [ 0.06052397]\n",
      " [ 0.66923777]\n",
      " [ 0.02760897]]\n",
      "\n",
      "Layer 5 weights:\n",
      "[-0.20243005]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 200  # Cut reviews after 200 words\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Further split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an ANN model for IMDB\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 32, input_length=maxlen),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize model weights by running a single forward pass\n",
    "model.predict(X_train[:1])  # Pass a single sample to initialize the model's weights\n",
    "\n",
    "# Genetic Algorithm parameters\n",
    "population_size = 20  # Number of individuals in population\n",
    "generations = 20  # Number of generations\n",
    "mutation_rate = 0.05  # Mutation rate\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to evaluate fitness (loss)\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_accuracy = np.mean((test_predictions > 0.5).astype('int') == y_test.reshape(-1, 1))\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Print the final weights of the best individual\n",
    "for i, weight_matrix in enumerate(best_weights):\n",
    "    print(f\"Layer {i+1} weights:\\n{weight_matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svzKFbbuGKo8",
    "outputId": "be2b6f7d-5f1e-47a0-9b15-ac45004c0e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Best Fitness: 0.0\n",
      "Iteration 2/10, Best Fitness: 0.0\n",
      "Iteration 3/10, Best Fitness: 0.0\n",
      "Iteration 4/10, Best Fitness: 0.0\n",
      "Iteration 5/10, Best Fitness: 0.0\n",
      "Iteration 6/10, Best Fitness: 0.0\n",
      "Iteration 7/10, Best Fitness: 0.0\n",
      "Iteration 8/10, Best Fitness: 0.0\n",
      "Iteration 9/10, Best Fitness: 0.0\n",
      "Iteration 10/10, Best Fitness: 0.0\n",
      "Best hyperparameters found: [5.16831170e+01 3.94161812e-03]\n",
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Final model accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load and preprocess the IMDB dataset\n",
    "# dataset = load_dataset('imdb')\n",
    "\n",
    "# # Preprocessing and feature extraction\n",
    "# train_texts = dataset['train']['text']\n",
    "# train_labels = dataset['train']['label']\n",
    "# test_texts = dataset['test']['text']\n",
    "# test_labels = dataset['test']['label']\n",
    "\n",
    "# # Split train into train and validation sets\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Vectorize the text data\n",
    "# vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "# X_train = vectorizer.fit_transform(train_texts).toarray()\n",
    "# X_val = vectorizer.transform(val_texts).toarray()\n",
    "# X_test = vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "# y_val_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "# y_test_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# class IMDBDataset(Dataset):\n",
    "#     def __init__(self, texts, labels):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'inputs': self.texts[idx],\n",
    "#             'labels': self.labels[idx]\n",
    "#         }\n",
    "\n",
    "# # Prepare datasets and dataloaders\n",
    "# train_dataset = IMDBDataset(X_train_tensor, y_train_tensor)\n",
    "# val_dataset = IMDBDataset(X_val_tensor, y_val_tensor)\n",
    "# test_dataset = IMDBDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# # Define the ANN model\n",
    "# class ANNModel(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_units, output_dim, weights=None):\n",
    "#         super(ANNModel, self).__init__()\n",
    "#         self.fc1 = torch.nn.Linear(input_dim, hidden_units)\n",
    "#         self.fc2 = torch.nn.Linear(hidden_units, hidden_units)\n",
    "#         self.fc3 = torch.nn.Linear(hidden_units, output_dim)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "\n",
    "#         # Assign the custom weights to the first layer if provided\n",
    "#         if weights is not None:\n",
    "#             with torch.no_grad():\n",
    "#                 self.fc1.weight.copy_(weights)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# def create_model(hidden_units, learning_rate, weights=None):\n",
    "#     input_dim = X_train.shape[1]  # Number of features\n",
    "#     output_dim = 2  # Binary classification\n",
    "#     model = ANNModel(input_dim, int(hidden_units), output_dim, weights)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     return model, optimizer\n",
    "\n",
    "# # Objective Function: Minimize Validation Loss\n",
    "# def objective_function(hyperparams):\n",
    "#     hidden_units = int(hyperparams[0])\n",
    "#     learning_rate = hyperparams[1]\n",
    "\n",
    "#     # Initialize weights with correct dimensions: (hidden_units, input_dim)\n",
    "#     weights = torch.randn((hidden_units, X_train.shape[1]), dtype=torch.float32)\n",
    "\n",
    "#     model, optimizer = create_model(hidden_units, learning_rate, weights)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Train the model for one epoch\n",
    "#     model.train()\n",
    "#     for batch in train_loader:\n",
    "#         inputs = batch['inputs']\n",
    "#         labels = batch['labels']\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Evaluate on validation set\n",
    "#     model.eval()\n",
    "#     preds = []\n",
    "#     labels_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             inputs = batch['inputs']\n",
    "#             labels = batch['labels']\n",
    "#             outputs = model(inputs)\n",
    "#             preds += torch.argmax(outputs, dim=1).tolist()\n",
    "#             labels_list += labels.tolist()\n",
    "\n",
    "#     accuracy = accuracy_score(labels_list, preds)\n",
    "#     return 1 - accuracy  # Minimize this value\n",
    "\n",
    "# # Bat Algorithm Implementation\n",
    "# class BatAlgorithm:\n",
    "#     def __init__(self, num_bats, max_iter, bounds, alpha=0.9, gamma=0.9):\n",
    "#         self.num_bats = num_bats\n",
    "#         self.max_iter = max_iter\n",
    "#         self.bounds = bounds\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#         # Initialize bats\n",
    "#         self.bats = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_bats, bounds.shape[0]))\n",
    "#         self.velocities = np.zeros((num_bats, bounds.shape[0]))\n",
    "#         self.fitness = np.zeros(num_bats)\n",
    "#         self.frequencies = np.zeros(num_bats)\n",
    "\n",
    "#         # Best bat initialization\n",
    "#         self.best_bat = self.bats[0]\n",
    "#         self.best_fitness = float(\"inf\")\n",
    "\n",
    "#     def optimize(self, objective_function):\n",
    "#         for iter in range(self.max_iter):\n",
    "#             for i in range(self.num_bats):\n",
    "#                 # Frequency update\n",
    "#                 self.frequencies[i] = np.random.uniform(0, 1)\n",
    "#                 # Velocity update\n",
    "#                 self.velocities[i] = self.velocities[i] + (self.bats[i] - self.best_bat) * self.frequencies[i]\n",
    "#                 # Position update\n",
    "#                 candidate_bat = self.bats[i] + self.velocities[i]\n",
    "#                 candidate_bat = np.clip(candidate_bat, self.bounds[:, 0], self.bounds[:, 1])\n",
    "\n",
    "#                 # Evaluate candidate\n",
    "#                 fitness = objective_function(candidate_bat)\n",
    "\n",
    "#                 # Update if new solution is better\n",
    "#                 if fitness < self.fitness[i]:\n",
    "#                     self.bats[i] = candidate_bat\n",
    "#                     self.fitness[i] = fitness\n",
    "\n",
    "#                 # Update the best bat\n",
    "#                 if self.fitness[i] < self.best_fitness:\n",
    "#                     self.best_bat = self.bats[i]\n",
    "#                     self.best_fitness = self.fitness[i]\n",
    "\n",
    "#             print(f\"Iteration {iter+1}/{self.max_iter}, Best Fitness: {self.best_fitness}\")\n",
    "\n",
    "#         return self.best_bat\n",
    "\n",
    "# # Hyperparameter bounds: hidden units (4 to 64), learning rate (1e-6 to 1e-2)\n",
    "# bounds = np.array([[4, 64], [1e-6, 1e-2]])\n",
    "\n",
    "# # Initialize and run Bat Algorithm\n",
    "# bat_algorithm = BatAlgorithm(num_bats=10, max_iter=10, bounds=bounds, alpha=0.9, gamma=0.9)\n",
    "# best_hyperparams = bat_algorithm.optimize(objective_function)\n",
    "# print(\"Best hyperparameters found:\", best_hyperparams)\n",
    "\n",
    "# # Extract the best hyperparameters\n",
    "# best_hidden_units = int(best_hyperparams[0])\n",
    "# best_lr = best_hyperparams[1]\n",
    "\n",
    "# # Initialize final model with the best hyperparameters\n",
    "# # Initialize weights with correct dimensions: (best_hidden_units, X_train.shape[1])\n",
    "# final_weights = torch.randn((best_hidden_units, X_train.shape[1]), dtype=torch.float32)\n",
    "# final_model, final_optimizer = create_model(best_hidden_units, best_lr, final_weights)\n",
    "\n",
    "# # Train the final model\n",
    "# num_epochs = 3  # You can adjust this based on your needs\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     final_model.train()\n",
    "#     for batch in train_loader:\n",
    "#         inputs = batch['inputs']\n",
    "#         labels = batch['labels']\n",
    "#         final_optimizer.zero_grad()\n",
    "#         outputs = final_model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         final_optimizer.step()\n",
    "#     print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "# # Evaluate the final model on the test set\n",
    "# final_model.eval()\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "# preds = []\n",
    "# labels_list = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         inputs = batch['inputs']\n",
    "#         labels = batch['labels']\n",
    "#         outputs = final_model(inputs)\n",
    "#         preds += torch.argmax(outputs, dim=1).tolist()\n",
    "#         labels_list += labels.tolist()\n",
    "\n",
    "# # Calculate and print accuracy\n",
    "# accuracy = accuracy_score(labels_list, preds)\n",
    "# print(f\"Final model accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm on CNN with dropout layer which is trained on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFKxULlgHMPH",
    "outputId": "1412d8ec-7d23-4a83-f639-40208105d46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "Early stopping at generation 11\n",
      "782/782 [==============================] - 2s 2ms/step\n",
      "Test set accuracy: 0.51\n",
      "Layer 1 weights:\n",
      "[[-0.35988579  0.84897452 -0.40005202 ... -0.2844398  -0.14436807\n",
      "   0.4301523 ]\n",
      " [-0.6232936   0.46431354 -0.42763575 ...  0.39351808  0.4156008\n",
      "  -0.46199049]\n",
      " [-0.63222245 -0.66246607 -0.10488483 ... -0.5961557  -0.44536021\n",
      "   0.54922633]\n",
      " ...\n",
      " [ 0.63144705  0.43628677 -0.35724277 ... -0.29339546 -0.20199538\n",
      "  -0.62600855]\n",
      " [ 0.32735023 -0.10112649  0.99901866 ... -0.63706175 -0.7228877\n",
      "  -0.52927442]\n",
      " [-0.73613596 -0.84538998  0.76163167 ... -0.51312007 -0.06823992\n",
      "  -0.72200804]]\n",
      "\n",
      "Layer 2 weights:\n",
      "[[[-0.60753341  0.3178871  -1.21406276 ...  0.58428471  1.0777442\n",
      "    0.92880067]\n",
      "  [ 0.25166232 -0.01317954 -0.3102024  ... -0.4782268   0.81053919\n",
      "   -0.3312514 ]\n",
      "  [-0.15808124 -0.93199049  0.52287361 ... -0.64348712  0.52927495\n",
      "    0.60249956]\n",
      "  ...\n",
      "  [-0.62032117 -0.59000462  0.88400106 ... -0.54777161  0.57009392\n",
      "    0.81348303]\n",
      "  [-0.29743695 -0.7190498  -1.10757741 ... -0.16554216 -0.39275552\n",
      "    0.35136036]\n",
      "  [-0.25628367 -0.40863255  1.53978332 ... -0.85672823 -0.390061\n",
      "   -1.10581122]]\n",
      "\n",
      " [[ 0.2096762  -0.37111085  0.3818586  ...  0.38131718  1.19560869\n",
      "   -0.97231743]\n",
      "  [-0.32424463  0.56160598 -0.32361697 ... -0.70109044 -0.13365668\n",
      "   -0.19262529]\n",
      "  [ 0.9361929  -1.01810729 -0.75453323 ... -0.99265471  0.70139734\n",
      "    0.20163159]\n",
      "  ...\n",
      "  [-0.12637708  0.95336991  0.59158225 ... -0.39500776 -0.99155844\n",
      "    0.07018646]\n",
      "  [-1.39561514  0.23078045 -0.81859059 ... -0.31947446  0.0246221\n",
      "   -0.01047841]\n",
      "  [ 0.61910344  0.84546411 -0.74710384 ... -0.39180644  0.68154813\n",
      "   -0.16532991]]\n",
      "\n",
      " [[-0.36486674  0.44284716  0.41445276 ...  0.64841801 -0.01756328\n",
      "    0.72961441]\n",
      "  [-0.77211694  0.44155029  0.15587387 ...  0.3636328   0.49424418\n",
      "    0.42927145]\n",
      "  [ 0.47544419  0.33853419  0.70290861 ... -0.06729059  1.01927308\n",
      "   -0.54506115]\n",
      "  ...\n",
      "  [-0.32746008 -0.75810861  0.55836973 ...  0.99036105  0.49072499\n",
      "   -0.16809671]\n",
      "  [-0.79863865  0.56460085 -0.09810171 ...  0.26383175  0.09498054\n",
      "    0.56694442]\n",
      "  [ 1.18346787  0.67576264  0.39557907 ... -0.11829424  0.60081539\n",
      "    0.17310119]]\n",
      "\n",
      " [[-0.40849221  0.09566931 -0.99526234 ... -1.13153658 -0.18607414\n",
      "    1.044868  ]\n",
      "  [-1.40069777 -0.34188201  0.18686877 ...  0.09096995  0.32551357\n",
      "    0.72940037]\n",
      "  [-0.31424935 -0.71571376  0.54067564 ...  0.8345786  -0.67263553\n",
      "   -0.42164432]\n",
      "  ...\n",
      "  [ 0.62496131  0.24106109  0.85285059 ... -0.01558105  0.99386196\n",
      "    0.08064978]\n",
      "  [-1.02705177  0.57717778  0.21350034 ...  0.13070796 -0.04103175\n",
      "    0.27965587]\n",
      "  [ 0.0179045  -0.92170574  0.09373789 ... -0.38479369  0.56332927\n",
      "    0.50240884]]\n",
      "\n",
      " [[ 1.3629591   0.34293305 -0.35259844 ...  0.35710177 -0.43733129\n",
      "    0.12516053]\n",
      "  [-0.4650547   1.08265826  0.59326735 ...  0.05913793 -0.75079954\n",
      "   -0.96044643]\n",
      "  [-0.2651969  -0.40337594 -0.65083842 ... -0.30401599  0.11624855\n",
      "   -0.55609255]\n",
      "  ...\n",
      "  [-0.54111183  0.35649903 -0.0499447  ...  0.850983   -0.77564074\n",
      "   -0.04038083]\n",
      "  [-0.08341958  0.36095891  0.62955089 ...  0.18753562  0.31130465\n",
      "    0.75764189]\n",
      "  [ 0.6040372  -0.61559376  0.93276168 ...  1.03044777 -0.47686221\n",
      "   -0.18301799]]]\n",
      "\n",
      "Layer 3 weights:\n",
      "[-0.20123533  0.55154469  0.44219905 -0.00962014 -0.54270178  0.96674959\n",
      "  0.36372922 -0.60880737  0.80255346 -0.38053291  1.22989916  0.82683018\n",
      "  0.33722391 -0.9677394  -1.18850415 -0.44109114 -0.13805663 -1.18491166\n",
      "  0.00679385 -0.35050805 -0.70534755  0.50529433  0.95171698 -1.10682417\n",
      "  1.10298684  0.45740061 -0.71374992  0.0864526  -0.55891448  1.20235945\n",
      " -0.48438425  0.97013358 -0.19374839  0.49871092 -0.28474318  0.94132378\n",
      " -0.39620284 -0.16628186  0.51093754  0.77160723  0.94097752 -0.10847386\n",
      " -0.65562077 -0.14362612 -0.99606761 -0.59339518 -0.51823204  0.81052164\n",
      " -0.13130777 -0.99669331 -0.21183057 -0.516922    1.33962057  0.00175055\n",
      "  1.25921064  0.79446312 -0.54360765  0.54532131  0.30113745  0.6496132\n",
      " -0.11671599  0.02359123  1.07576695 -0.85111582]\n",
      "\n",
      "Layer 4 weights:\n",
      "[[-0.24306492 -0.92976045 -0.76238495 ... -0.8778875   0.23106923\n",
      "   0.13173623]\n",
      " [ 0.96829425  0.46243698 -0.5729948  ... -0.51218626  0.13693218\n",
      "  -0.74528284]\n",
      " [ 0.38854859  0.42495868  0.2073478  ... -0.85730561  0.67038008\n",
      "   0.88602228]\n",
      " ...\n",
      " [-0.42461862  1.13155194  0.05987753 ...  0.21905688  0.83489373\n",
      "   0.56718001]\n",
      " [-0.9993383  -0.96202642  0.67308036 ... -0.79622938  0.32706799\n",
      "   0.6618917 ]\n",
      " [-0.20230276  0.18331518  0.11150413 ...  0.93576207 -0.57133129\n",
      "   0.59799372]]\n",
      "\n",
      "Layer 5 weights:\n",
      "[ 0.47946703  1.03145274 -0.66165064 -0.56142115  0.57882132  0.90450924\n",
      " -0.47266427  0.94241237  0.53766063  0.7238152  -0.17237382  0.04130761\n",
      " -0.01325936  0.63711345  0.37242756  1.16732321 -0.17815741  0.57598173\n",
      " -0.50655345  0.81558984 -0.19708066 -0.69958855 -0.80326063 -1.31456118\n",
      " -0.53541435 -0.68211975  1.23065487  0.09492547  0.63699807 -0.36712296\n",
      "  0.29509969 -0.45931781  0.73893097 -0.42372592  0.16542519 -0.66151391\n",
      " -0.90425781  0.94725404 -0.26963947  0.39167485  0.6956169   0.29819124\n",
      "  0.98117638 -0.54797955 -0.48665066  0.65735525  0.80207156 -0.22603324\n",
      "  0.76561077 -0.37233753  0.86481875 -1.08976333 -0.02383078  0.76800894\n",
      "  0.7021906  -0.90009579  0.24849119 -0.86742498  0.54691261 -0.87693598\n",
      "  0.37788685  0.21375937  1.12900449  0.50994461]\n",
      "\n",
      "Layer 6 weights:\n",
      "[[ 0.56687939 -0.68725181 -0.85347642 ...  0.31708471  0.79690579\n",
      "   0.36387981]\n",
      " [-0.46487816  0.71167656  0.43737773 ...  0.63750953  0.54437028\n",
      "  -0.10859869]\n",
      " [-0.50551742  0.95274332 -0.1771515  ...  0.95627285 -0.29812545\n",
      "  -0.99199868]\n",
      " ...\n",
      " [ 0.07007273  0.11135606  0.68193402 ... -0.91024126  0.06957785\n",
      "   0.27365149]\n",
      " [-0.31750654 -0.22689968 -0.5752666  ... -0.799921    0.83964365\n",
      "   0.991629  ]\n",
      " [-0.76082404 -0.51727625 -0.29138738 ...  0.16385935 -1.1993348\n",
      "   0.51226294]]\n",
      "\n",
      "Layer 7 weights:\n",
      "[-0.00473937 -0.277154    0.40998464  0.25351955  0.3775892  -0.11437214\n",
      "  0.94291901  0.30514476  0.58105811  0.47606874  0.90106371  0.94070063\n",
      " -0.0532385  -0.12968292 -0.62645419 -0.33095027]\n",
      "\n",
      "Layer 8 weights:\n",
      "[[ 0.21034963]\n",
      " [ 0.73165489]\n",
      " [-0.11597266]\n",
      " [ 0.36069697]\n",
      " [-0.26936936]\n",
      " [ 0.1030951 ]\n",
      " [-0.86311056]\n",
      " [ 0.57434973]\n",
      " [-0.52864561]\n",
      " [ 0.7035638 ]\n",
      " [ 0.24558066]\n",
      " [-0.66639353]\n",
      " [-0.6002849 ]\n",
      " [-0.73834284]\n",
      " [ 0.13794959]\n",
      " [ 0.36702812]]\n",
      "\n",
      "Layer 9 weights:\n",
      "[0.74013375]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 200  # Cut reviews after 200 words\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Further split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an enhanced ANN model for IMDB\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 32, input_length=maxlen),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=4),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize model weights by running a single forward pass\n",
    "model.predict(X_train[:1])  # Pass a single sample to initialize the model's weights\n",
    "\n",
    "# Genetic Algorithm parameters\n",
    "population_size = 20  # Number of individuals in population\n",
    "generations = 20  # Number of generations\n",
    "mutation_rate = 0.05  # Mutation rate\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to evaluate fitness (loss)\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_accuracy = np.mean((test_predictions > 0.5).astype('int') == y_test.reshape(-1, 1))\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Print the final weights of the best individual\n",
    "for i, weight_matrix in enumerate(best_weights):\n",
    "    print(f\"Layer {i+1} weights:\\n{weight_matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xicmGiRJY3gK",
    "outputId": "f19de42a-64cd-4033-d225-3b3b9e31b2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 144ms/step\n",
      "Early stopping at generation 11\n",
      "782/782 [==============================] - 2s 2ms/step\n",
      "Test set accuracy: 0.51\n",
      "Layer 1 weights:\n",
      "[[-0.35988579  0.84897452 -0.40005202 ... -0.2844398  -0.14436807\n",
      "   0.4301523 ]\n",
      " [-0.6232936   0.46431354 -0.42763575 ...  0.39351808  0.4156008\n",
      "  -0.46199049]\n",
      " [-0.63222245 -0.66246607 -0.10488483 ... -0.5961557  -0.44536021\n",
      "   0.54922633]\n",
      " ...\n",
      " [ 0.63144705  0.43628677 -0.35724277 ... -0.29339546 -0.20199538\n",
      "  -0.62600855]\n",
      " [ 0.32735023 -0.10112649  0.99901866 ... -0.63706175 -0.7228877\n",
      "  -0.52927442]\n",
      " [-0.73613596 -0.84538998  0.76163167 ... -0.51312007 -0.06823992\n",
      "  -0.72200804]]\n",
      "\n",
      "Layer 2 weights:\n",
      "[[[-0.60753341  0.3178871  -1.21406276 ...  0.58428471  1.0777442\n",
      "    0.92880067]\n",
      "  [ 0.25166232 -0.01317954 -0.3102024  ... -0.4782268   0.81053919\n",
      "   -0.3312514 ]\n",
      "  [-0.15808124 -0.93199049  0.52287361 ... -0.64348712  0.52927495\n",
      "    0.60249956]\n",
      "  ...\n",
      "  [-0.62032117 -0.59000462  0.88400106 ... -0.54777161  0.57009392\n",
      "    0.81348303]\n",
      "  [-0.29743695 -0.7190498  -1.10757741 ... -0.16554216 -0.39275552\n",
      "    0.35136036]\n",
      "  [-0.25628367 -0.40863255  1.53978332 ... -0.85672823 -0.390061\n",
      "   -1.10581122]]\n",
      "\n",
      " [[ 0.2096762  -0.37111085  0.3818586  ...  0.38131718  1.19560869\n",
      "   -0.97231743]\n",
      "  [-0.32424463  0.56160598 -0.32361697 ... -0.70109044 -0.13365668\n",
      "   -0.19262529]\n",
      "  [ 0.9361929  -1.01810729 -0.75453323 ... -0.99265471  0.70139734\n",
      "    0.20163159]\n",
      "  ...\n",
      "  [-0.12637708  0.95336991  0.59158225 ... -0.39500776 -0.99155844\n",
      "    0.07018646]\n",
      "  [-1.39561514  0.23078045 -0.81859059 ... -0.31947446  0.0246221\n",
      "   -0.01047841]\n",
      "  [ 0.61910344  0.84546411 -0.74710384 ... -0.39180644  0.68154813\n",
      "   -0.16532991]]\n",
      "\n",
      " [[-0.36486674  0.44284716  0.41445276 ...  0.64841801 -0.01756328\n",
      "    0.72961441]\n",
      "  [-0.77211694  0.44155029  0.15587387 ...  0.3636328   0.49424418\n",
      "    0.42927145]\n",
      "  [ 0.47544419  0.33853419  0.70290861 ... -0.06729059  1.01927308\n",
      "   -0.54506115]\n",
      "  ...\n",
      "  [-0.32746008 -0.75810861  0.55836973 ...  0.99036105  0.49072499\n",
      "   -0.16809671]\n",
      "  [-0.79863865  0.56460085 -0.09810171 ...  0.26383175  0.09498054\n",
      "    0.56694442]\n",
      "  [ 1.18346787  0.67576264  0.39557907 ... -0.11829424  0.60081539\n",
      "    0.17310119]]\n",
      "\n",
      " [[-0.40849221  0.09566931 -0.99526234 ... -1.13153658 -0.18607414\n",
      "    1.044868  ]\n",
      "  [-1.40069777 -0.34188201  0.18686877 ...  0.09096995  0.32551357\n",
      "    0.72940037]\n",
      "  [-0.31424935 -0.71571376  0.54067564 ...  0.8345786  -0.67263553\n",
      "   -0.42164432]\n",
      "  ...\n",
      "  [ 0.62496131  0.24106109  0.85285059 ... -0.01558105  0.99386196\n",
      "    0.08064978]\n",
      "  [-1.02705177  0.57717778  0.21350034 ...  0.13070796 -0.04103175\n",
      "    0.27965587]\n",
      "  [ 0.0179045  -0.92170574  0.09373789 ... -0.38479369  0.56332927\n",
      "    0.50240884]]\n",
      "\n",
      " [[ 1.3629591   0.34293305 -0.35259844 ...  0.35710177 -0.43733129\n",
      "    0.12516053]\n",
      "  [-0.4650547   1.08265826  0.59326735 ...  0.05913793 -0.75079954\n",
      "   -0.96044643]\n",
      "  [-0.2651969  -0.40337594 -0.65083842 ... -0.30401599  0.11624855\n",
      "   -0.55609255]\n",
      "  ...\n",
      "  [-0.54111183  0.35649903 -0.0499447  ...  0.850983   -0.77564074\n",
      "   -0.04038083]\n",
      "  [-0.08341958  0.36095891  0.62955089 ...  0.18753562  0.31130465\n",
      "    0.75764189]\n",
      "  [ 0.6040372  -0.61559376  0.93276168 ...  1.03044777 -0.47686221\n",
      "   -0.18301799]]]\n",
      "\n",
      "Layer 3 weights:\n",
      "[-0.20123533  0.55154469  0.44219905 -0.00962014 -0.54270178  0.96674959\n",
      "  0.36372922 -0.60880737  0.80255346 -0.38053291  1.22989916  0.82683018\n",
      "  0.33722391 -0.9677394  -1.18850415 -0.44109114 -0.13805663 -1.18491166\n",
      "  0.00679385 -0.35050805 -0.70534755  0.50529433  0.95171698 -1.10682417\n",
      "  1.10298684  0.45740061 -0.71374992  0.0864526  -0.55891448  1.20235945\n",
      " -0.48438425  0.97013358 -0.19374839  0.49871092 -0.28474318  0.94132378\n",
      " -0.39620284 -0.16628186  0.51093754  0.77160723  0.94097752 -0.10847386\n",
      " -0.65562077 -0.14362612 -0.99606761 -0.59339518 -0.51823204  0.81052164\n",
      " -0.13130777 -0.99669331 -0.21183057 -0.516922    1.33962057  0.00175055\n",
      "  1.25921064  0.79446312 -0.54360765  0.54532131  0.30113745  0.6496132\n",
      " -0.11671599  0.02359123  1.07576695 -0.85111582]\n",
      "\n",
      "Layer 4 weights:\n",
      "[[-0.24306492 -0.92976045 -0.76238495 ... -0.8778875   0.23106923\n",
      "   0.13173623]\n",
      " [ 0.96829425  0.46243698 -0.5729948  ... -0.51218626  0.13693218\n",
      "  -0.74528284]\n",
      " [ 0.38854859  0.42495868  0.2073478  ... -0.85730561  0.67038008\n",
      "   0.88602228]\n",
      " ...\n",
      " [-0.42461862  1.13155194  0.05987753 ...  0.21905688  0.83489373\n",
      "   0.56718001]\n",
      " [-0.9993383  -0.96202642  0.67308036 ... -0.79622938  0.32706799\n",
      "   0.6618917 ]\n",
      " [-0.20230276  0.18331518  0.11150413 ...  0.93576207 -0.57133129\n",
      "   0.59799372]]\n",
      "\n",
      "Layer 5 weights:\n",
      "[ 0.47946703  1.03145274 -0.66165064 -0.56142115  0.57882132  0.90450924\n",
      " -0.47266427  0.94241237  0.53766063  0.7238152  -0.17237382  0.04130761\n",
      " -0.01325936  0.63711345  0.37242756  1.16732321 -0.17815741  0.57598173\n",
      " -0.50655345  0.81558984 -0.19708066 -0.69958855 -0.80326063 -1.31456118\n",
      " -0.53541435 -0.68211975  1.23065487  0.09492547  0.63699807 -0.36712296\n",
      "  0.29509969 -0.45931781  0.73893097 -0.42372592  0.16542519 -0.66151391\n",
      " -0.90425781  0.94725404 -0.26963947  0.39167485  0.6956169   0.29819124\n",
      "  0.98117638 -0.54797955 -0.48665066  0.65735525  0.80207156 -0.22603324\n",
      "  0.76561077 -0.37233753  0.86481875 -1.08976333 -0.02383078  0.76800894\n",
      "  0.7021906  -0.90009579  0.24849119 -0.86742498  0.54691261 -0.87693598\n",
      "  0.37788685  0.21375937  1.12900449  0.50994461]\n",
      "\n",
      "Layer 6 weights:\n",
      "[[ 0.56687939 -0.68725181 -0.85347642 ...  0.31708471  0.79690579\n",
      "   0.36387981]\n",
      " [-0.46487816  0.71167656  0.43737773 ...  0.63750953  0.54437028\n",
      "  -0.10859869]\n",
      " [-0.50551742  0.95274332 -0.1771515  ...  0.95627285 -0.29812545\n",
      "  -0.99199868]\n",
      " ...\n",
      " [ 0.07007273  0.11135606  0.68193402 ... -0.91024126  0.06957785\n",
      "   0.27365149]\n",
      " [-0.31750654 -0.22689968 -0.5752666  ... -0.799921    0.83964365\n",
      "   0.991629  ]\n",
      " [-0.76082404 -0.51727625 -0.29138738 ...  0.16385935 -1.1993348\n",
      "   0.51226294]]\n",
      "\n",
      "Layer 7 weights:\n",
      "[-0.00473937 -0.277154    0.40998464  0.25351955  0.3775892  -0.11437214\n",
      "  0.94291901  0.30514476  0.58105811  0.47606874  0.90106371  0.94070063\n",
      " -0.0532385  -0.12968292 -0.62645419 -0.33095027]\n",
      "\n",
      "Layer 8 weights:\n",
      "[[ 0.21034963]\n",
      " [ 0.73165489]\n",
      " [-0.11597266]\n",
      " [ 0.36069697]\n",
      " [-0.26936936]\n",
      " [ 0.1030951 ]\n",
      " [-0.86311056]\n",
      " [ 0.57434973]\n",
      " [-0.52864561]\n",
      " [ 0.7035638 ]\n",
      " [ 0.24558066]\n",
      " [-0.66639353]\n",
      " [-0.6002849 ]\n",
      " [-0.73834284]\n",
      " [ 0.13794959]\n",
      " [ 0.36702812]]\n",
      "\n",
      "Layer 9 weights:\n",
      "[0.74013375]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 200  # Cut reviews after 200 words\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Further split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an enhanced ANN model for IMDB\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 32, input_length=maxlen),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=4),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize model weights by running a single forward pass\n",
    "model.predict(X_train[:1])  # Pass a single sample to initialize the model's weights\n",
    "\n",
    "# Genetic Algorithm parameters\n",
    "population_size = 20  # Number of individuals in population\n",
    "generations = 20  # Number of generations\n",
    "mutation_rate = 0.05  # Mutation rate\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to evaluate fitness (loss)\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_accuracy = np.mean((test_predictions > 0.5).astype('int') == y_test.reshape(-1, 1))\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Print the final weights of the best individual\n",
    "for i, weight_matrix in enumerate(best_weights):\n",
    "    print(f\"Layer {i+1} weights:\\n{weight_matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3GD4ZDIf82k",
    "outputId": "6cd30dd6-d040-40e7-94ea-4a92248d5325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m786.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.8/446.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.6.1 multidict-6.1.0 multiprocess-0.70.16 xxhash-3.5.0 yarl-1.11.1\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm with fitness function of MSE on ANN which is trained on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KBowyTYb4MU",
    "outputId": "bd40f30b-500b-43c9-914e-58cb2f068903"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at generation 15\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Test set MSE: 0.0380\n",
      "Test set accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=4, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model using mean squared error (MSE)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to evaluate fitness using MSE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "    mse = np.mean(np.square(predictions - y_train))  # MSE for training set predictions\n",
    "    return mse\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set using MSE\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = np.mean(np.square(test_predictions - y_test))  # MSE for test set\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm with hybrid fitness function of MSE and CCE on ANN which is trained on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvLOKygDdBXG",
    "outputId": "b1eb30fc-f1c8-4e3c-f54a-2a528b9873e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "Test set MSE: 0.0486\n",
      "Test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=4, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "\n",
    "    # Calculate CCE\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=4, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "\n",
    "    # Calculate CCE\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm with hybrid fitness function of MSE and CCE on ANN with more layers which is trained on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YuAnHIQyfMqQ",
    "outputId": "94774ab4-960a-446b-cdbb-202c91560feb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at generation 15\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Test set MSE: 0.1942\n",
      "Test set accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a larger neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=4, activation='relu'),   # Increased units\n",
    "    Dense(128, activation='relu'),                # Increased units\n",
    "    Dense(64, activation='relu'),                 # Increased units\n",
    "    Dense(32, activation='relu'),                 # New layer added\n",
    "    Dense(3, activation='softmax')                # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "\n",
    "    # Calculate CCE\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm with hybrid fitness function of MSE and CCE on CNN which is trained on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjSCl94cf4cQ",
    "outputId": "79a328e0-de2c-4c86-fa67-b611148d7e71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Test set MSE: 0.1693\n",
      "Test set accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Reshape X for CNN input (samples, height, width, channels)\n",
    "X = X.reshape(-1, 2, 2, 1)  # Reshape to (samples, 2, 2, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CNN model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(2, 2, 1)),\n",
    "        MaxPooling2D(pool_size=(1, 1)),\n",
    "        Conv2D(64, kernel_size=(1, 1), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(1, 1)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual(model):\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(model, individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE and CCE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "model = create_model()\n",
    "population = [create_individual(model) for _ in range(population_size)]\n",
    "\n",
    "# Genetic algorithm loop\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(model, ind) for ind in population]\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(model, ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm with hybrid fitness function of MSE and CCE on CNN which is trained on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AptJGC5yiSYS",
    "outputId": "079e8fda-2b52-423e-97c6-448aa4c2fb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step\n",
      "Test set MSE: 0.4892\n",
      "Test set accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000  # Top 10,000 words\n",
    "maxlen = 200  # Maximum sequence length\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create a CNN model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Embedding(max_features, 128, input_length=maxlen),\n",
    "        Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(128, kernel_size=5, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(2, activation='softmax')  # Binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual(model):\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(model, individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE and CCE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "model = create_model()\n",
    "population = [create_individual(model) for _ in range(population_size)]\n",
    "\n",
    "# Genetic algorithm loop\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(model, ind) for ind in population]\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(model, ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Genetic Algorithm with Hybrid Fitness Function of MSE and CCE on CNN Trained on IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFMk1hgbiZUg",
    "outputId": "a8ce6154-cee3-456a-fbb9-13e6e594a2f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at generation 15\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Test set MSE: 0.1942\n",
      "Test set accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a larger neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=4, activation='relu'),   # Increased units\n",
    "    Dense(128, activation='relu'),                # Increased units\n",
    "    Dense(64, activation='relu'),                 # Increased units\n",
    "    Dense(32, activation='relu'),                 # New layer added\n",
    "    Dense(3, activation='softmax')                # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "\n",
    "    # Calculate CCE\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 20000\n",
    "maxlen = 200  # Cut texts after this number of words (among top max_features most common words)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# One-hot encoding the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create a neural network architecture\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 128, input_length=maxlen),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # Output layer for 2 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "\n",
    "    # Calculate CCE\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "jQff6ze5r5Cw",
    "outputId": "7f303e90-2543-4185-da03-349d310cff1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer 'sequential' with a weight list of length 0, but the layer was expecting 7 weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cfea427695b8>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Genetic algorithm loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mfitness_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Elitism: Carry forward the best individual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cfea427695b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Genetic algorithm loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mfitness_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Elitism: Carry forward the best individual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cfea427695b8>\u001b[0m in \u001b[0;36mevaluate_fitness\u001b[0;34m(model, individual)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Hybrid fitness function combining MSE and CCE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindividual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlayer_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    678\u001b[0m                 \u001b[0;34mf\"You called `set_weights(weights)` on layer '{self.name}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;34mf\"with a weight list of length {len(weights)}, but the layer \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer 'sequential' with a weight list of length 0, but the layer was expecting 7 weights."
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.datasets import imdb\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Load and preprocess the IMDB dataset\n",
    "# max_features = 10000  # Top 10,000 words\n",
    "# maxlen = 200  # Maximum sequence length\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "# X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# # One-hot encode the labels\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "# y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# # Create an ANN model\n",
    "# def create_ann_model():\n",
    "#     model = Sequential([\n",
    "#         Embedding(max_features, 128, input_length=maxlen),\n",
    "#         Flatten(),  # Flatten the embeddings for fully connected layers\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dense(2, activation='softmax')  # Binary classification\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# # Genetic algorithm parameters\n",
    "# population_size = 20\n",
    "# generations = 20\n",
    "# mutation_rate = 0.05\n",
    "# mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "# cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# # Function to create random weights for the model\n",
    "# def create_individual(model):\n",
    "#     return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# # Function to calculate MSE for fitness\n",
    "# def calculate_mse(predictions, true_values):\n",
    "#     return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# # Function to calculate CCE for fitness\n",
    "# def calculate_cce(predictions, true_values):\n",
    "#     return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# # Hybrid fitness function combining MSE and CCE\n",
    "# def evaluate_fitness(model, individual):\n",
    "#     model.set_weights(individual)\n",
    "#     predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "#     # Calculate MSE and CCE\n",
    "#     mse = calculate_mse(predictions, y_train)\n",
    "#     cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "#     # Combine MSE and CCE with weights\n",
    "#     fitness = mse_weight * mse + cce_weight * cce\n",
    "#     return fitness\n",
    "\n",
    "# # Function to perform crossover between two parents\n",
    "# def crossover(parent1, parent2):\n",
    "#     child = []\n",
    "#     for p1, p2 in zip(parent1, parent2):\n",
    "#         mask = np.random.rand(*p1.shape) > 0.5\n",
    "#         child.append(np.where(mask, p1, p2))\n",
    "#     return child\n",
    "\n",
    "# # Function to perform mutation on an individual\n",
    "# def mutate(individual, rate):\n",
    "#     for i in range(len(individual)):\n",
    "#         if np.random.rand() < rate:\n",
    "#             mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "#             individual[i] += mutation\n",
    "#     return individual\n",
    "\n",
    "# # Initialize the population\n",
    "# model = create_ann_model()\n",
    "# population = [create_individual(model) for _ in range(population_size)]\n",
    "\n",
    "# # Genetic algorithm loop\n",
    "# for generation in range(generations):\n",
    "#     fitness_scores = [evaluate_fitness(model, ind) for ind in population]\n",
    "\n",
    "#     # Elitism: Carry forward the best individual\n",
    "#     best_individual = population[np.argmin(fitness_scores)]\n",
    "#     new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "#     selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "#     selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "#     while len(new_population) < population_size:\n",
    "#         parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "#         child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "#         child = mutate(child, mutation_rate)\n",
    "#         new_population.append(child)\n",
    "\n",
    "#     population = new_population\n",
    "\n",
    "# # Set the best weights found\n",
    "# best_weights = population[np.argmin([evaluate_fitness(model, ind) for ind in population])]\n",
    "# model.set_weights(best_weights)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_predictions = model.predict(X_test)\n",
    "# test_mse = calculate_mse(test_predictions, y_test)\n",
    "# test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "# print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "# print(f\"Test set accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MIagLuURqfw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess the IMDB dataset from Hugging Face\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "# Split train into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_texts).toarray()\n",
    "X_val = vectorizer.transform(val_texts).toarray()\n",
    "X_test = vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'inputs': self.texts[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = IMDBDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = IMDBDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = IMDBDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define the ANN model with L1 pruning\n",
    "class ANNModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_units)\n",
    "        self.fc2 = torch.nn.Linear(hidden_units, hidden_units)\n",
    "        self.fc3 = torch.nn.Linear(hidden_units, output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def create_model(hidden_units, learning_rate):\n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "    output_dim = 2  # Binary classification\n",
    "    model = ANNModel(input_dim, int(hidden_units), output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "# Objective Function: Minimize Validation Loss with L1 Regularization\n",
    "def objective_function(hyperparams):\n",
    "    hidden_units = hyperparams[0]\n",
    "    learning_rate = hyperparams[1]\n",
    "\n",
    "    model, optimizer = create_model(hidden_units, learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['inputs']\n",
    "        labels = batch['labels']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        # L1 Regularization\n",
    "        l1_lambda = 0.001  # Adjust as needed\n",
    "        l1_norm = sum(torch.norm(param, 1) for param in model.parameters())\n",
    "        loss += l1_lambda * l1_norm\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['inputs']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(inputs)\n",
    "            preds += torch.argmax(outputs, dim=1).tolist()\n",
    "            labels_list += labels.tolist()\n",
    "\n",
    "    accuracy = accuracy_score(labels_list, preds)\n",
    "    return 1 - accuracy  # Minimize this value\n",
    "\n",
    "# Bat Algorithm Implementation\n",
    "class BatAlgorithm:\n",
    "    def __init__(self, num_bats, max_iter, bounds, alpha=0.9, gamma=0.9):\n",
    "        self.num_bats = num_bats\n",
    "        self.max_iter = max_iter\n",
    "        self.bounds = bounds\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Initialize bats\n",
    "        self.bats = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_bats, bounds.shape[0]))\n",
    "        self.velocities = np.zeros((num_bats, bounds.shape[0]))\n",
    "        self.fitness = np.zeros(num_bats)\n",
    "        self.frequencies = np.zeros(num_bats)\n",
    "\n",
    "        # Best bat initialization\n",
    "        self.best_bat = self.bats[0]\n",
    "        self.best_fitness = float(\"inf\")\n",
    "\n",
    "    def optimize(self, objective_function):\n",
    "        for iter in range(self.max_iter):\n",
    "            for i in range(self.num_bats):\n",
    "                # Frequency update\n",
    "                self.frequencies[i] = np.random.uniform(0, 1)\n",
    "                # Velocity update\n",
    "                self.velocities[i] = self.velocities[i] + (self.bats[i] - self.best_bat) * self.frequencies[i]\n",
    "                # Position update\n",
    "                candidate_bat = self.bats[i] + self.velocities[i]\n",
    "                candidate_bat = np.clip(candidate_bat, self.bounds[:, 0], self.bounds[:, 1])\n",
    "\n",
    "                # Evaluate candidate\n",
    "                fitness = objective_function(candidate_bat)\n",
    "\n",
    "                # Update if new solution is better\n",
    "                if fitness < self.fitness[i]:\n",
    "                    self.bats[i] = candidate_bat\n",
    "                    self.fitness[i] = fitness\n",
    "\n",
    "                # Update the best bat\n",
    "                if self.fitness[i] < self.best_fitness:\n",
    "                    self.best_bat = self.bats[i]\n",
    "                    self.best_fitness = self.fitness[i]\n",
    "\n",
    "            print(f\"Iteration {iter+1}/{self.max_iter}, Best Fitness: {self.best_fitness}\")\n",
    "\n",
    "        return self.best_bat\n",
    "\n",
    "# Hyperparameter bounds: hidden units (4 to 64), learning rate (1e-6 to 1e-2)\n",
    "bounds = np.array([[4, 64], [1e-6, 1e-2]])\n",
    "\n",
    "# Initialize and run Bat Algorithm\n",
    "bat_algorithm = BatAlgorithm(num_bats=10, max_iter=10, bounds=bounds, alpha=0.9, gamma=0.9)\n",
    "best_hyperparams = bat_algorithm.optimize(objective_function)\n",
    "print(\"Best hyperparameters found:\", best_hyperparams)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "best_hidden_units, best_lr = best_hyperparams\n",
    "final_model, final_optimizer = create_model(best_hidden_units, best_lr)\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train and Evaluate Final Model with Best Hyperparameters\n",
    "for epoch in range(3):  # You can increase the number of epochs if needed\n",
    "    final_model.train()\n",
    "    for batch in final_train_loader:\n",
    "        inputs = batch['inputs']\n",
    "        labels = batch['labels']\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        # L1 Regularization\n",
    "        l1_norm = sum(torch.norm(param, 1) for param in final_model.parameters())\n",
    "        loss += l1_lambda * l1_norm\n",
    "\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "# Evaluate final model\n",
    "final_model.eval()\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "preds = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['inputs']\n",
    "        labels = batch['labels']\n",
    "        outputs = final_model(inputs)\n",
    "        preds += torch.argmax(outputs, dim=1).tolist()\n",
    "        labels_list += labels.tolist()\n",
    "\n",
    "accuracy = accuracy_score(labels_list, preds)\n",
    "print(f\"Final model accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OYqljyH4ALgS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chethanarkini/miniforge3/envs/rakuten/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at generation 15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Test set MSE: 0.1942\n",
      "Test set accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chethanarkini/miniforge3/envs/rakuten/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer 'sequential_3' with a weight list of length 0, but the layer was expecting 8 weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 234\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Genetic algorithm loop with early stopping\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generations):\n\u001b[0;32m--> 234\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [evaluate_fitness(ind) \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# Check for improvement\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     current_best_fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(fitness_scores)\n",
      "Cell \u001b[0;32mIn[5], line 234\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Genetic algorithm loop with early stopping\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generations):\n\u001b[0;32m--> 234\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mevaluate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# Check for improvement\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     current_best_fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(fitness_scores)\n",
      "Cell \u001b[0;32mIn[5], line 195\u001b[0m, in \u001b[0;36mevaluate_fitness\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_fitness\u001b[39m(individual):\n\u001b[0;32m--> 195\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# Calculate MSE\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rakuten/lib/python3.10/site-packages/keras/src/layers/layer.py:696\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    694\u001b[0m layer_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_weights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou called `set_weights(weights)` on layer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a weight list of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layer_weights, weights):\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer 'sequential_3' with a weight list of length 0, but the layer was expecting 8 weights."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a larger neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=4, activation='relu'),   # Increased units\n",
    "    Dense(128, activation='relu'),                # Increased units\n",
    "    Dense(64, activation='relu'),                 # Increased units\n",
    "    Dense(32, activation='relu'),                 # New layer added\n",
    "    Dense(3, activation='softmax')                # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "\n",
    "    # Calculate CCE\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 20000\n",
    "maxlen = 200  # Cut texts after this number of words (among top max_features most common words)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# One-hot encoding the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create a neural network architecture\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 128, input_length=maxlen),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # Output layer for 2 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 20\n",
    "generations = 20\n",
    "mutation_rate = 0.05\n",
    "mse_weight = 0.4  # Weight for MSE in the hybrid fitness function\n",
    "cce_weight = 0.6  # Weight for categorical crossentropy in the hybrid fitness function\n",
    "\n",
    "# Function to create random weights for the model\n",
    "def create_individual():\n",
    "    return [np.random.uniform(-1, 1, w.shape) for w in model.get_weights()]\n",
    "\n",
    "# Function to calculate MSE for fitness\n",
    "def calculate_mse(predictions, true_values):\n",
    "    return np.mean(np.square(predictions - true_values))\n",
    "\n",
    "# Function to calculate CCE for fitness\n",
    "def calculate_cce(predictions, true_values):\n",
    "    return np.mean(categorical_crossentropy(true_values, predictions).numpy())\n",
    "\n",
    "# Hybrid fitness function combining MSE and CCE\n",
    "def evaluate_fitness(individual):\n",
    "    model.set_weights(individual)\n",
    "    predictions = model.predict(X_train, verbose=0)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(predictions, y_train)\n",
    "\n",
    "    # Calculate CCE\n",
    "    cce = calculate_cce(predictions, y_train)\n",
    "\n",
    "    # Combine MSE and CCE with weights\n",
    "    fitness = mse_weight * mse + cce_weight * cce\n",
    "    return fitness\n",
    "\n",
    "# Function to perform crossover between two parents\n",
    "def crossover(parent1, parent2):\n",
    "    child = []\n",
    "    for p1, p2 in zip(parent1, parent2):\n",
    "        mask = np.random.rand(*p1.shape) > 0.5\n",
    "        child.append(np.where(mask, p1, p2))\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation on an individual\n",
    "def mutate(individual, rate):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < rate:\n",
    "            mutation = np.random.uniform(-0.5, 0.5, individual[i].shape)\n",
    "            individual[i] += mutation\n",
    "    return individual\n",
    "\n",
    "# Initialize the population\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_rounds = 5\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.inf\n",
    "\n",
    "# Genetic algorithm loop with early stopping\n",
    "for generation in range(generations):\n",
    "    fitness_scores = [evaluate_fitness(ind) for ind in population]\n",
    "\n",
    "    # Check for improvement\n",
    "    current_best_fitness = min(fitness_scores)\n",
    "    if current_best_fitness < best_fitness:\n",
    "        best_fitness = current_best_fitness\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at generation {generation}\")\n",
    "        break\n",
    "\n",
    "    # Elitism: Carry forward the best individual\n",
    "    best_individual = population[np.argmin(fitness_scores)]\n",
    "    new_population = [best_individual]  # Start new population with the best individual\n",
    "\n",
    "    selected_indices = np.argsort(fitness_scores)[:population_size // 2]\n",
    "    selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        parents = np.random.choice(range(len(selected_population)), 2, replace=False)\n",
    "        child = crossover(selected_population[parents[0]], selected_population[parents[1]])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_population.append(child)\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Set the best weights found\n",
    "best_weights = population[np.argmin([evaluate_fitness(ind) for ind in population])]\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = calculate_mse(test_predictions, y_test)\n",
    "test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "print(f\"Test set MSE: {test_mse:.4f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
