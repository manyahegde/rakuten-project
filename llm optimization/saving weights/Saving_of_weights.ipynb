{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcbN3YzrlOw5",
        "outputId": "eecd2ef0-7611-4c02-9836-831a4c8ba9f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a simple neural network\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden = np.dot(X, self.weights_input_hidden)\n",
        "        self.hidden = self.sigmoid(self.hidden)\n",
        "        output = np.dot(self.hidden, self.weights_hidden_output)\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def loss(self, predicted, actual):\n",
        "        return np.mean((predicted - actual) ** 2)\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.1):\n",
        "        output_error = y - output\n",
        "        output_delta = output_error * self.sigmoid_derivative(output)\n",
        "\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden)\n",
        "\n",
        "        # Update weights\n",
        "        self.weights_hidden_output += self.hidden.T.dot(output_delta) * learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmy6ZdZ8bdSP"
      },
      "outputs": [],
      "source": [
        "# Define the PSO optimizer\n",
        "class PSO:\n",
        "    def __init__(self, nn, num_particles, max_iter, X, y):\n",
        "        self.nn = nn\n",
        "        self.num_particles = num_particles\n",
        "        self.max_iter = max_iter\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Initialize particles\n",
        "        self.particles = [self.initialize_particle() for _ in range(num_particles)]\n",
        "        self.global_best_position = self.particles[0]['position']\n",
        "        self.global_best_loss = np.inf\n",
        "\n",
        "    def initialize_particle(self):\n",
        "        particle = {\n",
        "            'position': {\n",
        "                'input_hidden': np.random.randn(*self.nn.weights_input_hidden.shape),\n",
        "                'hidden_output': np.random.randn(*self.nn.weights_hidden_output.shape),\n",
        "            },\n",
        "            'velocity': {\n",
        "                'input_hidden': np.random.randn(*self.nn.weights_input_hidden.shape),\n",
        "                'hidden_output': np.random.randn(*self.nn.weights_hidden_output.shape),\n",
        "            },\n",
        "            'best_position': None,\n",
        "            'best_loss': np.inf,\n",
        "        }\n",
        "        return particle\n",
        "\n",
        "    def update_velocity(self, particle):\n",
        "        w = 0.5  # inertia weight\n",
        "        c1 = 1.5  # cognitive coefficient\n",
        "        c2 = 1.5  # social coefficient\n",
        "\n",
        "        r1 = np.random.rand()\n",
        "        r2 = np.random.rand()\n",
        "\n",
        "        for layer in ['input_hidden', 'hidden_output']:\n",
        "            cognitive_component = c1 * r1 * (particle['best_position'][layer] - particle['position'][layer])\n",
        "            social_component = c2 * r2 * (self.global_best_position[layer] - particle['position'][layer])\n",
        "            particle['velocity'][layer] = w * particle['velocity'][layer] + cognitive_component + social_component\n",
        "\n",
        "    def update_position(self, particle):\n",
        "        for layer in ['input_hidden', 'hidden_output']:\n",
        "            particle['position'][layer] += particle['velocity'][layer]\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.max_iter):\n",
        "            for particle in self.particles:\n",
        "                # Set the weights of the neural network to the current particle's position\n",
        "                self.nn.weights_input_hidden = particle['position']['input_hidden']\n",
        "                self.nn.weights_hidden_output = particle['position']['hidden_output']\n",
        "\n",
        "                # Calculate the loss\n",
        "                predictions = self.nn.forward(self.X)\n",
        "                loss = self.nn.loss(predictions, self.y)\n",
        "\n",
        "                # Update the particle's best known position\n",
        "                if loss < particle['best_loss']:\n",
        "                    particle['best_loss'] = loss\n",
        "                    particle['best_position'] = particle['position'].copy()\n",
        "\n",
        "                # Update the global best position\n",
        "                if loss < self.global_best_loss:\n",
        "                    self.global_best_loss = loss\n",
        "                    self.global_best_position = particle['position'].copy()\n",
        "\n",
        "                # Update particle velocity and position\n",
        "                self.update_velocity(particle)\n",
        "                self.update_position(particle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_0RfezbpiT",
        "outputId": "aa949953-cd44-4287-e4de-a3f54379ee56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input to hidden weights :  [[ 0.42171121  1.41930181 -2.43014888  0.3003442   2.19851654]\n",
            " [ 0.0956003   0.35025836  1.04737149 -5.77338272  1.45873103]\n",
            " [-4.45929078 -1.22717415 -0.41395803  0.29961942 -1.56566428]\n",
            " [-0.69496472 -1.12822077 -0.48472342  1.89368733  0.32301926]]\n",
            "Hidden to output weights :  [[ 2.45121592e+00 -3.96313835e-01 -1.36808538e+00]\n",
            " [ 2.22676843e-01  2.11321703e-03 -1.63096209e+00]\n",
            " [ 9.44590690e-01 -4.57889272e-01 -4.07931527e+00]\n",
            " [-4.28574883e+00  8.32837441e-01  8.27229612e-01]\n",
            " [-1.71209569e-01 -1.51497856e+00  1.97478507e+00]]\n",
            "Epoch 1/10 complete\n",
            "Input to hidden weights :  [[ 0.60360438  1.27167906 -2.72341424  0.98240258  2.18608226]\n",
            " [ 0.3587174   0.18325364  0.46828137 -5.47318469  2.17115234]\n",
            " [-4.48592815 -1.68760981 -1.03219369  0.88583895 -1.05294691]\n",
            " [-0.73626722 -1.70597544 -1.51133431  2.50374818  1.50835292]]\n",
            "Hidden to output weights :  [[ 2.81758817 -0.50803944 -2.43997986]\n",
            " [-0.07412938  0.16612391 -2.91564952]\n",
            " [ 0.97265889 -0.62349286 -4.73019265]\n",
            " [-4.59150859  1.35719513  0.34349677]\n",
            " [-1.21533072 -2.32899935  1.6396267 ]]\n",
            "Epoch 2/10 complete\n",
            "Input to hidden weights :  [[ 1.04170727  1.65364635 -2.35405887  1.10431411  1.86926759]\n",
            " [ 0.66118074  0.02676931  1.0525548  -5.45857447  1.92447541]\n",
            " [-4.34574129 -1.70880264 -1.12207361  1.01469116 -0.76178151]\n",
            " [-0.63415828 -1.84271375 -1.67666953  2.53166219  2.26673757]]\n",
            "Hidden to output weights :  [[ 3.13038174 -0.93654883 -2.78408774]\n",
            " [ 0.19924035  0.35117897 -3.56237705]\n",
            " [ 1.24283887 -0.75506476 -4.82247018]\n",
            " [-4.51750481  1.75042732  0.79764005]\n",
            " [-1.24803317 -2.80215681  1.4198055 ]]\n",
            "Epoch 3/10 complete\n",
            "Input to hidden weights :  [[ 1.21988138  1.69023713 -1.89291935  1.37508582  1.21976928]\n",
            " [ 0.84987063 -0.27317429  1.42102723 -5.33234221  1.56443705]\n",
            " [-4.34314424 -1.95583547 -1.01883865  1.34236669 -0.69220125]\n",
            " [-0.62911202 -2.13469658 -1.65332144  2.74146688  2.9145965 ]]\n",
            "Hidden to output weights :  [[ 3.27972625 -1.29795275 -3.17196298]\n",
            " [ 0.31979776  0.7082232  -4.20711862]\n",
            " [ 1.34731923 -0.96592442 -5.03749834]\n",
            " [-4.46314466  2.14542631  0.81751128]\n",
            " [-1.45669166 -3.24858728  1.40455298]]\n",
            "Epoch 4/10 complete\n",
            "Input to hidden weights :  [[ 1.10692765  1.73162519 -1.77060078  1.57973259  0.83367441]\n",
            " [ 0.88454491 -0.38236452  1.51268063 -5.20168757  1.33198111]\n",
            " [-4.41649687 -1.99432248 -0.95725998  1.60533717 -0.74149412]\n",
            " [-0.66619601 -2.15135941 -1.60949468  2.9387781   3.09247136]]\n",
            "Hidden to output weights :  [[ 3.2957429  -1.4348598  -3.30482415]\n",
            " [ 0.33250083  0.97273973 -4.42492411]\n",
            " [ 1.3437294  -1.12466095 -5.16673642]\n",
            " [-4.48593118  2.26838767  0.85135504]\n",
            " [-1.53688628 -3.42758575  1.45618075]]\n",
            "Epoch 5/10 complete\n",
            "Input to hidden weights :  [[ 0.7055514   1.62810903 -1.3656627   2.22962419 -0.01950893]\n",
            " [ 1.09222175 -0.70842582  1.8384596  -4.79240525  0.57939794]\n",
            " [-4.70421044 -2.28617098 -0.68914739  2.61691761 -0.72222749]\n",
            " [-0.8108748  -2.21912954 -1.38224165  3.7066545   3.68489707]]\n",
            "Hidden to output weights :  [[ 3.3352261  -2.06079272 -3.82786972]\n",
            " [ 0.35689412  1.94199122 -5.30307499]\n",
            " [ 1.31341727 -1.62174496 -5.75627911]\n",
            " [-4.59581076  2.91481652  0.83024434]\n",
            " [-1.81982869 -4.23904644  1.57582321]]\n",
            "Epoch 6/10 complete\n",
            "Input to hidden weights :  [[ 0.59682006  1.64638324 -1.36065183  2.16374095  0.19026846]\n",
            " [ 1.21650238 -0.65539046  1.85832094 -4.85716759  0.46705801]\n",
            " [-4.8362608  -2.34355371 -0.64829175  2.82656385 -0.67830466]\n",
            " [-0.89915185 -2.15053719 -1.32905677  3.8678828   3.54694112]]\n",
            "Hidden to output weights :  [[ 3.32426235 -2.36854088 -3.94205376]\n",
            " [ 0.33845086  2.09058068 -5.54331627]\n",
            " [ 1.29719268 -1.5734594  -5.98930738]\n",
            " [-4.63634295  3.26536441  0.77704978]\n",
            " [-1.84575308 -4.59801502  1.65725655]]\n",
            "Epoch 7/10 complete\n",
            "Input to hidden weights :  [[ 0.44757866  1.69520938 -1.3598341   2.07662059  0.30755635]\n",
            " [ 1.37294965 -0.65100332  1.82175176 -4.93472464  0.18547534]\n",
            " [-5.04224002 -2.41127811 -0.61969413  3.10613486 -0.70306606]\n",
            " [-1.04857205 -2.06944925 -1.30301653  4.08350258  3.3059271 ]]\n",
            "Hidden to output weights :  [[ 3.31409036 -2.73950067 -4.06727322]\n",
            " [ 0.32055323  2.32003214 -5.86217448]\n",
            " [ 1.28140033 -1.42165195 -6.2673292 ]\n",
            " [-4.67961699  3.64941338  0.72547397]\n",
            " [-1.87266842 -5.10997982  1.82685509]]\n",
            "Epoch 8/10 complete\n",
            "Input to hidden weights :  [[ 0.35040355  1.7072843  -1.31678827  2.00016377  0.48678171]\n",
            " [ 1.53556146 -0.69205514  1.77149546 -4.99487008 -0.06489921]\n",
            " [-5.23608776 -2.49889609 -0.5703969   3.3451773  -0.70320892]\n",
            " [-1.20051177 -2.01028124 -1.27972692  4.27080523  3.0670428 ]]\n",
            "Hidden to output weights :  [[ 3.30945591 -3.12521929 -4.15093182]\n",
            " [ 0.31116158  2.48377243 -6.13156587]\n",
            " [ 1.27289553 -1.27266833 -6.49525709]\n",
            " [-4.70750492  4.01083443  0.66001195]\n",
            " [-1.89039309 -5.5226848   1.96529995]]\n",
            "Epoch 9/10 complete\n",
            "Input to hidden weights :  [[ 0.26630066  1.77749835 -1.26351527  1.92960814  0.60641532]\n",
            " [ 1.73024707 -0.74009951  1.70307034 -5.03738734 -0.28280368]\n",
            " [-5.44761402 -2.60188065 -0.51912062  3.58856828 -0.70639456]\n",
            " [-1.37561927 -1.98065741 -1.27503454  4.46788995  2.82759305]]\n",
            "Hidden to output weights :  [[ 3.30736939 -3.51965374 -4.22136925]\n",
            " [ 0.30488473  2.69832931 -6.39954086]\n",
            " [ 1.26723258 -1.08321147 -6.71515035]\n",
            " [-4.73055368  4.35333586  0.61392122]\n",
            " [-1.90510747 -5.98648228  2.12951268]]\n",
            "Epoch 10/10 complete\n",
            "Test loss: 0.027685382077030696\n"
          ]
        }
      ],
      "source": [
        "# Function to save the weights after each epoch\n",
        "def save_weights(epoch, nn):\n",
        "    np.save(f'weights_input_hidden_epoch_{epoch}.npy', nn.weights_input_hidden)\n",
        "    print(\"Input to hidden weights : \",nn.weights_input_hidden)\n",
        "    np.save(f'weights_hidden_output_epoch_{epoch}.npy', nn.weights_hidden_output)\n",
        "    print(\"Hidden to output weights : \",nn.weights_hidden_output)\n",
        "\n",
        "# Example usage with epochs and saving weights\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the neural network and PSO optimizer\n",
        "    nn = NeuralNetwork(input_size=4, hidden_size=5, output_size=3)\n",
        "    pso = PSO(nn, num_particles=20, max_iter=100, X=X_train, y=y_train)\n",
        "\n",
        "    # Run the optimization with epochs and save weights\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        pso.optimize()\n",
        "\n",
        "        # After each epoch, perform a backward pass to refine the weights\n",
        "        for _ in range(10):  # Perform 10 steps of backpropagation after each epoch\n",
        "            output = nn.forward(X_train)\n",
        "            nn.backward(X_train, y_train, output)\n",
        "\n",
        "        # Save weights of each neuron after each epoch\n",
        "        save_weights(epoch, nn)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} complete\")\n",
        "\n",
        "    # Test the trained neural network on the test set\n",
        "    predictions = nn.forward(X_test)\n",
        "    test_loss = nn.loss(predictions, y_test)\n",
        "\n",
        "    print(\"Test loss:\", test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi1mHzEMbtWX",
        "outputId": "653fffe7-44f0-4025-e55e-febda110fe56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Same with accuracy\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a simple neural network\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden = np.dot(X, self.weights_input_hidden)\n",
        "        self.hidden = self.sigmoid(self.hidden)\n",
        "        output = np.dot(self.hidden, self.weights_hidden_output)\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def loss(self, predicted, actual):\n",
        "        return np.mean((predicted - actual) ** 2)\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.1):\n",
        "        output_error = y - output\n",
        "        output_delta = output_error * self.sigmoid_derivative(output)\n",
        "\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden)\n",
        "\n",
        "        # Update weights\n",
        "        self.weights_hidden_output += self.hidden.T.dot(output_delta) * learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTu5HFhHFis1"
      },
      "outputs": [],
      "source": [
        "# Define the PSO optimizer\n",
        "class PSO:\n",
        "    def __init__(self, nn, num_particles, max_iter, X, y):\n",
        "        self.nn = nn\n",
        "        self.num_particles = num_particles\n",
        "        self.max_iter = max_iter\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Initialize particles\n",
        "        self.particles = [self.initialize_particle() for _ in range(num_particles)]\n",
        "        self.global_best_position = self.particles[0]['position']\n",
        "        self.global_best_loss = np.inf\n",
        "\n",
        "    def initialize_particle(self):\n",
        "        particle = {\n",
        "            'position': {\n",
        "                'input_hidden': np.random.randn(*self.nn.weights_input_hidden.shape),\n",
        "                'hidden_output': np.random.randn(*self.nn.weights_hidden_output.shape),\n",
        "            },\n",
        "            'velocity': {\n",
        "                'input_hidden': np.random.randn(*self.nn.weights_input_hidden.shape),\n",
        "                'hidden_output': np.random.randn(*self.nn.weights_hidden_output.shape),\n",
        "            },\n",
        "            'best_position': None,\n",
        "            'best_loss': np.inf,\n",
        "        }\n",
        "        return particle\n",
        "\n",
        "    def update_velocity(self, particle):\n",
        "        w = 0.5  # inertia weight\n",
        "        c1 = 1.5  # cognitive coefficient\n",
        "        c2 = 1.5  # social coefficient\n",
        "\n",
        "        r1 = np.random.rand()\n",
        "        r2 = np.random.rand()\n",
        "\n",
        "        for layer in ['input_hidden', 'hidden_output']:\n",
        "            cognitive_component = c1 * r1 * (particle['best_position'][layer] - particle['position'][layer])\n",
        "            social_component = c2 * r2 * (self.global_best_position[layer] - particle['position'][layer])\n",
        "            particle['velocity'][layer] = w * particle['velocity'][layer] + cognitive_component + social_component\n",
        "\n",
        "    def update_position(self, particle):\n",
        "        for layer in ['input_hidden', 'hidden_output']:\n",
        "            particle['position'][layer] += particle['velocity'][layer]\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.max_iter):\n",
        "            for particle in self.particles:\n",
        "                # Set the weights of the neural network to the current particle's position\n",
        "                self.nn.weights_input_hidden = particle['position']['input_hidden']\n",
        "                self.nn.weights_hidden_output = particle['position']['hidden_output']\n",
        "\n",
        "                # Calculate the loss\n",
        "                predictions = self.nn.forward(self.X)\n",
        "                loss = self.nn.loss(predictions, self.y)\n",
        "\n",
        "                # Update the particle's best known position\n",
        "                if loss < particle['best_loss']:\n",
        "                    particle['best_loss'] = loss\n",
        "                    particle['best_position'] = particle['position'].copy()\n",
        "\n",
        "                # Update the global best position\n",
        "                if loss < self.global_best_loss:\n",
        "                    self.global_best_loss = loss\n",
        "                    self.global_best_position = particle['position'].copy()\n",
        "\n",
        "                # Update particle velocity and position\n",
        "                self.update_velocity(particle)\n",
        "                self.update_position(particle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_cNFq8_FsZd",
        "outputId": "a5158b11-a84e-455b-ea57-71b2d7e90c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 complete\n",
            "Epoch 2/10 complete\n",
            "Epoch 3/10 complete\n",
            "Epoch 4/10 complete\n",
            "Epoch 5/10 complete\n",
            "Epoch 6/10 complete\n",
            "Epoch 7/10 complete\n",
            "Epoch 8/10 complete\n",
            "Epoch 9/10 complete\n",
            "Epoch 10/10 complete\n",
            "Final weights of each neuron:\n",
            "Input to Hidden Layer Weights:\n",
            " [[-0.38841055  0.71173248 -1.23697273 -1.38642165 -0.24523533]\n",
            " [ 0.40131432 -3.0698096   4.25463793  0.67368792  0.58492827]\n",
            " [-0.87503074  5.36092339 -4.41104296  1.80951761 -0.23097126]\n",
            " [-0.93342123  0.82991196 -2.61323533  2.68240349 -1.66470785]]\n",
            "Hidden to Output Layer Weights:\n",
            " [[ 1.30107293  2.97373395 -4.44862273]\n",
            " [-4.47961686  3.47453084 -0.54869815]\n",
            " [ 3.93056968 -6.45192463 -3.2802569 ]\n",
            " [-3.62434213 -5.43239609  3.56255753]\n",
            " [-0.53067419  1.54518979 -6.39713624]]\n",
            "Test Accuracy: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "# Training and Evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the neural network and PSO optimizer\n",
        "    nn = NeuralNetwork(input_size=4, hidden_size=5, output_size=3)\n",
        "    pso = PSO(nn, num_particles=20, max_iter=100, X=X_train, y=y_train)\n",
        "\n",
        "    # Run the optimization with epochs\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        pso.optimize()\n",
        "\n",
        "        # After each epoch, perform a backward pass to refine the weights\n",
        "        for _ in range(10):  # Perform 10 steps of backpropagation after each epoch\n",
        "            output = nn.forward(X_train)\n",
        "            nn.backward(X_train, y_train, output)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} complete\")\n",
        "\n",
        "    # Save the final weights after training\n",
        "    best_weights_input_hidden = nn.weights_input_hidden\n",
        "    best_weights_hidden_output = nn.weights_hidden_output\n",
        "    print(\"Final weights of each neuron:\")\n",
        "    print(\"Input to Hidden Layer Weights:\\n\", best_weights_input_hidden)\n",
        "    print(\"Hidden to Output Layer Weights:\\n\", best_weights_hidden_output)\n",
        "\n",
        "    # Test the trained neural network on the test set\n",
        "    predictions = nn.forward(X_test)\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test_labels, predictions)\n",
        "    print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT8sqTjDFuod",
        "outputId": "92ce0d46-a53b-46f6-86dc-206f0a5017b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a simple neural network\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden = np.dot(X, self.weights_input_hidden)\n",
        "        self.hidden = self.sigmoid(self.hidden)\n",
        "        output = np.dot(self.hidden, self.weights_hidden_output)\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def loss(self, predicted, actual):\n",
        "        return np.mean((predicted - actual) ** 2)\n",
        "\n",
        "    def accuracy(self, predicted, actual):\n",
        "        predicted_labels = np.argmax(predicted, axis=1)\n",
        "        actual_labels = np.argmax(actual, axis=1)\n",
        "        return np.mean(predicted_labels == actual_labels)\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.1):\n",
        "        output_error = y - output\n",
        "        output_delta = output_error * self.sigmoid_derivative(output)\n",
        "\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden)\n",
        "\n",
        "        # Update weights\n",
        "        self.weights_hidden_output += self.hidden.T.dot(output_delta) * learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ondsnGCxIBmW"
      },
      "outputs": [],
      "source": [
        "# Define the PSO optimizer\n",
        "class PSO:\n",
        "    def __init__(self, nn, num_particles, max_iter, X, y):\n",
        "        self.nn = nn\n",
        "        self.num_particles = num_particles\n",
        "        self.max_iter = max_iter\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Initialize particles\n",
        "        self.particles = [self.initialize_particle() for _ in range(num_particles)]\n",
        "        self.global_best_position = self.particles[0]['position']\n",
        "        self.global_best_loss = np.inf\n",
        "\n",
        "    def initialize_particle(self):\n",
        "        particle = {\n",
        "            'position': {\n",
        "                'input_hidden': np.random.randn(*self.nn.weights_input_hidden.shape),\n",
        "                'hidden_output': np.random.randn(*self.nn.weights_hidden_output.shape),\n",
        "            },\n",
        "            'velocity': {\n",
        "                'input_hidden': np.random.randn(*self.nn.weights_input_hidden.shape),\n",
        "                'hidden_output': np.random.randn(*self.nn.weights_hidden_output.shape),\n",
        "            },\n",
        "            'best_position': None,\n",
        "            'best_loss': np.inf,\n",
        "        }\n",
        "        return particle\n",
        "\n",
        "    def update_velocity(self, particle):\n",
        "        w = 0.5  # inertia weight\n",
        "        c1 = 1.5  # cognitive coefficient\n",
        "        c2 = 1.5  # social coefficient\n",
        "\n",
        "        r1 = np.random.rand()\n",
        "        r2 = np.random.rand()\n",
        "\n",
        "        for layer in ['input_hidden', 'hidden_output']:\n",
        "            cognitive_component = c1 * r1 * (particle['best_position'][layer] - particle['position'][layer])\n",
        "            social_component = c2 * r2 * (self.global_best_position[layer] - particle['position'][layer])\n",
        "            particle['velocity'][layer] = w * particle['velocity'][layer] + cognitive_component + social_component\n",
        "\n",
        "    def update_position(self, particle):\n",
        "        for layer in ['input_hidden', 'hidden_output']:\n",
        "            particle['position'][layer] += particle['velocity'][layer]\n",
        "\n",
        "    def optimize(self):\n",
        "        for _ in range(self.max_iter):\n",
        "            for particle in self.particles:\n",
        "                # Set the weights of the neural network to the current particle's position\n",
        "                self.nn.weights_input_hidden = particle['position']['input_hidden']\n",
        "                self.nn.weights_hidden_output = particle['position']['hidden_output']\n",
        "\n",
        "                # Calculate the loss\n",
        "                predictions = self.nn.forward(self.X)\n",
        "                loss = self.nn.loss(predictions, self.y)\n",
        "\n",
        "                # Update the particle's best known position\n",
        "                if loss < particle['best_loss']:\n",
        "                    particle['best_loss'] = loss\n",
        "                    particle['best_position'] = particle['position'].copy()\n",
        "\n",
        "                # Update the global best position\n",
        "                if loss < self.global_best_loss:\n",
        "                    self.global_best_loss = loss\n",
        "                    self.global_best_position = particle['position'].copy()\n",
        "\n",
        "                # Update particle velocity and position\n",
        "                self.update_velocity(particle)\n",
        "                self.update_position(particle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v-kDjZBIG1_",
        "outputId": "16b598af-0584-47d9-8117-9bc7ec589ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input to hidden :  [[-0.25264669 -0.69681988  0.33272721  0.38724521 -2.62785346]\n",
            " [-1.17553141  0.25937668  0.2222974  -1.01278395 -0.46192258]\n",
            " [ 2.23943106  1.7203973   2.05641372 -0.37553394 -2.37585509]\n",
            " [ 1.95065035  1.68930211 -0.74522366 -2.13866962 -0.99714598]]\n",
            "Hidden to output :  [[-3.01351443  0.08301822 -1.88256106]\n",
            " [-0.68960396  0.38167036  1.57796096]\n",
            " [-0.31145617 -0.85431881  2.12294858]\n",
            " [-0.29163233  0.06608375 -2.14107325]\n",
            " [ 2.03880414 -0.71511894 -1.5317172 ]]\n",
            "Epoch 1/10, Accuracy: 90.00%\n",
            "Input to hidden :  [[-0.02810958 -1.00366896  0.06825572  0.75286987 -2.29027026]\n",
            " [-1.92976141 -0.15408346  0.76090475 -0.64214536  0.76348708]\n",
            " [ 2.63087367  1.72842998  2.21239534 -0.51946416 -2.56631925]\n",
            " [ 2.1773357   1.77070088  0.11278722 -2.40874962 -1.08443121]]\n",
            "Hidden to output :  [[-4.0432121   1.09963444 -1.78421864]\n",
            " [-1.29715098 -0.21665708  1.97919737]\n",
            " [-0.86719036 -1.43220249  1.84628562]\n",
            " [ 0.03605846  0.53936041 -3.36604586]\n",
            " [ 2.50970877 -1.57353163 -2.51305896]]\n",
            "Epoch 2/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.2776895  -1.22177474 -0.32342058  0.7625995  -1.86992225]\n",
            " [-2.3182301  -0.04177939  1.21013369 -0.33236722  2.13334904]\n",
            " [ 3.12423093  1.55571142  2.02613626 -0.72925829 -2.65466684]\n",
            " [ 2.5384114   1.67291421  0.51676708 -2.60015995 -1.00937873]]\n",
            "Hidden to output :  [[-4.13715304  2.09400963 -1.86825154]\n",
            " [-1.34698135 -0.82241726  2.28529864]\n",
            " [-0.81389975 -2.51182378  1.77480584]\n",
            " [ 0.43310856  1.08315698 -4.00684647]\n",
            " [ 2.99526392 -2.23058686 -2.94352644]]\n",
            "Epoch 3/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.43703478 -1.05935226 -0.56452482  0.5931058  -1.56561236]\n",
            " [-2.73651073  0.18322352  1.19906799 -0.2932218   2.9286511 ]\n",
            " [ 3.59511958  1.62137538  2.05264139 -0.82448731 -2.65194431]\n",
            " [ 2.90309457  1.78957789  1.06027986 -2.63137342 -0.94043967]]\n",
            "Hidden to output :  [[-4.18231709  3.05433743 -2.0045951 ]\n",
            " [-1.37278658 -1.28698113  2.47729434]\n",
            " [-0.79669552 -3.07231334  1.67460838]\n",
            " [ 0.52377103  0.70432713 -4.62841364]\n",
            " [ 3.09992794 -3.82916831 -3.46661257]]\n",
            "Epoch 4/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.58148896 -1.04001836 -0.62780705  0.57260638 -1.41447209]\n",
            " [-3.03257634  0.3511286   0.81777423 -0.11525769  3.53972083]\n",
            " [ 4.02910662  1.60439385  2.30602463 -0.8157736  -2.84240657]\n",
            " [ 3.23025933  1.8573378   1.70804003 -2.58922521 -1.01165442]]\n",
            "Hidden to output :  [[-4.20637148  3.78854731 -2.16497944]\n",
            " [-1.38436891 -1.76562226  2.70829421]\n",
            " [-0.78699834 -3.42018279  1.55445475]\n",
            " [ 0.59491763  1.80671284 -5.12822631]\n",
            " [ 3.18135612 -3.69618078 -3.88918471]]\n",
            "Epoch 5/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.65117565 -0.93689927 -0.80719806  0.39813099 -1.48706622]\n",
            " [-3.14960909  0.30376773  0.71995659 -0.0921993   3.58565213]\n",
            " [ 4.38638227  1.6034475   1.9845889  -0.78923186 -3.08894731]\n",
            " [ 3.49830365  1.87314648  1.54988573 -2.52998051 -1.16167807]]\n",
            "Hidden to output :  [[-4.2235544   4.1776483  -2.36165611]\n",
            " [-1.39333576 -2.11925381  2.8844665 ]\n",
            " [-0.79540207 -3.68947386  1.69760913]\n",
            " [ 0.63641213  1.9582159  -5.58265413]\n",
            " [ 3.22695341 -3.99508681 -4.20490576]]\n",
            "Epoch 6/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.64217573 -0.85684512 -0.75004875  0.30466309 -1.4774403 ]\n",
            " [-3.2181962   0.21519582  0.56312876  0.00514973  3.64563351]\n",
            " [ 4.5467995   1.58262812  1.91470132 -0.70455808 -3.20840217]\n",
            " [ 3.61864511  1.82784438  1.51163653 -2.37091752 -1.22886385]]\n",
            "Hidden to output :  [[-4.23143747  4.40638087 -2.45351194]\n",
            " [-1.39762092 -2.25570349  2.95462549]\n",
            " [-0.79893257 -3.81829581  1.76344365]\n",
            " [ 0.65455302  2.12013864 -5.78014132]\n",
            " [ 3.24749385 -4.07127867 -4.32650808]]\n",
            "Epoch 7/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.61252951 -0.71435982 -0.61494319  0.0808493  -1.42965897]\n",
            " [-3.36244607  0.1211138   0.34118279  0.14627159  3.76464333]\n",
            " [ 4.88286847  1.49102236  1.72863855 -0.53737503 -3.48554387]\n",
            " [ 3.86912049  1.69798583  1.40474332 -2.03867622 -1.39535164]]\n",
            "Hidden to output :  [[-4.24740641  4.88386873 -2.63239648]\n",
            " [-1.40659486 -2.5870805   3.14798149]\n",
            " [-0.80658042 -4.14095382  1.94895425]\n",
            " [ 0.69217763  2.41900162 -6.21269725]\n",
            " [ 3.28980858 -4.32122246 -4.55963256]]\n",
            "Epoch 8/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.59730734 -0.68127641 -0.56152562  0.00936756 -1.39173002]\n",
            " [-3.41315141  0.10927271  0.27960758  0.19476269  3.80266238]\n",
            " [ 5.0095364   1.44301966  1.64715944 -0.48545166 -3.61571308]\n",
            " [ 3.96218179  1.64292808  1.36114517 -1.92835962 -1.48489444]]\n",
            "Hidden to output :  [[-4.2532052   5.0722208  -2.70684738]\n",
            " [-1.40979964 -2.73470403  3.24062769]\n",
            " [-0.80946515 -4.28866672  2.04085803]\n",
            " [ 0.70647075  2.54225884 -6.39335659]\n",
            " [ 3.30597854 -4.44405146 -4.62861257]]\n",
            "Epoch 9/10, Accuracy: 96.67%\n",
            "Input to hidden :  [[ 0.47823106 -0.56114297 -0.22590011 -0.33894751 -1.07675359]\n",
            " [-3.81576337  0.15758657 -0.13723082  0.50629412  4.04171254]\n",
            " [ 6.0904258   1.16938211  1.06746705 -0.30302534 -4.76163488]\n",
            " [ 4.76580848  1.34370094  1.1420585  -1.36233368 -2.30496014]]\n",
            "Hidden to output :  [[-4.30287456  6.66761884 -3.40722923]\n",
            " [-1.43650379 -4.06292967  4.06436477]\n",
            " [-0.83385651 -5.61698216  2.83133167]\n",
            " [ 0.83035037  3.65377037 -7.95287049]\n",
            " [ 3.44663583 -5.4746827  -5.19699366]]\n",
            "Epoch 10/10, Accuracy: 100.00%\n",
            "Final Test loss: 0.02179164667354648\n",
            "Final Test accuracy: 100.0 %\n"
          ]
        }
      ],
      "source": [
        "# Example usage with epochs\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the neural network and PSO optimizer\n",
        "    nn = NeuralNetwork(input_size=4, hidden_size=5, output_size=3)\n",
        "    pso = PSO(nn, num_particles=20, max_iter=100, X=X_train, y=y_train)\n",
        "\n",
        "    # Run the optimization with epochs\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        pso.optimize()\n",
        "\n",
        "        # After each epoch, perform a backward pass to refine the weights\n",
        "        for _ in range(10):  # Perform 10 steps of backpropagation after each epoch\n",
        "            output = nn.forward(X_train)\n",
        "            nn.backward(X_train, y_train, output)\n",
        "\n",
        "        # Save weights after each epoch\n",
        "        np.save(f'weights_input_hidden_epoch_{epoch}.npy', nn.weights_input_hidden)\n",
        "        print(\"Input to hidden : \",nn.weights_input_hidden)\n",
        "        np.save(f'weights_hidden_output_epoch_{epoch}.npy', nn.weights_hidden_output)\n",
        "        print(\"Hidden to output : \", nn.weights_hidden_output)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predictions = nn.forward(X_test)\n",
        "        accuracy = nn.accuracy(predictions, y_test)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Final test loss and accuracy\n",
        "    predictions = nn.forward(X_test)\n",
        "    test_loss = nn.loss(predictions, y_test)\n",
        "    test_accuracy = nn.accuracy(predictions, y_test)\n",
        "\n",
        "    print(\"Final Test loss:\", test_loss)\n",
        "    print(\"Final Test accuracy:\", test_accuracy * 100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZSJaooDIcv1",
        "outputId": "abf49451-d363-4cdb-ae4c-2265e16a162d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-model-optimization --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Yl76XcY2WO",
        "outputId": "984f9652-2dfd-4f24-aa08-e92c9fa316c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyswarm\n",
            "  Downloading pyswarm-0.6.tar.gz (4.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyswarm) (1.26.4)\n",
            "Building wheels for collected packages: pyswarm\n",
            "  Building wheel for pyswarm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyswarm: filename=pyswarm-0.6-py3-none-any.whl size=4464 sha256=8d995a6db30435de908baa409682fd9a0b651df3fb3a4b25cce87b78b97cc69f\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/40/62fa158f497f942277cbab8199b05cb61c571ab324e67ad0d6\n",
            "Successfully built pyswarm\n",
            "Installing collected packages: pyswarm\n",
            "Successfully installed pyswarm-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyswarm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKSV6e9HMVsj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from pyswarm import pso\n",
        "import h5py\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# One-hot encode the target labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# PSO Objective Function\n",
        "def pso_objective(params,weights_filepath):\n",
        "    lr = params[0]\n",
        "    units1 = int(params[1])\n",
        "    units2 = int(params[2])\n",
        "\n",
        "    # Build neural network model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units1, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(units2, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer (3 classes)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    class WeightsSaver(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            with h5py.File(weights_filepath, 'a') as f:\n",
        "                group = f.create_group(f'epoch_{epoch + 1}')\n",
        "                for layer_idx, layer in enumerate(self.model.layers):\n",
        "                    weights = layer.get_weights()\n",
        "                    group.create_dataset(f'layer_{layer_idx}_weights', data=weights[0])\n",
        "                    group.create_dataset(f'layer_{layer_idx}_biases', data=weights[1])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=0, callbacks=[WeightsSaver()])\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return -accuracy  # PSO minimizes the objective function, so return negative accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "ucwZEAsTMaDq",
        "outputId": "b9bdb843-2f84-4bbe-b284-d0c375a3a57b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to synchronously create group (name already exists)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5323058dca6c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run PSO to find optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpso_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswarmsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print the best parameters and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ff46302e3f72>\u001b[0m in \u001b[0;36mpso_objective\u001b[0;34m(params, weights_filepath)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWeightsSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ff46302e3f72>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch_{epoch + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_group\u001b[0;34m(self, name, track_order)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mgcpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcpl_crt_order\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrack_order\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mgid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mGroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to synchronously create group (name already exists)"
          ]
        }
      ],
      "source": [
        "# PSO parameters and bounds\n",
        "lb = [1e-5, 16, 16]  # Lower bounds: learning rate, units1, units2\n",
        "ub = [1e-2, 128, 128]  # Upper bounds\n",
        "\n",
        "# Save the weights for each epoch to a .h5 file\n",
        "weights_filepath = 'model_weights.h5'\n",
        "\n",
        "# Run PSO to find optimal hyperparameters\n",
        "best_params, best_accuracy = pso(pso_objective(lb, weights_filepath), lb, ub, swarmsize=10, maxiter=5)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best Parameters: Learning rate={best_params[0]}, Units1={best_params[1]}, Units2={best_params[2]}\")\n",
        "print(f\"Best Accuracy achieved: {-best_accuracy}\")\n",
        "\n",
        "# Load and print weights from the HDF5 file (optional, just for checking)\n",
        "with h5py.File('model_weights.h5', 'r') as f:\n",
        "    for epoch in range(1, 11):\n",
        "        print(f\"\\nWeights from epoch {epoch}:\")\n",
        "        epoch_group = f[f'epoch_{epoch}']\n",
        "        for layer in epoch_group:\n",
        "            print(f\"{layer}: {epoch_group[layer][()]}\")  # Print the weights for each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywh4W0iQXVnx",
        "outputId": "70365859-8895-43a3-fb5e-c012e3ed6a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7076 - loss: 0.5125Saved weights for epoch 1 to saved_weights/weights_epoch_1.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7079 - loss: 0.5122 - val_accuracy: 0.8599 - val_loss: 0.3231\n",
            "Epoch 2/10\n",
            "\u001b[1m376/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9719 - loss: 0.0871Saved weights for epoch 2 to saved_weights/weights_epoch_2.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.0869 - val_accuracy: 0.8479 - val_loss: 0.4170\n",
            "Epoch 3/10\n",
            "\u001b[1m373/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0113Saved weights for epoch 3 to saved_weights/weights_epoch_3.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9979 - loss: 0.0112 - val_accuracy: 0.8460 - val_loss: 0.5243\n",
            "Epoch 4/10\n",
            "\u001b[1m387/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0016Saved weights for epoch 4 to saved_weights/weights_epoch_4.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.8519 - val_loss: 0.5465\n",
            "Epoch 5/10\n",
            "\u001b[1m372/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.9709e-04Saved weights for epoch 5 to saved_weights/weights_epoch_5.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.9609e-04 - val_accuracy: 0.8542 - val_loss: 0.5630\n",
            "Epoch 6/10\n",
            "\u001b[1m371/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.6814e-04Saved weights for epoch 6 to saved_weights/weights_epoch_6.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.6760e-04 - val_accuracy: 0.8546 - val_loss: 0.5771\n",
            "Epoch 7/10\n",
            "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.1067e-04Saved weights for epoch 7 to saved_weights/weights_epoch_7.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1064e-04 - val_accuracy: 0.8558 - val_loss: 0.5897\n",
            "Epoch 8/10\n",
            "\u001b[1m377/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 7.9639e-05Saved weights for epoch 8 to saved_weights/weights_epoch_8.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.9461e-05 - val_accuracy: 0.8561 - val_loss: 0.6019\n",
            "Epoch 9/10\n",
            "\u001b[1m388/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.7974e-05Saved weights for epoch 9 to saved_weights/weights_epoch_9.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.7948e-05 - val_accuracy: 0.8568 - val_loss: 0.6130\n",
            "Epoch 10/10\n",
            "\u001b[1m385/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.3066e-05Saved weights for epoch 10 to saved_weights/weights_epoch_10.weights.h5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.3039e-05 - val_accuracy: 0.8570 - val_loss: 0.6240\n",
            "Final accuracy: 0.8569599986076355, Final loss: 0.6239796280860901\n"
          ]
        }
      ],
      "source": [
        "# 1 Line Description for the below code\n",
        "## The code builds and trains a 3-layer ANN on the IMDb dataset for binary sentiment classification, saving model weights after each epoch.\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "# Load and preprocess the IMDb dataset\n",
        "max_features = 10000  # Vocabulary size\n",
        "maxlen = 200  # Max length of the review\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Create the directory to save weights if it doesn't exist\n",
        "if not os.path.exists('saved_weights'):\n",
        "    os.makedirs('saved_weights')\n",
        "\n",
        "# Define the ANN model with 3 layers\n",
        "def create_ann():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "    model.add(Flatten())  # Converts 3D sequence to 2D\n",
        "    model.add(Dense(64, activation='relu'))  # First hidden layer\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Custom callback to save weights after each epoch\n",
        "class SaveWeightsCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save model weights at the end of each epoch\n",
        "        weight_file = f\"saved_weights/weights_epoch_{epoch+1}.weights.h5\"\n",
        "        self.model.save_weights(weight_file)\n",
        "        print(f\"Saved weights for epoch {epoch+1} to {weight_file}\")\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_ann()\n",
        "\n",
        "# Train the model for 10 epochs and save weights after each epoch\n",
        "save_weights_callback = SaveWeightsCallback()\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), callbacks=[save_weights_callback])\n",
        "\n",
        "# Evaluate final model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Final accuracy: {accuracy}, Final loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2O3dys2Sic6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}