{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Bat Algorithm on ANN which is trained on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEgFQrLEiFa7",
    "outputId": "8ae1dd51-ccb9-4eb1-f6d4-e08ae801b297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Best Fitness: 0.0\n",
      "Iteration 2/10, Best Fitness: 0.0\n",
      "Iteration 3/10, Best Fitness: 0.0\n",
      "Iteration 4/10, Best Fitness: 0.0\n",
      "Iteration 5/10, Best Fitness: 0.0\n",
      "Iteration 6/10, Best Fitness: 0.0\n",
      "Iteration 7/10, Best Fitness: 0.0\n",
      "Iteration 8/10, Best Fitness: 0.0\n",
      "Iteration 9/10, Best Fitness: 0.0\n",
      "Iteration 10/10, Best Fitness: 0.0\n",
      "Best hyperparameters found: [1.85448225e+01 5.65977599e-03]\n",
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Final model accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess the IMDB dataset from Hugging Face\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "# Split train into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_texts).toarray()\n",
    "X_val = vectorizer.transform(val_texts).toarray()\n",
    "X_test = vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'inputs': self.texts[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = IMDBDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = IMDBDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = IMDBDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define the ANN model\n",
    "class ANNModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_units)\n",
    "        self.fc2 = torch.nn.Linear(hidden_units, hidden_units)\n",
    "        self.fc3 = torch.nn.Linear(hidden_units, output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def create_model(hidden_units, learning_rate):\n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "    output_dim = 2  # Binary classification\n",
    "    model = ANNModel(input_dim, int(hidden_units), output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "# Objective Function: Minimize Validation Loss\n",
    "def objective_function(hyperparams):\n",
    "    hidden_units = hyperparams[0]\n",
    "    learning_rate = hyperparams[1]\n",
    "\n",
    "    model, optimizer = create_model(hidden_units, learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['inputs']\n",
    "        labels = batch['labels']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['inputs']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(inputs)\n",
    "            preds += torch.argmax(outputs, dim=1).tolist()\n",
    "            labels_list += labels.tolist()\n",
    "\n",
    "    accuracy = accuracy_score(labels_list, preds)\n",
    "    return 1 - accuracy  # Minimize this value\n",
    "\n",
    "# Bat Algorithm Implementation\n",
    "class BatAlgorithm:\n",
    "    def __init__(self, num_bats, max_iter, bounds, alpha=0.9, gamma=0.9):\n",
    "        self.num_bats = num_bats\n",
    "        self.max_iter = max_iter\n",
    "        self.bounds = bounds\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Initialize bats\n",
    "        self.bats = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_bats, bounds.shape[0]))\n",
    "        self.velocities = np.zeros((num_bats, bounds.shape[0]))\n",
    "        self.fitness = np.zeros(num_bats)\n",
    "        self.frequencies = np.zeros(num_bats)\n",
    "\n",
    "        # Best bat initialization\n",
    "        self.best_bat = self.bats[0]\n",
    "        self.best_fitness = float(\"inf\")\n",
    "\n",
    "    def optimize(self, objective_function):\n",
    "        for iter in range(self.max_iter):\n",
    "            for i in range(self.num_bats):\n",
    "                # Frequency update\n",
    "                self.frequencies[i] = np.random.uniform(0, 1)\n",
    "                # Velocity update\n",
    "                self.velocities[i] = self.velocities[i] + (self.bats[i] - self.best_bat) * self.frequencies[i]\n",
    "                # Position update\n",
    "                candidate_bat = self.bats[i] + self.velocities[i]\n",
    "                candidate_bat = np.clip(candidate_bat, self.bounds[:, 0], self.bounds[:, 1])\n",
    "\n",
    "                # Evaluate candidate\n",
    "                fitness = objective_function(candidate_bat)\n",
    "\n",
    "                # Update if new solution is better\n",
    "                if fitness < self.fitness[i]:\n",
    "                    self.bats[i] = candidate_bat\n",
    "                    self.fitness[i] = fitness\n",
    "\n",
    "                # Update the best bat\n",
    "                if self.fitness[i] < self.best_fitness:\n",
    "                    self.best_bat = self.bats[i]\n",
    "                    self.best_fitness = self.fitness[i]\n",
    "\n",
    "            print(f\"Iteration {iter+1}/{self.max_iter}, Best Fitness: {self.best_fitness}\")\n",
    "\n",
    "        return self.best_bat\n",
    "\n",
    "# Hyperparameter bounds: hidden units (4 to 64), learning rate (1e-6 to 1e-2)\n",
    "bounds = np.array([[4, 64], [1e-6, 1e-2]])\n",
    "\n",
    "# Initialize and run Bat Algorithm\n",
    "bat_algorithm = BatAlgorithm(num_bats=10, max_iter=10, bounds=bounds, alpha=0.9, gamma=0.9)\n",
    "best_hyperparams = bat_algorithm.optimize(objective_function)\n",
    "print(\"Best hyperparameters found:\", best_hyperparams)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "best_hidden_units, best_lr = best_hyperparams\n",
    "final_model, final_optimizer = create_model(best_hidden_units, best_lr)\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train and Evaluate Final Model with Best Hyperparameters\n",
    "for epoch in range(3):  # You can increase the number of epochs if needed\n",
    "    final_model.train()\n",
    "    for batch in final_train_loader:\n",
    "        inputs = batch['inputs']\n",
    "        labels = batch['labels']\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "# Evaluate final model\n",
    "final_model.eval()\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "preds = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['inputs']\n",
    "        labels = batch['labels']\n",
    "        outputs = final_model(inputs)\n",
    "        preds += torch.argmax(outputs, dim=1).tolist()\n",
    "        labels_list += labels.tolist()\n",
    "\n",
    "accuracy = accuracy_score(labels_list, preds)\n",
    "print(f\"Final model accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Bat Algorithm on ANN which is trained on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3_Lp_wteAHX",
    "outputId": "b1cfc2e6-5a24-42ed-8034-fff1f275cca7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Best Fitness: 0.0\n",
      "Iteration 2/10, Best Fitness: 0.0\n",
      "Iteration 3/10, Best Fitness: 0.0\n",
      "Iteration 4/10, Best Fitness: 0.0\n",
      "Iteration 5/10, Best Fitness: 0.0\n",
      "Iteration 6/10, Best Fitness: 0.0\n",
      "Iteration 7/10, Best Fitness: 0.0\n",
      "Iteration 8/10, Best Fitness: 0.0\n",
      "Iteration 9/10, Best Fitness: 0.0\n",
      "Iteration 10/10, Best Fitness: 0.0\n",
      "Best hyperparameters found: [1.87425958e+01 8.94280738e-03]\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5079 - loss: 0.8729\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7962 - loss: 0.5848 \n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7990 - loss: 0.4610 \n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8223 - loss: 0.3952 \n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8083 - loss: 0.3553 \n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8846 - loss: 0.2933 \n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9285 - loss: 0.2634 \n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9240 - loss: 0.2305 \n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9013 - loss: 0.2078 \n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9685 - loss: 0.1562 \n",
      "Final model accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a model function\n",
    "def create_model(hidden_units, learning_rate):\n",
    "    model = Sequential([\n",
    "        Dense(int(hidden_units), input_dim=4, activation='relu'),\n",
    "        Dense(int(hidden_units), activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Objective Function: Minimize Validation Loss\n",
    "def objective_function(hyperparams):\n",
    "    hidden_units = int(round(hyperparams[0]))  # Round and convert to integer\n",
    "    learning_rate = hyperparams[1]\n",
    "\n",
    "    model = create_model(hidden_units, learning_rate)\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=32, verbose=0)  # Train for one epoch\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return 1 - val_accuracy  # Minimize this value\n",
    "\n",
    "# Bat Algorithm Implementation\n",
    "class BatAlgorithm:\n",
    "    def __init__(self, num_bats, max_iter, bounds, alpha=0.9, gamma=0.9):\n",
    "        self.num_bats = num_bats\n",
    "        self.max_iter = max_iter\n",
    "        self.bounds = bounds\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Initialize bats\n",
    "        self.bats = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_bats, bounds.shape[0]))\n",
    "        self.velocities = np.zeros((num_bats, bounds.shape[0]))\n",
    "        self.fitness = np.zeros(num_bats)\n",
    "        self.frequencies = np.zeros(num_bats)\n",
    "\n",
    "        # Best bat initialization\n",
    "        self.best_bat = self.bats[0]\n",
    "        self.best_fitness = float(\"inf\")\n",
    "\n",
    "    def optimize(self, objective_function):\n",
    "        for iter in range(self.max_iter):\n",
    "            for i in range(self.num_bats):\n",
    "                # Frequency update\n",
    "                self.frequencies[i] = np.random.uniform(0, 1)\n",
    "                # Velocity update\n",
    "                self.velocities[i] = self.velocities[i] + (self.bats[i] - self.best_bat) * self.frequencies[i]\n",
    "                # Position update\n",
    "                candidate_bat = self.bats[i] + self.velocities[i]\n",
    "                candidate_bat = np.clip(candidate_bat, self.bounds[:, 0], self.bounds[:, 1])\n",
    "\n",
    "                # Evaluate candidate\n",
    "                fitness = objective_function(candidate_bat)\n",
    "\n",
    "                # Update if new solution is better\n",
    "                if fitness < self.fitness[i]:\n",
    "                    self.bats[i] = candidate_bat\n",
    "                    self.fitness[i] = fitness\n",
    "\n",
    "                # Update the best bat\n",
    "                if self.fitness[i] < self.best_fitness:\n",
    "                    self.best_bat = self.bats[i]\n",
    "                    self.best_fitness = self.fitness[i]\n",
    "\n",
    "            print(f\"Iteration {iter+1}/{self.max_iter}, Best Fitness: {self.best_fitness}\")\n",
    "\n",
    "        return self.best_bat\n",
    "\n",
    "# Hyperparameter bounds: hidden units (4 to 64), learning rate (1e-6 to 1e-2)\n",
    "bounds = np.array([[4, 64], [1e-6, 1e-2]])\n",
    "\n",
    "# Initialize and run Bat Algorithm\n",
    "bat_algorithm = BatAlgorithm(num_bats=10, max_iter=10, bounds=bounds, alpha=0.9, gamma=0.9)\n",
    "best_hyperparams = bat_algorithm.optimize(objective_function)\n",
    "print(\"Best hyperparameters found:\", best_hyperparams)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "best_hidden_units, best_lr = best_hyperparams\n",
    "final_model = create_model(best_hidden_units, best_lr)\n",
    "final_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate final model\n",
    "test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Final model accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svzKFbbuGKo8",
    "outputId": "be2b6f7d-5f1e-47a0-9b15-ac45004c0e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Best Fitness: 0.0\n",
      "Iteration 2/10, Best Fitness: 0.0\n",
      "Iteration 3/10, Best Fitness: 0.0\n",
      "Iteration 4/10, Best Fitness: 0.0\n",
      "Iteration 5/10, Best Fitness: 0.0\n",
      "Iteration 6/10, Best Fitness: 0.0\n",
      "Iteration 7/10, Best Fitness: 0.0\n",
      "Iteration 8/10, Best Fitness: 0.0\n",
      "Iteration 9/10, Best Fitness: 0.0\n",
      "Iteration 10/10, Best Fitness: 0.0\n",
      "Best hyperparameters found: [5.16831170e+01 3.94161812e-03]\n",
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Final model accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load and preprocess the IMDB dataset\n",
    "# dataset = load_dataset('imdb')\n",
    "\n",
    "# # Preprocessing and feature extraction\n",
    "# train_texts = dataset['train']['text']\n",
    "# train_labels = dataset['train']['label']\n",
    "# test_texts = dataset['test']['text']\n",
    "# test_labels = dataset['test']['label']\n",
    "\n",
    "# # Split train into train and validation sets\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Vectorize the text data\n",
    "# vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "# X_train = vectorizer.fit_transform(train_texts).toarray()\n",
    "# X_val = vectorizer.transform(val_texts).toarray()\n",
    "# X_test = vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "# y_val_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "# y_test_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# class IMDBDataset(Dataset):\n",
    "#     def __init__(self, texts, labels):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'inputs': self.texts[idx],\n",
    "#             'labels': self.labels[idx]\n",
    "#         }\n",
    "\n",
    "# # Prepare datasets and dataloaders\n",
    "# train_dataset = IMDBDataset(X_train_tensor, y_train_tensor)\n",
    "# val_dataset = IMDBDataset(X_val_tensor, y_val_tensor)\n",
    "# test_dataset = IMDBDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# # Define the ANN model\n",
    "# class ANNModel(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_units, output_dim, weights=None):\n",
    "#         super(ANNModel, self).__init__()\n",
    "#         self.fc1 = torch.nn.Linear(input_dim, hidden_units)\n",
    "#         self.fc2 = torch.nn.Linear(hidden_units, hidden_units)\n",
    "#         self.fc3 = torch.nn.Linear(hidden_units, output_dim)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "\n",
    "#         # Assign the custom weights to the first layer if provided\n",
    "#         if weights is not None:\n",
    "#             with torch.no_grad():\n",
    "#                 self.fc1.weight.copy_(weights)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# def create_model(hidden_units, learning_rate, weights=None):\n",
    "#     input_dim = X_train.shape[1]  # Number of features\n",
    "#     output_dim = 2  # Binary classification\n",
    "#     model = ANNModel(input_dim, int(hidden_units), output_dim, weights)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     return model, optimizer\n",
    "\n",
    "# # Objective Function: Minimize Validation Loss\n",
    "# def objective_function(hyperparams):\n",
    "#     hidden_units = int(hyperparams[0])\n",
    "#     learning_rate = hyperparams[1]\n",
    "\n",
    "#     # Initialize weights with correct dimensions: (hidden_units, input_dim)\n",
    "#     weights = torch.randn((hidden_units, X_train.shape[1]), dtype=torch.float32)\n",
    "\n",
    "#     model, optimizer = create_model(hidden_units, learning_rate, weights)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Train the model for one epoch\n",
    "#     model.train()\n",
    "#     for batch in train_loader:\n",
    "#         inputs = batch['inputs']\n",
    "#         labels = batch['labels']\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Evaluate on validation set\n",
    "#     model.eval()\n",
    "#     preds = []\n",
    "#     labels_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             inputs = batch['inputs']\n",
    "#             labels = batch['labels']\n",
    "#             outputs = model(inputs)\n",
    "#             preds += torch.argmax(outputs, dim=1).tolist()\n",
    "#             labels_list += labels.tolist()\n",
    "\n",
    "#     accuracy = accuracy_score(labels_list, preds)\n",
    "#     return 1 - accuracy  # Minimize this value\n",
    "\n",
    "# # Bat Algorithm Implementation\n",
    "# class BatAlgorithm:\n",
    "#     def __init__(self, num_bats, max_iter, bounds, alpha=0.9, gamma=0.9):\n",
    "#         self.num_bats = num_bats\n",
    "#         self.max_iter = max_iter\n",
    "#         self.bounds = bounds\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#         # Initialize bats\n",
    "#         self.bats = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_bats, bounds.shape[0]))\n",
    "#         self.velocities = np.zeros((num_bats, bounds.shape[0]))\n",
    "#         self.fitness = np.zeros(num_bats)\n",
    "#         self.frequencies = np.zeros(num_bats)\n",
    "\n",
    "#         # Best bat initialization\n",
    "#         self.best_bat = self.bats[0]\n",
    "#         self.best_fitness = float(\"inf\")\n",
    "\n",
    "#     def optimize(self, objective_function):\n",
    "#         for iter in range(self.max_iter):\n",
    "#             for i in range(self.num_bats):\n",
    "#                 # Frequency update\n",
    "#                 self.frequencies[i] = np.random.uniform(0, 1)\n",
    "#                 # Velocity update\n",
    "#                 self.velocities[i] = self.velocities[i] + (self.bats[i] - self.best_bat) * self.frequencies[i]\n",
    "#                 # Position update\n",
    "#                 candidate_bat = self.bats[i] + self.velocities[i]\n",
    "#                 candidate_bat = np.clip(candidate_bat, self.bounds[:, 0], self.bounds[:, 1])\n",
    "\n",
    "#                 # Evaluate candidate\n",
    "#                 fitness = objective_function(candidate_bat)\n",
    "\n",
    "#                 # Update if new solution is better\n",
    "#                 if fitness < self.fitness[i]:\n",
    "#                     self.bats[i] = candidate_bat\n",
    "#                     self.fitness[i] = fitness\n",
    "\n",
    "#                 # Update the best bat\n",
    "#                 if self.fitness[i] < self.best_fitness:\n",
    "#                     self.best_bat = self.bats[i]\n",
    "#                     self.best_fitness = self.fitness[i]\n",
    "\n",
    "#             print(f\"Iteration {iter+1}/{self.max_iter}, Best Fitness: {self.best_fitness}\")\n",
    "\n",
    "#         return self.best_bat\n",
    "\n",
    "# # Hyperparameter bounds: hidden units (4 to 64), learning rate (1e-6 to 1e-2)\n",
    "# bounds = np.array([[4, 64], [1e-6, 1e-2]])\n",
    "\n",
    "# # Initialize and run Bat Algorithm\n",
    "# bat_algorithm = BatAlgorithm(num_bats=10, max_iter=10, bounds=bounds, alpha=0.9, gamma=0.9)\n",
    "# best_hyperparams = bat_algorithm.optimize(objective_function)\n",
    "# print(\"Best hyperparameters found:\", best_hyperparams)\n",
    "\n",
    "# # Extract the best hyperparameters\n",
    "# best_hidden_units = int(best_hyperparams[0])\n",
    "# best_lr = best_hyperparams[1]\n",
    "\n",
    "# # Initialize final model with the best hyperparameters\n",
    "# # Initialize weights with correct dimensions: (best_hidden_units, X_train.shape[1])\n",
    "# final_weights = torch.randn((best_hidden_units, X_train.shape[1]), dtype=torch.float32)\n",
    "# final_model, final_optimizer = create_model(best_hidden_units, best_lr, final_weights)\n",
    "\n",
    "# # Train the final model\n",
    "# num_epochs = 3  # You can adjust this based on your needs\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     final_model.train()\n",
    "#     for batch in train_loader:\n",
    "#         inputs = batch['inputs']\n",
    "#         labels = batch['labels']\n",
    "#         final_optimizer.zero_grad()\n",
    "#         outputs = final_model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         final_optimizer.step()\n",
    "#     print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "# # Evaluate the final model on the test set\n",
    "# final_model.eval()\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "# preds = []\n",
    "# labels_list = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         inputs = batch['inputs']\n",
    "#         labels = batch['labels']\n",
    "#         outputs = final_model(inputs)\n",
    "#         preds += torch.argmax(outputs, dim=1).tolist()\n",
    "#         labels_list += labels.tolist()\n",
    "\n",
    "# # Calculate and print accuracy\n",
    "# accuracy = accuracy_score(labels_list, preds)\n",
    "# print(f\"Final model accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
