{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5a8066-127f-4c21-9f7e-22d6c0433090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\NM21AI059\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening on 0.0.0.0:1700...\n",
      "Connected by ('172.16.19.49', 49882)\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "import pickle\n",
    "\n",
    "# Set up the server to communicate with the second part of the model\n",
    "def start_server(host='0.0.0.0', port=1700):\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((host, port))\n",
    "    server_socket.listen(1)  # Listen for 1 connection\n",
    "    print(f\"Server is listening on {host}:{port}...\")\n",
    "\n",
    "    conn, addr = server_socket.accept()  # Accept the connection\n",
    "    print(f\"Connected by {addr}\")\n",
    "\n",
    "    return conn, server_socket\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=2)\n",
    "\n",
    "# Manually split the model: First part of the model (BERT encoder)\n",
    "class BertPart1(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BertPart1, self).__init__()\n",
    "        self.bert = model.bert  # Extract the BERT part of the model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]  # Return hidden states\n",
    "\n",
    "# Create the first part of the model\n",
    "model_part1 = BertPart1(model)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_part1.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model_part1.parameters(), lr=5e-5)\n",
    "\n",
    "# Start the server\n",
    "conn, server_socket = start_server()\n",
    "\n",
    "# Assume input_ids and attention_mask are pre-defined tensors for demonstration\n",
    "# Replace this with your actual DataLoader or batch processing\n",
    "input_ids = torch.randint(0, 1000, (16, 256)).to(device)  # Random input for example\n",
    "attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "# Forward pass and send data to the client\n",
    "for _ in range(10):  # Example loop, replace with actual training loop\n",
    "    optimizer.zero_grad()\n",
    "    hidden_states = model_part1(input_ids, attention_mask)\n",
    "\n",
    "    # Send the output to the client using a socket\n",
    "    data_to_send = pickle.dumps(hidden_states.cpu().detach().numpy())  # Serialize the output\n",
    "    conn.sendall(data_to_send)  # Send to client\n",
    "    print(\"Sent hidden states to client\")\n",
    "\n",
    "# Close the connection and server socket\n",
    "conn.close()\n",
    "server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a5c3ac-4e7d-4231-ace9-d5106842be79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertPart1(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6894be7-06ab-45a0-abef-a0438215705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\NM21AI059\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Exam\\.cache\\huggingface\\hub\\models--huawei-noah--TinyBERT_General_4L_312D. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening on 0.0.0.0:1700...\n",
      "Connected by ('172.16.19.49', 49995)\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pickle\n",
    "\n",
    "# Set up the server to communicate with the second part of the model\n",
    "def start_server(host='0.0.0.0', port=1700):\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((host, port))\n",
    "    server_socket.listen(1)  # Listen for 1 connection\n",
    "    print(f\"Server is listening on {host}:{port}...\")\n",
    "\n",
    "    conn, addr = server_socket.accept()  # Accept the connection\n",
    "    print(f\"Connected by {addr}\")\n",
    "\n",
    "    return conn, server_socket\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=2)\n",
    "\n",
    "# Manually split the model: First half (encoder)\n",
    "class BertPart1(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BertPart1, self).__init__()\n",
    "        self.bert = model.bert  # Extract the BERT part of the model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]  # Return hidden states\n",
    "\n",
    "# Create the first part of the model\n",
    "model_part1 = BertPart1(model)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_part1.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model_part1.parameters(), lr=5e-5)\n",
    "\n",
    "# Start the server\n",
    "conn, server_socket = start_server()\n",
    "\n",
    "# Example data: Replace with your DataLoader or batch processing\n",
    "# Assume input_ids and attention_mask are pre-defined tensors\n",
    "input_ids = torch.randint(0, 1000, (16, 128)).to(device)  # Random input for example\n",
    "attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "# Forward pass and send data to the client\n",
    "for _ in range(10):  # Example loop, replace with actual training loop\n",
    "    optimizer.zero_grad()\n",
    "    hidden_states = model_part1(input_ids, attention_mask)\n",
    "\n",
    "    # Send the output to the client using a socket\n",
    "    data_to_send = pickle.dumps(hidden_states.cpu().detach().numpy())  # Serialize the output\n",
    "    conn.sendall(data_to_send)  # Send to client\n",
    "    print(\"Sent hidden states to client\")\n",
    "\n",
    "# Close the connection and server socket\n",
    "conn.close()\n",
    "server_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cadb94f6-462d-4e7c-b709-f56052c89cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertPart1(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 312)\n",
      "      (token_type_embeddings): Embedding(2, 312)\n",
      "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
      "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b520826c-2257-41a5-85e7-f4b01b4acea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening on 0.0.0.0:1700...\n",
      "Connected by ('172.16.19.49', 50079)\n",
      "Sent hidden states and pooled output to client\n",
      "Sent hidden states and pooled output to client\n",
      "Sent hidden states and pooled output to client\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Set up the server to communicate with the second part of the model\n",
    "def start_server(host='0.0.0.0', port=1700):\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((host, port))\n",
    "    server_socket.listen(1)  # Listen for 1 connection\n",
    "    print(f\"Server is listening on {host}:{port}...\")\n",
    "\n",
    "    conn, addr = server_socket.accept()  # Accept the connection\n",
    "    print(f\"Connected by {addr}\")\n",
    "\n",
    "    return conn, server_socket\n",
    "\n",
    "# Custom Dataset for IMDb\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = [\n",
    "            \"This movie was great!\", \"I did not enjoy this film.\", \n",
    "            \"It was an average movie.\", \"The plot was fantastic!\", \n",
    "            \"I wouldn't recommend it.\", \"It was a waste of time.\"\n",
    "        ]\n",
    "        self.labels = [1, 0, 0, 1, 0, 0]  # Sample labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.sentences[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=2)\n",
    "\n",
    "# Manually split the model: Encoder and pooler\n",
    "class BertPart1(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BertPart1, self).__init__()\n",
    "        self.bert = model.bert  # Extract the BERT part of the model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state, outputs.pooler_output  # Return hidden states and pooled output\n",
    "\n",
    "# Create the first part of the model\n",
    "model_part1 = BertPart1(model)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_part1.to(device)\n",
    "\n",
    "# Start the server\n",
    "conn, server_socket = start_server()\n",
    "\n",
    "# Create a DataLoader for the IMDb dataset\n",
    "dataset = IMDBDataset(tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Forward pass and send data to the client\n",
    "for batch in data_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    hidden_states, pooled_output = model_part1(input_ids, attention_mask)\n",
    "\n",
    "    # Send the output to the client using a socket\n",
    "    data_to_send = {\n",
    "        'hidden_states': hidden_states.cpu().detach().numpy(), \n",
    "        'pooled_output': pooled_output.cpu().detach().numpy()\n",
    "    }\n",
    "    conn.sendall(pickle.dumps(data_to_send))  # Send to client\n",
    "    print(\"Sent hidden states and pooled output to client\")\n",
    "\n",
    "# Close the connection and server socket\n",
    "conn.close()\n",
    "server_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a84b7acf-0ac6-43ac-a0c2-f41bdbca887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertPart1(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 312)\n",
      "      (token_type_embeddings): Embedding(2, 312)\n",
      "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
      "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bd99f-e2cd-441e-8202-4a8dbd90b0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening on 0.0.0.0:1700...\n",
      "Connected by ('172.16.19.49', 50106)\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n",
      "Sent hidden states to client\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set up the server to communicate with the second part of the model\n",
    "def start_server(host='0.0.0.0', port=1700):\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((host, port))\n",
    "    server_socket.listen(1)  # Listen for 1 connection\n",
    "    print(f\"Server is listening on {host}:{port}...\")\n",
    "\n",
    "    conn, addr = server_socket.accept()  # Accept the connection\n",
    "    print(f\"Connected by {addr}\")\n",
    "\n",
    "    return conn, server_socket\n",
    "\n",
    "# Custom Dataset for IMDb\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, tokenizer, split):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = load_dataset('imdb', split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            item['text'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = 1 if item['label'] == 'pos' else 0\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "model = BertModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")  # Only the encoder\n",
    "\n",
    "# Manually split the model: Encoder only\n",
    "class BertPart1(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BertPart1, self).__init__()\n",
    "        self.bert = model  # Use only the BERT part of the model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state  # Return hidden states only\n",
    "\n",
    "# Create the first part of the model\n",
    "model_part1 = BertPart1(model)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_part1.to(device)\n",
    "\n",
    "# Start the server\n",
    "conn, server_socket = start_server()\n",
    "\n",
    "# Create a DataLoader for the IMDb dataset\n",
    "dataset = IMDBDataset(tokenizer, split='train')\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Forward pass and send data to the client\n",
    "for batch in data_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    hidden_states = model_part1(input_ids, attention_mask)\n",
    "\n",
    "    # Send the output to the client using a socket\n",
    "    conn.sendall(pickle.dumps(hidden_states.cpu().detach().numpy()))  # Send to client\n",
    "    print(\"Sent hidden states to client\")\n",
    "\n",
    "# Close the connection and server socket\n",
    "conn.close()\n",
    "server_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d120aa-84f1-4c08-ab74-44993d54832b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44fb09-bfad-4bf2-becb-4e6a9ccd7a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
