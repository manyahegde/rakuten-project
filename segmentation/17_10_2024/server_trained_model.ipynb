{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c9fac4-97d6-4fcd-83b3-4b9532a5f8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Exam\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Exam\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4689/4689 06:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.367773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.353482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.349449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=2)\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',  # Ensure padding to max_length\n",
    "        truncation=True,\n",
    "        max_length=128  # Set this according to model constraints\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Model training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb31cba7-d779-4820-a2f7-41a5fb8c37b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ae46ad-2432-4662-a297-ffe07279d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening for connections...\n",
      "Connected to ('172.16.19.7', 57464)\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Receive input data from the client\u001b[39;00m\n\u001b[0;32m     24\u001b[0m data \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mrecv(\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m inputs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mloads(data)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Move inputs to the same device as the model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=2)\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the correct device\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('0.0.0.0', 10300))  # Bind to all available interfaces and port 10300\n",
    "server_socket.listen(1)\n",
    "\n",
    "print(\"Server is listening for connections...\")\n",
    "conn, addr = server_socket.accept()\n",
    "print(f\"Connected to {addr}\")\n",
    "\n",
    "# Receive input data from the client\n",
    "data = conn.recv(4096)\n",
    "inputs = pickle.loads(data)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "for key in inputs.keys():\n",
    "    inputs[key] = inputs[key].to(device)\n",
    "\n",
    "# Make predictions using the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)  # Get the predicted class (0 or 1)\n",
    "\n",
    "# Serialize the predictions and send them back to the client\n",
    "predictions_data = pickle.dumps(predictions.cpu())  # Move to CPU before serializing\n",
    "conn.sendall(predictions_data)\n",
    "\n",
    "conn.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ea3b9-3f4f-4d2a-a31d-e4761bab20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import pickle\n",
    "\n",
    "# Function to receive data in chunks from the client\n",
    "def receive_data(conn):\n",
    "    buffer_size = 4096  # Adjust buffer size as needed\n",
    "    data = b\"\"  # Use a bytes object to store the received data\n",
    "    \n",
    "    # Keep receiving data in chunks until there is no more data\n",
    "    while True:\n",
    "        part = conn.recv(buffer_size)\n",
    "        if not part:\n",
    "            break  # Stop receiving when no more data is sent\n",
    "        data += part  # Append each chunk to the data\n",
    "\n",
    "    return pickle.loads(data)  # Unpickle the complete data after receiving all parts\n",
    "\n",
    "# Set up the server\n",
    "def server_program():\n",
    "    host = '0.0.0.0'  # Localhost\n",
    "    port = 10300        # Arbitrary non-privileged port\n",
    "    \n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_socket.bind((host, port))  # Bind the server to the host and port\n",
    "    server_socket.listen(1)           # Listen for incoming connections\n",
    "\n",
    "    print(\"Server is listening for connections...\")\n",
    "    \n",
    "    # Accept a connection from the client\n",
    "    conn, address = server_socket.accept()\n",
    "    print(f\"Connected to {address}\")\n",
    "    \n",
    "    try:\n",
    "        # Receive and process the data from the client\n",
    "        inputs = receive_data(conn)\n",
    "        print(\"Received data:\", inputs)\n",
    "        \n",
    "        # Your code for handling the received data goes here\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error receiving data: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        conn.close()  # Close the connection\n",
    "        print(\"Connection closed\")\n",
    "\n",
    "# Start the server\n",
    "if __name__ == \"__main__\":\n",
    "    server_program()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94c6f02-821d-4e42-a56e-b8b51213c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening for connections...\n",
      "Connected to ('172.16.19.59', 52549)\n",
      "Connection closed.\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '8'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mloads(data)  \u001b[38;5;66;03m# Unpickle the complete received data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Receive input data from the client\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m receive_data(conn)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Move inputs to the same device as the model\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m, in \u001b[0;36mreceive_data\u001b[1;34m(conn)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packet\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mloads(data)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '8'."
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=2)\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the correct device\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('0.0.0.0', 10300))  # Bind to all available interfaces and port 10300\n",
    "server_socket.listen(1)\n",
    "\n",
    "print(\"Server is listening for connections...\")\n",
    "conn, addr = server_socket.accept()\n",
    "print(f\"Connected to {addr}\")\n",
    "\n",
    "def receive_data(conn):\n",
    "    # First receive the length of the data\n",
    "    data_length = int.from_bytes(conn.recv(4), 'big')  # Receive 4 bytes (for data size)\n",
    "    data = b''\n",
    "    \n",
    "    # Keep receiving data until the full length is received\n",
    "    while len(data) < data_length:\n",
    "        packet = conn.recv(4096)\n",
    "        if not packet:\n",
    "            break\n",
    "        data += packet\n",
    "    \n",
    "    return pickle.loads(data)  # Unpickle the complete received data\n",
    "\n",
    "try:\n",
    "    # Receive input data from the client\n",
    "    inputs = receive_data(conn)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)  # Get the predicted class (0 or 1)\n",
    "\n",
    "    # Serialize the predictions and send them back to the client\n",
    "    predictions_data = pickle.dumps(predictions.cpu())  # Move to CPU before serializing\n",
    "    conn.sendall(len(predictions_data).to_bytes(4, 'big'))  # Send the length of the data first\n",
    "    conn.sendall(predictions_data)  # Send the actual predictions\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b312b4-e95f-495f-806b-b05b64baa319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the IMDb dataset using Hugging Face's datasets library\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Convert the train and test splits to pandas DataFrames\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Display the first few rows of the train DataFrame\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eceaecf-9cb3-43a2-b984-e7775a5b6ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichÃ©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31592384-7ed4-4554-9892-2163b50d4176",
   "metadata": {},
   "source": [
    "## trained one final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f200df-03b0-44bd-b442-1fe50cb9207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDb dataset...\n",
      "Loading TinyBERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Exam\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the dataset...\n",
      "Initializing the trainer...\n",
      "Training the model on IMDb dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4689/4689 06:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.367260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.355444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.349609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the trained model...\n",
      "Server is listening for connections...\n",
      "Connected to ('172.16.19.59', 52728)\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "print(\"Loading IMDb dataset...\")\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Load the tokenizer and model\n",
    "print(\"Loading TinyBERT model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=2)\n",
    "\n",
    "# Preprocess the dataset for model input\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',  # Ensure padding to max_length\n",
    "        truncation=True,\n",
    "        max_length=128  # Truncate longer reviews\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing the dataset...\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the correct device\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',        # Output directory\n",
    "    evaluation_strategy=\"epoch\",   # Evaluate after every epoch\n",
    "    learning_rate=2e-5,            # Learning rate\n",
    "    per_device_train_batch_size=16,# Training batch size\n",
    "    per_device_eval_batch_size=16, # Evaluation batch size\n",
    "    num_train_epochs=3,            # Number of training epochs\n",
    "    weight_decay=0.01,             # Weight decay\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "print(\"Initializing the trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model on IMDb dataset...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model after training\n",
    "print(\"Saving the trained model...\")\n",
    "model.save_pretrained('./trained_model')\n",
    "tokenizer.save_pretrained('./trained_model')\n",
    "\n",
    "# Now that the model is trained, set up the server for inference requests\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('0.0.0.0', 10300))  # Bind to all available interfaces and port 10300\n",
    "server_socket.listen(1)\n",
    "\n",
    "print(\"Server is listening for connections...\")\n",
    "conn, addr = server_socket.accept()\n",
    "print(f\"Connected to {addr}\")\n",
    "\n",
    "def receive_data(conn):\n",
    "    # First receive the length of the data\n",
    "    data_length = int.from_bytes(conn.recv(4), 'big')  # Receive 4 bytes (for data size)\n",
    "    data = b''\n",
    "    \n",
    "    # Keep receiving data until the full length is received\n",
    "    while len(data) < data_length:\n",
    "        packet = conn.recv(4096)\n",
    "        if not packet:\n",
    "            break\n",
    "        data += packet\n",
    "    \n",
    "    return pickle.loads(data)  # Unpickle the complete received data\n",
    "\n",
    "try:\n",
    "    # Receive input data from the client\n",
    "    inputs = receive_data(conn)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)  # Get the predicted class (0 or 1)\n",
    "\n",
    "    # Serialize the predictions and send them back to the client\n",
    "    predictions_data = pickle.dumps(predictions.cpu())  # Move to CPU before serializing\n",
    "    conn.sendall(len(predictions_data).to_bytes(4, 'big'))  # Send the length of the data first\n",
    "    conn.sendall(predictions_data)  # Send the actual predictions\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc23b8a-b3b7-42d0-90e4-a5d0b44ed094",
   "metadata": {},
   "source": [
    "### run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d52ebb2-3339-446e-b9b2-3a49a98fd4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening for connections...\n",
      "Connected to ('172.16.19.59', 52857)\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('0.0.0.0', 10300))  # Bind to all available interfaces and port 10300\n",
    "server_socket.listen(1)\n",
    "\n",
    "print(\"Server is listening for connections...\")\n",
    "conn, addr = server_socket.accept()\n",
    "print(f\"Connected to {addr}\")\n",
    "\n",
    "def receive_data(conn):\n",
    "    # First receive the length of the data\n",
    "    data_length = int.from_bytes(conn.recv(4), 'big')  # Receive 4 bytes (for data size)\n",
    "    data = b''\n",
    "    \n",
    "    # Keep receiving data until the full length is received\n",
    "    while len(data) < data_length:\n",
    "        packet = conn.recv(4096)\n",
    "        if not packet:\n",
    "            break\n",
    "        data += packet\n",
    "    \n",
    "    return pickle.loads(data)  # Unpickle the complete received data\n",
    "\n",
    "try:\n",
    "    # Receive input data from the client\n",
    "    inputs = receive_data(conn)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)  # Get the predicted class (0 or 1)\n",
    "\n",
    "    # Serialize the predictions and send them back to the client\n",
    "    predictions_data = pickle.dumps(predictions.cpu())  # Move to CPU before serializing\n",
    "    conn.sendall(len(predictions_data).to_bytes(4, 'big'))  # Send the length of the data first\n",
    "    conn.sendall(predictions_data)  # Send the actual predictions\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a36ba3b-2e01-467d-8192-0c7339c8bad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44625088-5a17-4510-b94e-c482f4e08e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
