{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4bdf79-cb5b-4db1-8b2b-4921a3a8de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Example input texts\n",
    "texts = [\n",
    "    \"This movie was fantastic! I loved it.\",\n",
    "    \"Terrible film. I would not recommend it.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs with specified max length\n",
    "max_length = 128  # Set max_length based on model constraints\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',  # Pad to max_length\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# Serialize the inputs to send to the server\n",
    "data = pickle.dumps(inputs)\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Change IP as needed\n",
    "client_socket.sendall(data)\n",
    "client_socket.close()\n",
    "print(\"Input data sent to server.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c0148b-45be-4ba2-8935-d82c6e947ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\Rakuten\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m predictions_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Initialize an empty byte string\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     packet \u001b[38;5;241m=\u001b[39m client_socket\u001b[38;5;241m.\u001b[39mrecv(\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packet:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Example input texts\n",
    "texts = [\n",
    "    \"This movie was fantastic! I loved it.\",\n",
    "    \"Terrible film. I would not recommend it.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs with specified max length\n",
    "max_length = 128  # Set max_length based on model constraints\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',  # Pad to max_length\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# Serialize the inputs to send to the server\n",
    "data = pickle.dumps(inputs)\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Change IP as needed\n",
    "client_socket.sendall(data)\n",
    "\n",
    "# Receive predictions from the server\n",
    "predictions_data = b''  # Initialize an empty byte string\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    predictions_data += packet\n",
    "\n",
    "# Deserialize the predictions\n",
    "predictions = torch.from_numpy(pickle.loads(predictions_data))\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Received predictions:\", predictions.numpy())\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "print(\"Connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfc125-2591-4bf3-8e2d-fe744d483c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Example input texts\n",
    "texts = [\n",
    "    \"This movie was fantastic! I loved it.\",\n",
    "    \"Terrible film. I would not recommend it.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Serialize the inputs\n",
    "data = pickle.dumps(inputs)\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Change IP as needed\n",
    "client_socket.sendall(data)\n",
    "\n",
    "# Receive predictions from the server\n",
    "predictions_data = client_socket.recv(4096)  # Adjust buffer size as needed\n",
    "\n",
    "# Deserialize the predictions\n",
    "predictions = torch.from_numpy(pickle.loads(predictions_data))\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Received predictions:\", predictions.numpy())\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac7b656-80a0-484d-ba17-7ec9817d1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\Rakuten\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data sent to server.\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput data sent to server.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Receive predictions from the server\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m predictions_data \u001b[38;5;241m=\u001b[39m client_socket\u001b[38;5;241m.\u001b[39mrecv(\u001b[38;5;241m4096\u001b[39m)  \u001b[38;5;66;03m# Adjust buffer size as needed\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Deserialize the predictions\u001b[39;00m\n\u001b[0;32m     37\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(pickle\u001b[38;5;241m.\u001b[39mloads(predictions_data))\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Example input texts\n",
    "texts = [\n",
    "    \"This movie was fantastic! I loved it.\",\n",
    "    \"Terrible film. I would not recommend it.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Serialize the inputs\n",
    "data = pickle.dumps(inputs)\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Change IP as needed\n",
    "client_socket.sendall(data)\n",
    "\n",
    "# Print message indicating input data sent\n",
    "print(\"Input data sent to server.\")\n",
    "\n",
    "# Receive predictions from the server\n",
    "predictions_data = client_socket.recv(4096)  # Adjust buffer size as needed\n",
    "\n",
    "# Deserialize the predictions\n",
    "predictions = torch.from_numpy(pickle.loads(predictions_data))\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Received predictions:\", predictions.numpy())\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c539bb6-647b-41e6-a1cc-1b9ddc78410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server at 172.16.19.49:10300\n",
      "Input sentences sent to server.\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     packet \u001b[38;5;241m=\u001b[39m client_socket\u001b[38;5;241m.\u001b[39mrecv(\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packet:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "\n",
    "# Example input sentences for the model\n",
    "input_sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here is another sentence.\",\n",
    "    \"And yet another one.\"\n",
    "]\n",
    "\n",
    "# Serialize the input data\n",
    "serialized_data = pickle.dumps(input_sentences)\n",
    "\n",
    "# Client setup for inference\n",
    "host = '172.16.19.49'  # Replace with the server's IP address\n",
    "port = 10300\n",
    "\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect((host, port))\n",
    "print(f\"Connected to server at {host}:{port}\")\n",
    "\n",
    "# Send input data to the server\n",
    "client_socket.sendall(serialized_data)\n",
    "print(\"Input sentences sent to server.\")\n",
    "\n",
    "# Receive predictions from the server\n",
    "predictions = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    predictions += packet\n",
    "\n",
    "# Deserialize the predictions\n",
    "predictions = pickle.loads(predictions)\n",
    "\n",
    "print(\"Received predictions from server:\", predictions)\n",
    "\n",
    "# Close the connection\n",
    "client_socket.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849b811-9fa1-453b-85e6-80287f3a044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import pickle\n",
    "\n",
    "# Set up the client\n",
    "HOST = '172.16.19.49'  # Replace with the server's IP address\n",
    "PORT = 5000\n",
    "\n",
    "def get_prediction(review):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.connect((HOST, PORT))\n",
    "       \n",
    "        # Serialize the review\n",
    "        data = pickle.dumps(review)\n",
    "        s.sendall(data)\n",
    "\n",
    "        # Receive the prediction\n",
    "        prediction = s.recv(1024)\n",
    "        return pickle.loads(prediction)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        review = input(\"Enter a movie review (or 'exit' to quit): \")\n",
    "        if review.lower() == 'exit':\n",
    "            break\n",
    "        prediction = get_prediction(review)\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313353ee-b311-4cc3-960f-5d63fba51f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertForSequenceClassification\n",
      "File \u001b[1;32m~\\.conda\\envs\\Rakuten\\Lib\\site-packages\\torch\\__init__.py:899\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;66;03m# The __file__ check only works for Python 3.7 and above.\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _C_for_compiled_check\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    900\u001b[0m             textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[0;32m    901\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m                Failed to load PyTorch C extensions:\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m                    It appears that PyTorch has loaded the `torch/_C` folder\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m                    of the PyTorch repository rather than the C extensions which\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;124;03m                    are expected in the `torch._C` namespace. This can occur when\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m                    using the `install` workflow. e.g.\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;124;03m                        $ python setup.py install && python -c \"import torch\"\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m                    This error can generally be solved using the `develop` workflow\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m                        $ python setup.py develop && python -c \"import torch\"  # This should succeed\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m                    or by running Python from a different directory.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m             )\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    914\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# The torch._C submodule is already loaded via `from torch._C import *` above\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;66;03m# Make an explicit reference to the _C submodule to appease linters\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory."
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Load TinyBERT model and split it\n",
    "model = BertForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels=2)\n",
    "\n",
    "class TinyBERT_Part2(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(model.bert.encoder.layer[2:], model.bert.pooler, model.classifier)\n",
    "   \n",
    "    def forward(self, hidden_states):\n",
    "        return self.model(hidden_states)\n",
    "\n",
    "# Create the second part of the model and move it to CPU\n",
    "part2 = TinyBERT_Part2(model).to('cpu')\n",
    "\n",
    "# Socket setup for receiving hidden states from Laptop 1\n",
    "client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client.connect(('172.16.19.49', 10300))  # Replace with Laptop 1's IP address\n",
    "\n",
    "# Receive the hidden states\n",
    "data = b\"\"\n",
    "while True:\n",
    "    packet = client.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "# Convert bytes back to numpy and then torch tensor\n",
    "hidden_states_np = np.frombuffer(data, dtype=np.float32).reshape(8, 512, 312)  # Adjust the shape\n",
    "hidden_states = torch.tensor(hidden_states_np)\n",
    "\n",
    "# Forward pass on Laptop 2\n",
    "logits = part2(hidden_states)\n",
    "\n",
    "# Loss calculation, backpropagation, etc. can be done here\n",
    "# Assuming labels are also communicated or loaded separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa61aae8-3af5-4a4b-a2f6-e6113a6f2670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dd5a28-210e-4cf5-adc5-62c60b72e493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp38-cp38-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp38-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp38-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\exam\\.conda\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading regex-2024.9.11-cp38-cp38-win_amd64.whl (274 kB)\n",
      "Downloading safetensors-0.4.5-cp38-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.1-cp38-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 762.0 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.4 MB 762.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.8/2.4 MB 763.2 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 740.5 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 740.5 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 745.8 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 745.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.6/2.4 MB 742.2 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.8/2.4 MB 745.8 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 745.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 743.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 743.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 738.4 kB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.9.0 huggingface-hub-0.25.2 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 tqdm-4.66.5 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ef761c-0c4e-4202-a94d-d08c042b0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m     received_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packet\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Deserialize the predictions\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreceived_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Collect all predictions\u001b[39;00m\n\u001b[0;32m     69\u001b[0m all_predictions\u001b[38;5;241m.\u001b[39mextend(predictions\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def get_model_inputs(text_list):\n",
    "    # Tokenize and encode text for model input\n",
    "    inputs = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    # Convert the tensor predictions to a list and compare with true labels\n",
    "    predicted_labels = predictions.tolist()\n",
    "    correct_predictions = sum(p == t for p, t in zip(predicted_labels, true_labels))\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Connect to the server\n",
    "\n",
    "try:\n",
    "    batch_size = 16  # Define batch size for processing\n",
    "    num_batches = len(test_df) // batch_size\n",
    "\n",
    "    all_predictions = []\n",
    "   \n",
    "    for i in range(num_batches):\n",
    "        # Extract batch of text and labels from test_df\n",
    "        batch_texts = test_df['text'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "        batch_labels = test_df['label'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        data = get_model_inputs(batch_texts)\n",
    "       \n",
    "        # Serialize the input data\n",
    "        serialized_data = pickle.dumps(data)\n",
    "\n",
    "        # Send the length of the data first\n",
    "        client_socket.sendall(len(serialized_data).to_bytes(4, 'big'))\n",
    "\n",
    "        # Send the serialized input data\n",
    "        client_socket.sendall(serialized_data)\n",
    "       \n",
    "        # Receive the size of the incoming data\n",
    "        data_length = int.from_bytes(client_socket.recv(4), 'big')\n",
    "       \n",
    "        # Receive the actual predictions data\n",
    "        received_data = b\"\"\n",
    "        while len(received_data) < data_length:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                break\n",
    "            received_data += packet\n",
    "\n",
    "        # Deserialize the predictions\n",
    "        predictions = pickle.loads(received_data)\n",
    "\n",
    "        # Collect all predictions\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "\n",
    "    # Calculate accuracy using the full set of predictions\n",
    "    accuracy = calculate_accuracy(all_predictions, test_df['label'].tolist())\n",
    "    print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "finally:\n",
    "    client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49267c0d-cbf0-4041-95b1-44f09b7d6979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected data length: 0, received data length: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expected data length: {data_length}, received data length: {len(received_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f990cf85-0b67-4c91-baf7-36e1b52d2409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected data length: 522, received data length: 522\n",
      "Expected data length: 0, received data length: 0\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Check if we received enough data before deserialization\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(received_data) \u001b[38;5;241m==\u001b[39m data_length:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Deserialize the predictions\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreceived_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Collect all predictions\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     all_predictions\u001b[38;5;241m.\u001b[39mextend(predictions\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def get_model_inputs(text_list):\n",
    "    # Tokenize and encode text for model input\n",
    "    inputs = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    # Convert the tensor predictions to a list and compare with true labels\n",
    "    predicted_labels = predictions.tolist()\n",
    "    correct_predictions = sum(p == t for p, t in zip(predicted_labels, true_labels))\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Connect to the server\n",
    "\n",
    "try:\n",
    "    batch_size = 16  # Define batch size for processing\n",
    "    num_batches = len(test_df) // batch_size\n",
    "\n",
    "    all_predictions = []\n",
    "   \n",
    "    for i in range(num_batches):\n",
    "        # Extract batch of text and labels from test_df\n",
    "        batch_texts = test_df['text'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "        batch_labels = test_df['label'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        data = get_model_inputs(batch_texts)\n",
    "       \n",
    "        # Serialize the input data\n",
    "        serialized_data = pickle.dumps(data)\n",
    "\n",
    "        # Send the length of the data first\n",
    "        client_socket.sendall(len(serialized_data).to_bytes(4, 'big'))\n",
    "\n",
    "        # Send the serialized input data\n",
    "        client_socket.sendall(serialized_data)\n",
    "       \n",
    "        # Receive the size of the incoming data\n",
    "        data_length = int.from_bytes(client_socket.recv(4), 'big')\n",
    "       \n",
    "        # Receive the actual predictions data\n",
    "        received_data = b\"\"\n",
    "        while len(received_data) < data_length:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                print(\"No more data received from server. Closing connection.\")\n",
    "                break\n",
    "            received_data += packet\n",
    "\n",
    "        # Debug output for received data length\n",
    "        print(f\"Expected data length: {data_length}, received data length: {len(received_data)}\")\n",
    "\n",
    "        # Check if we received enough data before deserialization\n",
    "        if len(received_data) == data_length:\n",
    "            # Deserialize the predictions\n",
    "            predictions = pickle.loads(received_data)\n",
    "            # Collect all predictions\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "        else:\n",
    "            print(\"Received data length does not match expected length.\")\n",
    "\n",
    "    # Calculate accuracy using the full set of predictions\n",
    "    accuracy = calculate_accuracy(all_predictions, test_df['label'].tolist())\n",
    "    print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "finally:\n",
    "    client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3a4d3-2790-425d-81e7-d09d9a7b8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def get_model_inputs(text_list):\n",
    "    # Tokenize and encode text for model input\n",
    "    inputs = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    # Convert the tensor predictions to a list and compare with true labels\n",
    "    predicted_labels = predictions.tolist()\n",
    "    correct_predictions = sum(p == t for p, t in zip(predicted_labels, true_labels))\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Connect to the server\n",
    "\n",
    "try:\n",
    "    batch_size = 16  # Define batch size for processing\n",
    "    num_batches = len(test_df) // batch_size\n",
    "\n",
    "    all_predictions = []\n",
    "   \n",
    "    for i in range(num_batches):\n",
    "        # Extract batch of text and labels from test_df\n",
    "        batch_texts = test_df['text'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "        batch_labels = test_df['label'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        data = get_model_inputs(batch_texts)\n",
    "       \n",
    "        # Serialize the input data\n",
    "        serialized_data = pickle.dumps(data)\n",
    "\n",
    "        # Send the length of the data first\n",
    "        client_socket.sendall(len(serialized_data).to_bytes(4, 'big'))\n",
    "\n",
    "        # Send the serialized input data\n",
    "        client_socket.sendall(serialized_data)\n",
    "       \n",
    "        # Receive the size of the incoming data\n",
    "        data_length = int.from_bytes(client_socket.recv(4), 'big')\n",
    "       \n",
    "        # Receive the actual predictions data\n",
    "        received_data = b\"\"\n",
    "        while len(received_data) < data_length:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                print(\"No more data received from server. Closing connection.\")\n",
    "                break\n",
    "            received_data += packet\n",
    "\n",
    "        # Debug output for received data length\n",
    "        print(f\"Expected data length: {data_length}, received data length: {len(received_data)}\")\n",
    "\n",
    "        # Check if we received enough data before deserialization\n",
    "        if len(received_data) == data_length and data_length > 0:\n",
    "            try:\n",
    "                # Deserialize the predictions\n",
    "                predictions = pickle.loads(received_data)\n",
    "                # Collect all predictions\n",
    "                all_predictions.extend(predictions.tolist())\n",
    "            except EOFError:\n",
    "                print(\"Error deserializing data. Received data may be corrupted.\")\n",
    "        else:\n",
    "            print(\"Received data length does not match expected length or is empty.\")\n",
    "\n",
    "    # Calculate accuracy using the full set of predictions\n",
    "    if all_predictions:\n",
    "        accuracy = calculate_accuracy(all_predictions, test_df['label'].tolist())\n",
    "        print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"No predictions to calculate accuracy.\")\n",
    "\n",
    "finally:\n",
    "    client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2166c9af-ad91-43b0-a06c-e739b30abd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model input preparation\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def get_model_inputs(text):\n",
    "    # Tokenize and encode text for model input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Connect to the server\n",
    "\n",
    "# Example input data\n",
    "data = get_model_inputs([\"This is a great movie!\", \"I didn't like this film.\"])\n",
    "\n",
    "try:\n",
    "    # Serialize the input data\n",
    "    serialized_data = pickle.dumps(data)\n",
    "\n",
    "    # Send the length of the data first\n",
    "    client_socket.sendall(len(serialized_data).to_bytes(4, 'big'))\n",
    "\n",
    "    # Send the serialized input data\n",
    "    client_socket.sendall(serialized_data)\n",
    "   \n",
    "    # Receive the size of the incoming data\n",
    "    data_length = int.from_bytes(client_socket.recv(4), 'big')\n",
    "   \n",
    "    # Receive the actual predictions data\n",
    "    received_data = b\"\"\n",
    "    while len(received_data) < data_length:\n",
    "        packet = client_socket.recv(4096)\n",
    "        if not packet:\n",
    "            break\n",
    "        received_data += packet\n",
    "\n",
    "    predictions = pickle.loads(received_data)\n",
    "    print(\"Predictions:\", predictions)\n",
    "\n",
    "finally:\n",
    "    client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5d80a7-db61-4b68-a177-ab70dc6bd49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected data length: 522, received data length: 522\n",
      "Expected data length: 0, received data length: 0\n",
      "Error deserializing data. Received data may be corrupted.\n"
     ]
    },
    {
     "ename": "ConnectionAbortedError",
     "evalue": "[WinError 10053] An established connection was aborted by the software in your host machine",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionAbortedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m serialized_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(data)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Send the length of the data first\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mclient_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mserialized_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Send the serialized input data\u001b[39;00m\n\u001b[0;32m     50\u001b[0m client_socket\u001b[38;5;241m.\u001b[39msendall(serialized_data)\n",
      "\u001b[1;31mConnectionAbortedError\u001b[0m: [WinError 10053] An established connection was aborted by the software in your host machine"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def get_model_inputs(text_list):\n",
    "    # Tokenize and encode text for model input\n",
    "    inputs = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    predicted_labels = predictions.tolist()\n",
    "    correct_predictions = sum(p == t for p, t in zip(predicted_labels, true_labels))\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Connect to the server\n",
    "\n",
    "try:\n",
    "    batch_size = 16  # Define batch size for processing\n",
    "    num_batches = len(test_df) // batch_size\n",
    "\n",
    "    all_predictions = []\n",
    "   \n",
    "    for i in range(num_batches):\n",
    "        # Extract batch of text and labels from test_df\n",
    "        batch_texts = test_df['text'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "        batch_labels = test_df['label'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        data = get_model_inputs(batch_texts)\n",
    "       \n",
    "        # Serialize the input data\n",
    "        serialized_data = pickle.dumps(data)\n",
    "\n",
    "        # Send the length of the data first\n",
    "        client_socket.sendall(len(serialized_data).to_bytes(4, 'big'))\n",
    "\n",
    "        # Send the serialized input data\n",
    "        client_socket.sendall(serialized_data)\n",
    "       \n",
    "        # Receive the size of the incoming data\n",
    "        data_length = int.from_bytes(client_socket.recv(4), 'big')\n",
    "       \n",
    "        # Receive the actual predictions data\n",
    "        received_data = b\"\"\n",
    "        while len(received_data) < data_length:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                print(\"No more data received from server. Closing connection.\")\n",
    "                break\n",
    "            received_data += packet\n",
    "\n",
    "        # Debug output for received data length\n",
    "        print(f\"Expected data length: {data_length}, received data length: {len(received_data)}\")\n",
    "\n",
    "        # Check if we received enough data before deserialization\n",
    "        if len(received_data) == data_length:\n",
    "            try:\n",
    "                # Deserialize the predictions\n",
    "                predictions = pickle.loads(received_data)\n",
    "                all_predictions.extend(predictions.tolist())\n",
    "            except EOFError:\n",
    "                print(\"Error deserializing data. Received data may be corrupted.\")\n",
    "        else:\n",
    "            print(\"Received data length does not match expected length.\")\n",
    "\n",
    "    # Calculate accuracy using the full set of predictions\n",
    "    if all_predictions:\n",
    "        accuracy = calculate_accuracy(all_predictions, test_df['label'].tolist())\n",
    "        print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"No predictions to calculate accuracy.\")\n",
    "\n",
    "finally:\n",
    "    client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd0ec83-8775-4b61-8f1f-6dc557204c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([1, 0])\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model input preparation\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def get_model_inputs(text):\n",
    "    # Tokenize and encode text for model input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    correct_predictions = sum(p == t for p, t in zip(predictions, true_labels))\n",
    "    accuracy = correct_predictions / len(true_labels) * 100  # Convert to percentage\n",
    "    return accuracy\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.49', 10300))  # Connect to the server\n",
    "\n",
    "# Example input data and true labels\n",
    "input_texts = [\"This is a great movie!\", \"I didn't like this film.\"]\n",
    "true_labels = [1, 0]  # Assuming 1 for positive and 0 for negative\n",
    "\n",
    "data = get_model_inputs(input_texts)\n",
    "\n",
    "try:\n",
    "    # Serialize the input data\n",
    "    serialized_data = pickle.dumps(data)\n",
    "\n",
    "    # Send the length of the data first\n",
    "    client_socket.sendall(len(serialized_data).to_bytes(4, 'big'))\n",
    "\n",
    "    # Send the serialized input data\n",
    "    client_socket.sendall(serialized_data)\n",
    "   \n",
    "    # Receive the size of the incoming data\n",
    "    data_length = int.from_bytes(client_socket.recv(4), 'big')\n",
    "   \n",
    "    # Receive the actual predictions data\n",
    "    received_data = b\"\"\n",
    "    while len(received_data) < data_length:\n",
    "        packet = client_socket.recv(4096)\n",
    "        if not packet:\n",
    "            break\n",
    "        received_data += packet\n",
    "\n",
    "    predictions = pickle.loads(received_data)\n",
    "    \n",
    "    # Assuming predictions are returned as a list of integers\n",
    "    print(\"Predictions:\", predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = calculate_accuracy(predictions, true_labels)\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "finally:\n",
    "    client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a03fc62-01c0-41a2-ac71-8c6783ea2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected data length: 522, received data length: 522\n",
      "Expected data length: 0, received data length: 0\n",
      "Error deserializing data. Received data may be corrupted.\n",
      "Connection error: [WinError 10053] An established connection was aborted by the software in your host machine\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def get_model_inputs(text_list):\n",
    "    inputs = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    predicted_labels = predictions.tolist()\n",
    "    correct_predictions = sum(p == t for p, t in zip(predicted_labels, true_labels))\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "try:\n",
    "    client_socket.connect(('172.16.19.49', 10300))  # Connect to the server\n",
    "\n",
    "    batch_size = 16\n",
    "    num_batches = len(test_df) // batch_size\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_texts = test_df['text'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "        batch_labels = test_df['label'][i*batch_size:(i+1)*batch_size].tolist()\n",
    "\n",
    "        data = get_model_inputs(batch_texts)\n",
    "        serialized_data = pickle.dumps(data)\n",
    "\n",
    "        # Send the length of the data first\n",
    "        client_socket.sendall(len(serialized_data).to_bytes(4, 'big'))\n",
    "        client_socket.sendall(serialized_data)\n",
    "\n",
    "        # Receive the size of the incoming data\n",
    "        data_length = int.from_bytes(client_socket.recv(4), 'big')\n",
    "        received_data = b\"\"\n",
    "\n",
    "        while len(received_data) < data_length:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                print(\"No more data received from server. Closing connection.\")\n",
    "                break\n",
    "            received_data += packet\n",
    "\n",
    "        print(f\"Expected data length: {data_length}, received data length: {len(received_data)}\")\n",
    "\n",
    "        if len(received_data) == data_length:\n",
    "            try:\n",
    "                predictions = pickle.loads(received_data)\n",
    "                all_predictions.extend(predictions.tolist())\n",
    "            except EOFError:\n",
    "                print(\"Error deserializing data. Received data may be corrupted.\")\n",
    "        else:\n",
    "            print(\"Received data length does not match expected length.\")\n",
    "\n",
    "    if all_predictions:\n",
    "        accuracy = calculate_accuracy(all_predictions, test_df['label'].tolist())\n",
    "        print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"No predictions to calculate accuracy.\")\n",
    "\n",
    "except (ConnectionAbortedError, ConnectionResetError) as e:\n",
    "    print(f\"Connection error: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a06dc1-35af-4533-9f38-931ea727ffa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
