{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e987ba11-ffab-41bd-8ff6-b930f7b0479a",
   "metadata": {},
   "source": [
    "## Optimizing Tinybert fine-tuned with IMDB dataset using PSO and pruning using L1 pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2d771-408b-4d15-bef9-619bb2ba3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1: Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Step 2: Load TinyBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Step 4: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Step 5: Implement custom Trainer with L1 regularization\n",
    "class L1Trainer(Trainer):\n",
    "    def __init__(self, l1_lambda=1e-5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1_lambda = l1_lambda\n",
    "\n",
    "    # Override the compute_loss method to add L1 regularization\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Compute the standard loss using Hugging Face's method\n",
    "        loss, outputs = super().compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        # Apply L1 regularization (penalty for non-zero weights)\n",
    "        l1_loss = 0\n",
    "        for param in model.parameters():\n",
    "            l1_loss += torch.sum(torch.abs(param))\n",
    "\n",
    "        # Add L1 regularization to the loss\n",
    "        loss += self.l1_lambda * l1_loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Step 6: Define the L1Trainer with L1 regularization\n",
    "trainer = L1Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    "    l1_lambda=1e-5,  # L1 regularization coefficient\n",
    ")\n",
    "\n",
    "# Step 7: Train the model with L1 regularization\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")\n",
    "\n",
    "# Step 9: Load the accuracy metric for evaluation\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Step 10: Define a custom compute_metrics function to calculate accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Step 11: Redefine the Trainer to include compute_metrics and L1 regularization\n",
    "trainer = L1Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    l1_lambda=1e-5  # L1 regularization coefficient\n",
    ")\n",
    "\n",
    "# Step 12: Evaluate the model with L1 regularization\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "# Step 13: Print the evaluation results and accuracy\n",
    "print(f\"Evaluation results: {eval_result}\")\n",
    "print(f\"Accuracy: {eval_result['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69e685e-e312-4e0f-96d7-b2f8498cd1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./optimized_tinybert\\\\tokenizer_config.json',\n",
       " './optimized_tinybert\\\\special_tokens_map.json',\n",
       " './optimized_tinybert\\\\vocab.txt',\n",
       " './optimized_tinybert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./optimized_tinybert\")\n",
    "tokenizer.save_pretrained(\"./optimized_tinybert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf46a8a-1463-48d5-8c50-e8578fc14b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b75a87-d1b7-4888-a6a0-caaf07842335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2b8ab1-6015-4d56-a064-8d2bf37ffb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.19.49', 56963)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Perform forward pass through Part 2\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 66\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_part2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Part 2 forward pass completed. Output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Step 1: Convert the output to logits\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m, in \u001b[0;36mTinyBERTPart2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[1;32m---> 38\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(x)  \u001b[38;5;66;03m# Use the pooler after the encoder layers\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)  \u001b[38;5;66;03m# Classification layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:393\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    376\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`attn_implementation=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` when loading the model.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    384\u001b[0m         hidden_states,\n\u001b[0;32m    385\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         output_attentions,\n\u001b[0;32m    391\u001b[0m     )\n\u001b[1;32m--> 393\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[0;32m    395\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states))\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "class TinyBERTPart1(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart1, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.bert.encoder.layer[:2])  # Access encoder layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TinyBERTPart2(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart2, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.bert.encoder.layer[2:])  # Access remaining layers\n",
    "        self.pooler = original_model.bert.pooler  # Access the pooler\n",
    "        self.classifier = nn.Linear(original_model.config.hidden_size, 2)  # Adjust the output size as needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        x = self.pooler(x)  # Use the pooler after the encoder layers\n",
    "        x = self.classifier(x)  # Classification layer\n",
    "        return x\n",
    "\n",
    "model_part1 = TinyBERTPart1(tinybert)\n",
    "model_part2 = TinyBERTPart2(tinybert)\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    output = model_part2(intermediate_output)\n",
    "\n",
    "print(f\"Model Part 2 forward pass completed. Output shape: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdecd6b-125b-4b1b-8927-039bb90b91f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of intermediate_output: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of intermediate_output: {type(intermediate_output)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d078967d-d21d-408a-84b7-cdc34bb22f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of intermediate_output: torch.Size([5, 13, 312])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of intermediate_output: {intermediate_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d625903-8f5a-4744-8c51-7574f258a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.19.49', 56978)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Type of intermediate_output: <class 'torch.Tensor'>\n",
      "Shape of intermediate_output: torch.Size([5, 13, 312])\n",
      "Before Part 2, shape of input: torch.Size([5, 13, 312])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore Part 2, shape of input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintermediate_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_part2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape from Part 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Step 1: Convert the output to logits\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mTinyBERTPart2.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[1;32m---> 38\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Process the input tensor through the layers\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(hidden_states)  \u001b[38;5;66;03m# Use the pooler after the encoder layers\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)  \u001b[38;5;66;03m# Classification layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:393\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    376\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`attn_implementation=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` when loading the model.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    384\u001b[0m         hidden_states,\n\u001b[0;32m    385\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         output_attentions,\n\u001b[0;32m    391\u001b[0m     )\n\u001b[1;32m--> 393\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[0;32m    395\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states))\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "class TinyBERTPart1(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart1, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.bert.encoder.layer[:2])  # Access encoder layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TinyBERTPart2(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart2, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.bert.encoder.layer[2:])  # Access remaining layers\n",
    "        self.pooler = original_model.bert.pooler  # Access the pooler\n",
    "        self.classifier = nn.Linear(original_model.config.hidden_size, 2)  # Adjust the output size as needed\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states)  # Process the input tensor through the layers\n",
    "        pooled_output = self.pooler(hidden_states)  # Use the pooler after the encoder layers\n",
    "        logits = self.classifier(pooled_output)  # Classification layer\n",
    "        return logits\n",
    "\n",
    "model_part1 = TinyBERTPart1(tinybert)\n",
    "model_part2 = TinyBERTPart2(tinybert)\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Type of intermediate_output: {type(intermediate_output)}\")\n",
    "print(f\"Shape of intermediate_output: {intermediate_output.shape}\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    print(f\"Before Part 2, shape of input: {intermediate_output.shape}\")\n",
    "    output = model_part2(intermediate_output)\n",
    "    print(f\"Output shape from Part 2: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd4d58d-9b22-406d-8170-cbafa1624df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.19.49', 56997)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Type of intermediate_output: <class 'torch.Tensor'>\n",
      "Shape of intermediate_output: torch.Size([5, 13, 312])\n",
      "Before Part 2, shape of input: torch.Size([5, 13, 312])\n",
      "Output shape from Part 2: torch.Size([5, 2])\n",
      "Accuracy: 60.00%\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "class TinyBERTPart1(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart1, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.bert.encoder.layer[:2])  # Access encoder layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)[0]  # Get the output tensor from each layer\n",
    "        return x\n",
    "\n",
    "class TinyBERTPart2(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart2, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.bert.encoder.layer[2:])  # Access remaining layers\n",
    "        self.pooler = original_model.bert.pooler  # Access the pooler\n",
    "        self.classifier = nn.Linear(original_model.config.hidden_size, 2)  # Adjust the output size as needed\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        for layer in self.encoder_layers:\n",
    "            output = layer(hidden_states)\n",
    "            hidden_states = output[0] if isinstance(output, tuple) else output  # Unpack if it's a tuple\n",
    "        pooled_output = self.pooler(hidden_states)  # Use the pooler after the encoder layers\n",
    "        logits = self.classifier(pooled_output)  # Classification layer\n",
    "        return logits\n",
    "\n",
    "model_part1 = TinyBERTPart1(tinybert)\n",
    "model_part2 = TinyBERTPart2(tinybert)\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Type of intermediate_output: {type(intermediate_output)}\")\n",
    "print(f\"Shape of intermediate_output: {intermediate_output.shape}\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    print(f\"Before Part 2, shape of input: {intermediate_output.shape}\")\n",
    "    output = model_part2(intermediate_output)\n",
    "    print(f\"Output shape from Part 2: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e11f93b-dfdf-48d2-bb9b-680fa4d64b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyBERTPart2(\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSdpaSelfAttention(\n",
       "          (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80448d86-ea38-488b-bda8-ae608b455b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.19.49', 57013)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Type of intermediate_output: <class 'torch.Tensor'>\n",
      "Shape of intermediate_output: torch.Size([5, 13, 312])\n",
      "Before Part 2, shape of input: torch.Size([5, 13, 312])\n",
      "Output shape from Part 2: torch.Size([5, 2])\n",
      "Accuracy: 40.00%\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "class TinyBERTPart2(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart2, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.bert.encoder.layer[2:])  # Access remaining layers\n",
    "        self.pooler = original_model.bert.pooler  # Access the pooler\n",
    "        self.classifier = nn.Linear(original_model.config.hidden_size, 2)  # Adjust the output size as needed\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        for layer in self.encoder_layers:\n",
    "            output = layer(hidden_states)\n",
    "            hidden_states = output[0] if isinstance(output, tuple) else output  # Unpack if it's a tuple\n",
    "        pooled_output = self.pooler(hidden_states)  # Use the pooler after the encoder layers\n",
    "        logits = self.classifier(pooled_output)  # Classification layer\n",
    "        return logits\n",
    "\n",
    "model_part2 = TinyBERTPart2(tinybert)\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Type of intermediate_output: {type(intermediate_output)}\")\n",
    "print(f\"Shape of intermediate_output: {intermediate_output.shape}\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    print(f\"Before Part 2, shape of input: {intermediate_output.shape}\")\n",
    "    output = model_part2(intermediate_output)\n",
    "    print(f\"Output shape from Part 2: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15bd91ed-b93a-4bf4-bc6e-a574e9faad6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyBERTPart1(\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSdpaSelfAttention(\n",
       "          (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57904159-6b7e-422c-8ad8-58dc4348b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.19.49', 57088)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Type of intermediate_output: <class 'torch.Tensor'>\n",
      "Shape of intermediate_output: torch.Size([5, 13, 312])\n",
      "Before Part 2, shape of input: torch.Size([5, 13, 312])\n",
      "Output shape from Part 2: torch.Size([5, 2])\n",
      "Accuracy: 60.00%\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "# Access remaining encoder layers for Part 2\n",
    "model_part2_layers = tinybert.bert.encoder.layer[2:]  # Access remaining layers\n",
    "modelpart2 = nn.ModuleList(model_part2_layers)  # Wrap layers in ModuleList\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Type of intermediate_output: {type(intermediate_output)}\")\n",
    "print(f\"Shape of intermediate_output: {intermediate_output.shape}\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    print(f\"Before Part 2, shape of input: {intermediate_output.shape}\")\n",
    "    \n",
    "    # Initialize hidden_states with intermediate_output\n",
    "    hidden_states = intermediate_output\n",
    "    \n",
    "    # Forward through each layer in Part 2\n",
    "    for layer in modelpart2:\n",
    "        hidden_states = layer(hidden_states)[0]  # Get the output from each layer\n",
    "\n",
    "    # Pooling and final classification (you may need to adapt this part)\n",
    "    pooled_output = tinybert.bert.pooler(hidden_states)\n",
    "    output = tinybert.classifier(pooled_output)\n",
    "\n",
    "    print(f\"Output shape from Part 2: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76fa6470-f1f3-4333-8fac-09ce9cd6abfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSdpaSelfAttention(\n",
       "        (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91ff1791-c76f-4ec6-a366-822f24c79b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSdpaSelfAttention(\n",
       "        (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part2_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3782f10f-5822-4c01-a483-d8d70665559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "174d7a59-7fdc-4036-90e8-1b9f9b79d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['text', 'label'], 'test': ['text', 'label'], 'unsupervised': ['text', 'label']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aac09d8-149f-40a6-8bb3-dbb522d86907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "057ccabf-765b-4235-b423-8152fa9262ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.', 'label': 0}\n",
      "\n",
      "Sample 2:\n",
      "{'text': \"Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\", 'label': 0}\n",
      "\n",
      "Sample 3:\n",
      "{'text': \"its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\", 'label': 0}\n",
      "\n",
      "Sample 4:\n",
      "{'text': \"STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\", 'label': 0}\n",
      "\n",
      "Sample 5:\n",
      "{'text': \"First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\", 'label': 0}\n",
      "\n",
      "Sample 6:\n",
      "{'text': \"I had high hopes for this one until they changed the name to 'The Shepherd : Border Patrol, the lamest movie name ever, what was wrong with just 'The Shepherd'. This is a by the numbers action flick that tips its hat at many classic Van Damme films. There is a nice bit of action in a bar which reminded me of hard target and universal soldier but directed with no intensity or flair which is a shame. There is one great line about 'being p*ss drunk and carrying a rabbit' and some OK action scenes let down by the cheapness of it all. A lot of the times the dialogue doesn't match the characters mouth and the stunt men fall down dead a split second before even being shot. The end fight is one of the better Van Damme fights except the Director tries to go a bit too John Woo and fails also introducing flashbacks which no one really cares about just gets in the way of the action which is the whole point of a van Damme film.<br /><br />Not good, not bad, just average generic action.\", 'label': 0}\n",
      "\n",
      "Sample 7:\n",
      "{'text': \"Isaac Florentine has made some of the best western Martial Arts action movies ever produced. In particular US Seals 2, Cold Harvest, Special Forces and Undisputed 2 are all action classics. You can tell Isaac has a real passion for the genre and his films are always eventful, creative and sharp affairs, with some of the best fight sequences an action fan could hope for. In particular he has found a muse with Scott Adkins, as talented an actor and action performer as you could hope for. This is borne out with Special Forces and Undisputed 2, but unfortunately The Shepherd just doesn't live up to their abilities.<br /><br />There is no doubt that JCVD looks better here fight-wise than he has done in years, especially in the fight he has (for pretty much no reason) in a prison cell, and in the final showdown with Scott, but look in his eyes. JCVD seems to be dead inside. There's nothing in his eyes at all. It's like he just doesn't care about anything throughout the whole film. And this is the leading man.<br /><br />There are other dodgy aspects to the film, script-wise and visually, but the main problem is that you are utterly unable to empathise with the hero of the film. A genuine shame as I know we all wanted this film to be as special as it genuinely could have been. There are some good bits, mostly the action scenes themselves. This film had a terrific director and action choreographer, and an awesome opponent for JCVD to face down. This could have been the one to bring the veteran action star back up to scratch in the balls-out action movie stakes.<br /><br />Sincerely a shame that this didn't happen.\", 'label': 0}\n",
      "\n",
      "Sample 8:\n",
      "{'text': \"It actually pains me to say it, but this movie was horrible on every level. The blame does not lie entirely with Van Damme as you can see he tried his best, but let's face it, he's almost fifty, how much more can you ask of him? I find it so hard to believe that the same people who put together Undisputed 2; arguably the best (western) martial arts movie in years, created this. Everything from the plot, to the dialog, to the editing, to the overall acting was just horribly put together and in many cases outright boring and nonsensical. Scott Adkins who's fight scenes seemed more like a demo reel, was also terribly underused and not even the main villain which is such a shame because 1) He is more than capable of playing that role and 2) The actual main villain was not only not intimidating at all but also quite annoying. Again, not blaming Van Damme. I will always be a fan, but avoid this one.\", 'label': 0}\n",
      "\n",
      "Sample 9:\n",
      "{'text': \"Technically I'am a Van Damme Fan, or I was. this movie is so bad that I hated myself for wasting those 90 minutes. Do not let the name Isaac Florentine (Undisputed II) fool you, I had big hopes for this one, depending on what I saw in (Undisputed II), man.. was I wrong ??! all action fans wanted a big comeback for the classic action hero, but i guess we wont be able to see that soon, as our hero keep coming with those (going -to-a-border - far-away-town-and -kill -the-bad-guys- than-comeback- home) movies I mean for God's sake, we are in 2008, and they insist on doing those disappointing movies on every level. Why ??!!! Do your self a favor, skip it.. seriously.\", 'label': 0}\n",
      "\n",
      "Sample 10:\n",
      "{'text': 'Honestly awful film, bad editing, awful lighting, dire dialog and scrappy screenplay.<br /><br />The lighting at is so bad there\\'s moments you can\\'t even see what\\'s going on, I even tried to playing with the contrast and brightness so I could see something but that didn\\'t help.<br /><br />They must have found the script in a bin, the character development is just as awful and while you hardly expect much from a Jean-Claude Van Damme film this one manages to hit an all time low. You can\\'t even laugh at the cheesy\\'ness.<br /><br />The directing and editing are also terrible, the whole film follows an extremely tired routine and fails at every turn as it bumbles through the plot that is so weak it\\'s just unreal.<br /><br />There\\'s not a lot else to say other than it\\'s really bad and nothing like Jean-Claude Van Damme\\'s earlier work which you could enjoy.<br /><br />Avoid like the plaque, frankly words fail me in condemning this \"film\".', 'label': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Access the test dataset\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Display the first 10 test samples\n",
    "for i in range(10):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(test_data[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c14c42f4-4158-40a7-bb0d-28f9cb117c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 52488)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Type of intermediate_output: <class 'torch.Tensor'>\n",
      "Shape of intermediate_output: torch.Size([5, 13, 128])\n",
      "Before Part 2, shape of input: torch.Size([5, 13, 128])\n",
      "Output shape from Part 2: torch.Size([5, 2])\n",
      "Accuracy: 80.00%\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Access remaining encoder layers for Part 2\n",
    "model_part2_layers = tinybert.bert.encoder.layer[2:]  # Access remaining layers\n",
    "modelpart2 = nn.ModuleList(model_part2_layers)  # Wrap layers in ModuleList\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Type of intermediate_output: {type(intermediate_output)}\")\n",
    "print(f\"Shape of intermediate_output: {intermediate_output.shape}\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    print(f\"Before Part 2, shape of input: {intermediate_output.shape}\")\n",
    "    \n",
    "    # Initialize hidden_states with intermediate_output\n",
    "    hidden_states = intermediate_output\n",
    "    \n",
    "    # Forward through each layer in Part 2\n",
    "    for layer in modelpart2:\n",
    "        hidden_states = layer(hidden_states)[0]  # Get the output from each layer\n",
    "\n",
    "    # Pooling and final classification (you may need to adapt this part)\n",
    "    pooled_output = tinybert.bert.pooler(hidden_states)\n",
    "    output = tinybert.classifier(pooled_output)\n",
    "\n",
    "    print(f\"Output shape from Part 2: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e925b531-87c6-4304-be16-f409a3654903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a79e96b0-637d-40c8-b11e-a262a519fbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8efd0cdf-e925-4f30-bbf0-4d3f7c207d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54c2411-1b57-4633-afc4-f5f74a113e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part2_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c0a37e9-4ce0-4b6d-8bdb-8bf9d6596e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4f5baa9-e2ea-42bf-b985-8dcaed5977a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tinybert.bert.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4718ca3a-447c-4a75-ad75-d123beaab7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tinybert.bert.encoder.layer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01b734b-5038-42fe-8758-c98c06db56ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "134e0b1b-ebcb-4fa2-bc3a-4dd6d72532b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 52785)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Accuracy: 80.00%\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Accept client connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Continue forward pass through the remaining layers\n",
    "with torch.no_grad():\n",
    "    # Pass through the second BertLayer (Layer 1)\n",
    "    hidden_states = tinybert.bert.encoder.layer[1](intermediate_output)[0]\n",
    "\n",
    "    # Pooling and classification layers\n",
    "    pooled_output = tinybert.bert.pooler(hidden_states)\n",
    "    output = tinybert.classifier(pooled_output)\n",
    "\n",
    "# Convert output to logits and get predictions\n",
    "predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "# Define true labels for accuracy calculation (example labels)\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb05c63c-8a3c-407c-9197-f3aa810f2394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybert.bert.encoder.layer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fde95ddd-9ef9-4be8-bd9d-82e309e734a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 52852)\n",
      "Intermediate output received from client.\n",
      "Accuracy: 80.00%\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 (remaining encoder layers, pooler, and classifier)\n",
    "modelpart2_layers = tinybert.bert.encoder.layer[1:]  # Remaining encoder layers\n",
    "modelpart2 = nn.ModuleList(modelpart2_layers)  # Wrap remaining layers in ModuleList\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "\n",
    "# Pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from intermediate output received\n",
    "\n",
    "    # Forward pass through each layer in modelpart2\n",
    "    for layer in modelpart2:\n",
    "        hidden_states = layer(hidden_states)[0]\n",
    "\n",
    "    # Pooling and final classification\n",
    "    pooled_output = tinybert.bert.pooler(hidden_states)\n",
    "    output = tinybert.classifier(pooled_output)\n",
    "\n",
    "# Get predictions and calculate accuracy (using example true labels)\n",
    "predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7febe3f-ae22-4e58-9beb-004946116887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],   # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "modelpart2_classifier = tinybert.classifier  # Classifier layer\n",
    "\n",
    "# Forward pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "\n",
    "    # Pass through each encoder layer in modelpart2\n",
    "    for layer in modelpart2_layers:\n",
    "        hidden_states = layer(hidden_states)[0]\n",
    "\n",
    "    # Pooling and final classification\n",
    "    pooled_output = modelpart2_layers[-1](hidden_states)  # Pooling layer output\n",
    "    output = modelpart2_classifier(pooled_output)  # Classification layer output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9b84d-64d1-4caa-b9b2-f76de1ace085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 (remaining encoder layers, pooler, and classifier)\n",
    "modelpart2_layers = tinybert.bert.encoder.layer[1:]  # Remaining encoder layers\n",
    "modelpart2 = nn.ModuleList(modelpart2_layers)  # Wrap remaining layers in ModuleList\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "\n",
    "# Pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from intermediate output received\n",
    "\n",
    "    # Forward pass through each layer in modelpart2\n",
    "    for layer in modelpart2:\n",
    "        hidden_states = layer(hidden_states)[0]\n",
    "\n",
    "    # Pooling and final classification\n",
    "    pooled_output = tinybert.bert.pooler(hidden_states)\n",
    "    output = tinybert.classifier(pooled_output)\n",
    "\n",
    "# Get predictions and calculate accuracy (using example true labels)\n",
    "predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13bde4d0-6f1f-481f-98bd-cbe1a389742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 53098)\n",
      "Intermediate output received from client.\n",
      "Intermediate output shape: torch.Size([5, 13, 128])\n",
      "Accuracy: 80.00%\n",
      "Connection closed.\n",
      "ModelPart2(\n",
      "  (layers): ModuleList(\n",
      "    (0): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSdpaSelfAttention(\n",
      "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "        \n",
    "        # Now, use the last layer's output for pooling\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Intermediate output shape: {intermediate_output.shape}\")  # Add this line for debugging\n",
    "\n",
    "# Forward pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "    output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "# Get predictions and calculate accuracy (using example true labels)\n",
    "predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n",
    "\n",
    "# Print the model structure\n",
    "print(modelpart2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c4073e6-3a11-4d6f-a19e-43f6bce91a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 1: BertPooler(\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "Classifier: Linear(in_features=128, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Print each layer in modelpart2\n",
    "for i, layer in enumerate(modelpart2.layers):\n",
    "    print(f\"Layer {i}: {layer}\")\n",
    "\n",
    "# Print the classifier\n",
    "print(f\"Classifier: {modelpart2.classifier}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25891660-37f3-4886-8bb2-126709fb2e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelPart2(\n",
       "  (layers): ModuleList(\n",
       "    (0): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSdpaSelfAttention(\n",
       "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "63c141c0-17b4-4bc6-b6e2-9c5b9ecfe37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 53247)\n",
      "Intermediate output received from client.\n",
      "Intermediate output shape: torch.Size([5, 13, 128])\n",
      "Accuracy: 80.00%\n",
      "Connection closed.\n",
      "ModelPart2(\n",
      "  (layers): ModuleList(\n",
      "    (0): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSdpaSelfAttention(\n",
      "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "        \n",
    "        # Now, use the last layer's output for pooling\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Intermediate output shape: {intermediate_output.shape}\")  # Add this line for debugging\n",
    "\n",
    "# Forward pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "    output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "# Get predictions and calculate accuracy (using example true labels)\n",
    "predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n",
    "\n",
    "# Print the model structure\n",
    "print(modelpart2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ac61fa8-307a-4ca3-a62c-022dccc222e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0665ef04-4f33-43e0-ab3c-edc7bd9367ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813175cc-a391-46c3-9b40-bb2fdbfca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0b3b69-9697-48af-ac50-93a8756bfaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6897bd-ec07-4786-90c4-59168c2b0df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\Project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 58308)\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "        \n",
    "        # Pooling layer\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Loop to receive multiple inputs and predict until client sends \"STOP\"\n",
    "while True:\n",
    "    # Receive the intermediate tensor from client\n",
    "    data = b''\n",
    "    while True:\n",
    "        packet = client_socket.recv(4096)\n",
    "        if not packet:\n",
    "            break\n",
    "        data += packet\n",
    "    \n",
    "    # Check if the client wants to stop\n",
    "    if data.decode(\"utf-8\") == \"STOP\":\n",
    "        print(\"Received stop signal from client.\")\n",
    "        break\n",
    "\n",
    "    # Deserialize the received data\n",
    "    intermediate_output = pickle.loads(data)\n",
    "\n",
    "    # Check the shape of intermediate_output (optional for debugging)\n",
    "    print(f\"Intermediate output shape: {intermediate_output.shape}\")\n",
    "\n",
    "    # Forward pass through modelpart2\n",
    "    with torch.no_grad():\n",
    "        hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "        output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "\n",
    "    # Send predictions back to client\n",
    "    client_socket.sendall(pickle.dumps(predictions.tolist()))\n",
    "    print(f\"Sent prediction: {predictions.tolist()}\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e22f092-fe4c-45a3-b672-b6723c6a447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\Project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 58334)\n",
      "Intermediate output received from client.\n",
      "Intermediate output shape: torch.Size([5, 13, 128])\n",
      "Accuracy: 80.00%\n",
      "Connection closed.\n",
      "ModelPart2(\n",
      "  (layers): ModuleList(\n",
      "    (0): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSdpaSelfAttention(\n",
      "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "        \n",
    "        # Now, use the last layer's output for pooling\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Intermediate output shape: {intermediate_output.shape}\")  # Add this line for debugging\n",
    "\n",
    "# Forward pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "    output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "# Get predictions and calculate accuracy (using example true labels)\n",
    "predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n",
    "\n",
    "# Print the model structure\n",
    "print(modelpart2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbc41cf-3371-431b-b5f3-d4209d5b2f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.16.77', 58863)\n",
      "Intermediate output received from client.\n",
      "Intermediate output shape: torch.Size([3, 7, 128])\n",
      "tensor([1, 1, 1])\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "       \n",
    "        # Now, use the last layer's output for pooling\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "       \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "#print(intermediate_output)\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Intermediate output shape: {intermediate_output.shape}\")  # Add this line for debugging\n",
    "\n",
    "# Forward pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "    output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "# Get predictions and calculate accuracy (using example true labels)\n",
    "predictions = torch.argmax(output ,dim=1)  # Predicted class indices\n",
    "#true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "print(predictions)\n",
    "#print(true_labels)\n",
    "#accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "#print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77eb76dc-77c2-45b5-b6cc-b935d3d60eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('192.168.76.90', 50199)\n",
      "Intermediate output received from client.\n",
      "Intermediate output shape: torch.Size([2, 7, 128])\n",
      "tensor([0, 0])\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "       \n",
    "        # Now, use the last layer's output for pooling\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "       \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "#print(intermediate_output)\n",
    "# Check the shape of intermediate_output\n",
    "print(f\"Intermediate output shape: {intermediate_output.shape}\")  # Add this line for debugging\n",
    "\n",
    "# Forward pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "    output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "# Get predictions and calculate accuracy (using example true labels)\n",
    "predictions = torch.argmax(output ,dim=1)  # Predicted class indices\n",
    "#true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "print(predictions)\n",
    "#print(true_labels)\n",
    "#accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "#print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42618b81-8a98-4c10-87f8-b806422e53d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelPart2(\n",
       "  (layers): ModuleList(\n",
       "    (0): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSdpaSelfAttention(\n",
       "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07b399b-6867-4b47-b9c0-16b47b15254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.82 MB\n"
     ]
    }
   ],
   "source": [
    "def model_size_in_MB(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_size_in_bytes = total_params * 4  # Assuming float32 (4 bytes)\n",
    "    total_size_in_MB = total_size_in_bytes / (1024 ** 2)  # Convert bytes to MB\n",
    "    return total_size_in_MB\n",
    "\n",
    "# Calculate the model size\n",
    "size_MB = model_size_in_MB(modelpart2)\n",
    "print(f\"Model size: {size_MB:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ca43b0-0693-401b-97ee-359d646c9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 16.73 MB\n"
     ]
    }
   ],
   "source": [
    "def model_size_in_MB(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_size_in_bytes = total_params * 4  # Assuming float32 (4 bytes)\n",
    "    total_size_in_MB = total_size_in_bytes / (1024 ** 2)  # Convert bytes to MB\n",
    "    return total_size_in_MB\n",
    "\n",
    "# Calculate the model size\n",
    "size_MB = model_size_in_MB(tinybert)\n",
    "print(f\"Model size: {size_MB:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac822728-152b-4f92-a243-a2ce19796be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('192.168.76.90', 50352)\n",
      "Intermediate output received from client.\n",
      "Intermediate output shape: torch.Size([1, 7, 128])\n",
      "Predictions: tensor([1])\n",
      "Predictions sent back to client.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "       \n",
    "        # Now, use the last layer's output for pooling\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "       \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "# Accept connection from client\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(f\"Intermediate output shape: {intermediate_output.shape}\")  # Debugging line\n",
    "\n",
    "# Forward pass through modelpart2\n",
    "with torch.no_grad():\n",
    "    hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "    output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "print(f\"Predictions: {predictions}\")\n",
    "\n",
    "# Serialize predictions\n",
    "serialized_predictions = pickle.dumps(predictions)\n",
    "\n",
    "# Send the predictions back to the client\n",
    "client_socket.sendall(serialized_predictions)\n",
    "print(\"Predictions sent back to client.\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3adc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('192.168.76.90', 50616)\n",
      "Intermediate output received from client.\n",
      "Intermediate output shape: torch.Size([1, 6, 128])\n",
      "tensor([0])\n",
      "Predictions sent back to client.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Server Side Code (modelpart2)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load TinyBERT model\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning_model/\")\n",
    "\n",
    "# Define modelpart2 to include remaining encoder layers, pooler, and classifier\n",
    "modelpart2_layers = nn.ModuleList([\n",
    "    *tinybert.bert.encoder.layer[1:],  # Remaining encoder layers after the first\n",
    "    tinybert.bert.pooler                # Pooling layer\n",
    "])\n",
    "\n",
    "# Combine modelpart2_layers and classifier into a single module\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self, layers, classifier):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Pass through each encoder layer in modelpart2\n",
    "        for layer in self.layers[:-1]:  # Exclude the pooling layer for now\n",
    "            hidden_states = layer(hidden_states)[0]\n",
    "       \n",
    "        # Now, use the last layer's output for pooling\n",
    "        pooled_output = self.layers[-1](hidden_states)  # Pooling layer output\n",
    "       \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_output)  # Classification layer output\n",
    "        return output\n",
    "\n",
    "# Instantiate modelpart2\n",
    "modelpart2 = ModelPart2(modelpart2_layers, tinybert.classifier)\n",
    "\n",
    "while True:\n",
    "    # Accept connection from client\n",
    "    client_socket, client_address = server_socket.accept()\n",
    "    print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "    try:\n",
    "        # Receive the intermediate tensor from client\n",
    "        data = b''\n",
    "        while True:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                break\n",
    "            data += packet\n",
    "\n",
    "        if data:\n",
    "            print(\"Intermediate output received from client.\")\n",
    "            # Deserialize the received data\n",
    "            intermediate_output = pickle.loads(data)\n",
    "            print(f\"Intermediate output shape: {intermediate_output.shape}\")\n",
    "\n",
    "            # Forward pass through modelpart2\n",
    "            with torch.no_grad():\n",
    "                hidden_states = intermediate_output  # Start from the intermediate output received\n",
    "                output = modelpart2(hidden_states)  # Get the final output\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(output, dim=1)  # Predicted class indices\n",
    "            print(predictions)\n",
    "            # Serialize and send predictions back to client\n",
    "            serialized_predictions = pickle.dumps(predictions)\n",
    "            client_socket.sendall(serialized_predictions)\n",
    "            print(\"Predictions sent back to client.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the socket connection for the client\n",
    "        client_socket.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71178ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
