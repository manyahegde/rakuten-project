{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a603648-4783-455e-90f0-d99a6a229fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Forward pass through the first two layers\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model_part1 \u001b[38;5;241m=\u001b[39m tinybert\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Get the initial hidden states from the embedding layer\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m tinybert\u001b[38;5;241m.\u001b[39membeddings(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AutoTokenizer, AutoModel \n",
    "import pickle\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./optimized_tinybert\")\n",
    "# Example input texts (batch)\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The service was fantastic!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"I would recommend this!\"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass through the first two layers\n",
    "model_part1 = tinybert.encoder.layer[:2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the initial hidden states from the embedding layer\n",
    "    hidden_states = tinybert.embeddings(inputs['input_ids'])\n",
    "    \n",
    "    # Pass through the first two layers\n",
    "    for layer in model_part1:\n",
    "        hidden_states = layer(hidden_states)[0]  # Get the output from each layer\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.59', 10300))  # Adjust host and port as needed\n",
    "client_socket.sendall(data)\n",
    "client_socket.close()\n",
    "print(\"Intermediate output sent to server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6462e56-be02-46bf-920c-29d7698e15b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load TinyBERT model and tokenizer from local files\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./optimized model\")\n",
    "\n",
    "# Example input texts (batch)\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The service was fantastic!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"I would recommend this!\"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass through the first two layers\n",
    "model_part1 = tinybert.bert.encoder.layer[:2]  # Access the encoder correctly\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the initial hidden states from the embedding layer\n",
    "    hidden_states = tinybert.bert.embeddings(inputs['input_ids'])\n",
    "    \n",
    "    # Pass through the first two layers\n",
    "    for layer in model_part1:\n",
    "        hidden_states = layer(hidden_states)[0]  # Get the output from each layer\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "\n",
    "try:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Adjust host and port as needed\n",
    "    client_socket.sendall(data)\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n",
    "\n",
    "print(\"Intermediate output sent to server.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7be6dec-b3d0-465c-a43f-2e2f1d94ea10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSdpaSelfAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "981d9368-dcd1-4449-aaca-0922020bd1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load TinyBERT model and tokenizer from local files\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "# Example input texts (batch)\n",
    "texts = [\n",
    "    \"I love this product!\",               # Expected label: 1\n",
    "    \"This is the worst experience I've ever had.\",  # Expected label: 0\n",
    "    \"The service was fantastic!\",         # Expected label: 1\n",
    "    \"Not worth the price.\",               # Expected label: 0\n",
    "    \"I would recommend this!\"             # Expected label: 1\n",
    "]\n",
    "\n",
    "# Example true labels (adjust these based on your actual labels)\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Replace with actual labels\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = tinybert(**inputs)  # Forward pass\n",
    "    logits = outputs.logits  # Get the logits\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get the predicted classes\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predictions == true_labels).float().mean().item()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3640d8-6982-4309-afe5-50ea6a6a36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load TinyBERT model and tokenizer from local files\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use the test set\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = torch.tensor(dataset['test']['label'])\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(test_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = tinybert(**inputs)  # Forward pass\n",
    "    logits = outputs.logits  # Get the logits\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get the predicted classes\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predictions == test_labels).float().mean().item()\n",
    "print(f\"Accuracy on IMDb test set: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcd059d-8cae-4168-8e25-e375c014a363",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hi\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hi' is not defined"
     ]
    }
   ],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a57a2e1-9861-4430-9d38-27a2dddb175b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ji'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ji\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1334dc66-56af-47b0-b4c2-5a745b6eefc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load TinyBERT model and tokenizer from local files\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./optimized_tinybert\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./optimized_tinybert\")\n",
    "\n",
    "# IMDb test reviews and their expected labels\n",
    "texts = [\n",
    "    \"I loved this movie! It was fantastic and well-made.\",      # Expected label: 1\n",
    "    \"This was the worst film I have ever seen. Very disappointing.\",  # Expected label: 0\n",
    "    \"An absolute masterpiece. The performances were outstanding.\",  # Expected label: 1\n",
    "    \"I wouldn't recommend this to anyone. It's a waste of time.\",  # Expected label: 0\n",
    "    \"Great storyline and excellent direction. A must-watch!\",      # Expected label: 1\n",
    "    \"Terrible acting and a boring plot. I regret watching it.\"     # Expected label: 0\n",
    "]\n",
    "\n",
    "# Corresponding true labels for the reviews\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1, 0])  # Replace with actual labels\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = tinybert(**inputs)  # Forward pass\n",
    "    logits = outputs.logits  # Get the logits\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get the predicted classes\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predictions == true_labels).float().mean().item()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7da230-be74-4748-b993-b600dda78ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1cbd5e-c534-4231-a78e-b2ac4c687baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98862a7c-6ab4-4499-8fbd-5a415df1304c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e249e7-2c6f-48b5-8c1a-c0ed15f5523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\Exam\\Desktop\\RAKUTEN MANYA\\rakuten-project-main\\segmentation\\optimized llm segmentation\\PSO+Pruning saved model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a569bf-179f-4927-a627-a1f5160f6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load TinyBERT model and tokenizer from local files\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Example input texts (batch)\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The service was fantastic!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"I would recommend this!\"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass through the first two layers\n",
    "model_part1 = tinybert.bert.encoder.layer[:2]  # Access the encoder correctly\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the initial hidden states from the embedding layer\n",
    "    hidden_states = tinybert.bert.embeddings(inputs['input_ids'])\n",
    "    \n",
    "    # Pass through the first two layers\n",
    "    for layer in model_part1:\n",
    "        hidden_states = layer(hidden_states)[0]  # Get the output from each layer\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "\n",
    "try:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Adjust host and port as needed\n",
    "    client_socket.sendall(data)\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n",
    "\n",
    "print(\"Intermediate output sent to server.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fad6e62-1cca-4fd3-98cb-701bdf797d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSdpaSelfAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f37a8b4-26b1-427e-a028-b660a4fc1c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Input texts\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The service was fantastic!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"I would recommend this!\"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass through embeddings and first encoder layers (Layers 0-5)\n",
    "with torch.no_grad():\n",
    "    hidden_states = tinybert.bert.embeddings(inputs['input_ids'])\n",
    "    for layer in tinybert.bert.encoder.layer[:6]:  # Only Layers 0-5\n",
    "        hidden_states = layer(hidden_states)[0]\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "\n",
    "try:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Replace SERVER_IP with server's IP\n",
    "    client_socket.sendall(data)\n",
    "    print(\"Intermediate output sent to server.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efe9068-d3e7-4153-8b37-c77f7f267166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Sample input texts\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The service was fantastic!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"I would recommend this!\"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass through embeddings and first BertLayer (Layer 0)\n",
    "with torch.no_grad():\n",
    "    # Embedding and first BertLayer\n",
    "    hidden_states = tinybert.bert.embeddings(inputs['input_ids'])\n",
    "    hidden_states = tinybert.bert.encoder.layer[0](hidden_states)[0]  # Only Layer 0\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "\n",
    "try:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Replace SERVER_IP with server's IP address\n",
    "    client_socket.sendall(data)\n",
    "    print(\"Intermediate output sent to server.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daf4c796-848c-4b42-bfb7-610b0085cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "# Client Side Code (modelpart1)\n",
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Example input texts\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The service was fantastic!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"I would recommend this!\"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Define modelpart1 (Embeddings and first encoder layer)\n",
    "modelpart1 = nn.ModuleList([\n",
    "    tinybert.bert.embeddings,\n",
    "    tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "])\n",
    "\n",
    "# Pass through modelpart1\n",
    "with torch.no_grad():\n",
    "    # Get embeddings\n",
    "    hidden_states = modelpart1[0](inputs['input_ids'])\n",
    "    # Pass through the first encoder layer (Layer 0)\n",
    "    hidden_states = modelpart1[1](hidden_states)[0]  # Only Layer 0\n",
    "\n",
    "# Serialize the output and send it to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "\n",
    "try:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Use the server's IP and port\n",
    "    client_socket.sendall(data)\n",
    "    print(\"Intermediate output sent to server.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52e19d0-a161-4bde-831b-f9074f97778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSdpaSelfAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50046016-4605-4980-9d96-d82b02c7c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 128)\n",
       "  (token_type_embeddings): Embedding(2, 128)\n",
       "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybert.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528f63b1-a2a5-4c0b-81c4-9cd283162ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybert.bert.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e35dda94-4a2c-4902-a699-7a31e72898f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in model_part1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a598296-c230-4f37-96e2-784432b7269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78965b0b-f4c0-45c8-abf2-2ee44c8bb3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31e55588-37a5-46a2-8f9e-085bae8108bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4218329e-cc62-49d1-be61-119a08fee148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 128)\n",
      "  (token_type_embeddings): Embedding(2, 128)\n",
      "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(modelpart1):\n",
    "    print(f\"Layer {idx}: {layer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08de1a-f839-43ad-ab12-87cd212cc810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output and true labels sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model/\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model/\")\n",
    "\n",
    "# Define modelpart1 (Embeddings and first encoder layer)\n",
    "modelpart1 = nn.ModuleList([\n",
    "    tinybert.bert.embeddings,\n",
    "    tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "])\n",
    "\n",
    "# Load IMDb dataset without torchtext\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "input_texts = [example[\"text\"] for example in dataset]\n",
    "true_labels = [1 if example[\"label\"] == 1 else 0 for example in dataset]\n",
    "\n",
    "# Tokenize input text and prepare for processing in batches\n",
    "inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Pass through modelpart1\n",
    "with torch.no_grad():\n",
    "    hidden_states = modelpart1[0](inputs['input_ids'])\n",
    "    hidden_states = modelpart1[1](hidden_states)[0]  # First encoder layer\n",
    "\n",
    "# Serialize intermediate output and true labels\n",
    "data = pickle.dumps({\n",
    "    \"intermediate_output\": hidden_states,\n",
    "    \"true_labels\": true_labels\n",
    "})\n",
    "\n",
    "# Send data to server\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.89', 10300))  # Replace with your server IP\n",
    "client_socket.sendall(data)\n",
    "print(\"Intermediate output and true labels sent to server.\")\n",
    "\n",
    "# Receive final results from the server\n",
    "result_data = b\"\"\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    result_data += packet\n",
    "\n",
    "# Deserialize received results\n",
    "result = pickle.loads(result_data)\n",
    "predictions = result[\"predictions\"]\n",
    "accuracy = result[\"accuracy\"]\n",
    "\n",
    "# Display results\n",
    "print(f\"Model Predictions: {predictions}\")\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa516af-51f8-4b40-ae88-99c87602b5c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tinybert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_size\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model_size \u001b[38;5;241m=\u001b[39m get_model_size(tinybert)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyBERT model size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tinybert' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.numel() * param.element_size()  # total bytes\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.numel() * buffer.element_size()  # total bytes\n",
    "\n",
    "    total_size = (param_size + buffer_size) / (1024**2)  # convert bytes to MB\n",
    "    return total_size\n",
    "\n",
    "# Example usage\n",
    "model_size = get_model_size(tinybert)\n",
    "print(f\"TinyBERT model size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab4b053-3dec-4bf2-a114-28aa416f1208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n",
      "Intermediate output sent to server.\n",
      "Error: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Initialize the client socket\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "try:\n",
    "    # Connect to the server\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Use the server's IP and port\n",
    "    print(\"Connected to the server.\")\n",
    "\n",
    "    while True:\n",
    "        # Example input texts\n",
    "        texts = [\n",
    "            \"I love this product!\",\n",
    "            \"This is the worst experience I've ever had.\",\n",
    "            \"The service was fantastic!\",\n",
    "            \"Not worth the price.\",\n",
    "            \"I would recommend this!\"\n",
    "        ]\n",
    "\n",
    "        # Tokenize the input texts\n",
    "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # Define modelpart1 (Embeddings and first encoder layer)\n",
    "        modelpart1 = nn.ModuleList([\n",
    "            tinybert.bert.embeddings,\n",
    "            tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "        ])\n",
    "\n",
    "        # Pass through modelpart1\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            hidden_states = modelpart1[0](inputs['input_ids'])\n",
    "            # Pass through the first encoder layer (Layer 0)\n",
    "            hidden_states = modelpart1[1](hidden_states)[0]  # Only Layer 0\n",
    "\n",
    "        # Serialize the output and send it to the server\n",
    "        data = pickle.dumps(hidden_states)\n",
    "        client_socket.sendall(data)\n",
    "        print(\"Intermediate output sent to server.\")\n",
    "\n",
    "        # Wait for the prediction result from the server\n",
    "        predictions_data = b''\n",
    "        while True:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                break\n",
    "            predictions_data += packet\n",
    "\n",
    "        # Deserialize the received predictions\n",
    "        predictions = pickle.loads(predictions_data)\n",
    "        print(f\"Received predictions from server: {predictions}\")\n",
    "\n",
    "        # Option to stop the process\n",
    "        user_input = input(\"Do you want to send more inputs? (yes/no): \").strip().lower()\n",
    "        if user_input != 'yes':\n",
    "            client_socket.sendall(pickle.dumps(\"STOP\"))  # Send stop signal\n",
    "            print(\"Stopping the client...\")\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb3229c-ca45-40a4-8234-b8f0e6ef793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence (or type 'done' to finish):  chethana is bad\n",
      "Enter a sentence (or type 'done' to finish):  movie is good\n",
      "Enter a sentence (or type 'done' to finish):  movie was bad\n",
      "Enter a sentence (or type 'done' to finish):  done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Get input from the user\n",
    "texts = []\n",
    "while True:\n",
    "    text = input(\"Enter a sentence (or type 'done' to finish): \")\n",
    "    if text.lower() == 'done':\n",
    "        break\n",
    "    texts.append(text)\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Define modelpart1 (Embeddings and first encoder layer)\n",
    "modelpart1 = nn.ModuleList([\n",
    "    tinybert.bert.embeddings,\n",
    "    tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "])\n",
    "\n",
    "# Pass through modelpart1\n",
    "with torch.no_grad():\n",
    "    # Get embeddings\n",
    "    hidden_states = modelpart1[0](inputs['input_ids'])\n",
    "    # Pass through the first encoder layer (Layer 0)\n",
    "    hidden_states = modelpart1[1](hidden_states)[0]  # Only Layer 0\n",
    "\n",
    "# Serialize the output and send it to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "\n",
    "try:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Use the server's IP and port\n",
    "    client_socket.sendall(data)\n",
    "    print(\"Intermediate output sent to server.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09401fd-2874-45e8-b84e-d18fdd390505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text to predict (or 'END' to stop):  movie was good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n",
      "Error: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Initialize the client socket\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "try:\n",
    "    # Connect to the server\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Use the server's IP and port\n",
    "    print(\"Connected to the server.\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Enter text to predict (or 'END' to stop): \")\n",
    "        if user_input.strip().upper() == 'END':\n",
    "            client_socket.sendall(pickle.dumps(\"STOP\"))  # Send stop signal\n",
    "            break\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(user_input, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # Define modelpart1 (Embeddings and first encoder layer)\n",
    "        modelpart1 = nn.ModuleList([\n",
    "            tinybert.bert.embeddings,\n",
    "            tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "        ])\n",
    "\n",
    "        # Pass through modelpart1\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            hidden_states = modelpart1[0](inputs['input_ids'])\n",
    "            # Pass through the first encoder layer (Layer 0)\n",
    "            hidden_states = modelpart1[1](hidden_states)[0]  # Only Layer 0\n",
    "\n",
    "        # Serialize the output and send it to the server\n",
    "        data = pickle.dumps(hidden_states)\n",
    "        client_socket.sendall(data)\n",
    "        print(\"Intermediate output sent to server.\")\n",
    "\n",
    "        # Wait for the prediction result from the server\n",
    "        predictions_data = b''\n",
    "        while True:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                break\n",
    "            predictions_data += packet\n",
    "\n",
    "        # Deserialize the received predictions\n",
    "        predictions = pickle.loads(predictions_data)\n",
    "        print(f\"Received predictions from server: {predictions}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b88085-228f-41ae-a9b8-88778c0d1a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence (or type 'done' to finish):  manya is good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n",
      "Error communicating with the server: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Define modelpart1 (Embeddings and first encoder layer)\n",
    "modelpart1 = nn.ModuleList([\n",
    "    tinybert.bert.embeddings,\n",
    "    tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "])\n",
    "\n",
    "while True:\n",
    "    # Get input from the user\n",
    "    text = input(\"Enter a sentence (or type 'done' to finish): \")\n",
    "    if text.lower() == 'done':\n",
    "        break\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer([text], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Pass through modelpart1\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        hidden_states = modelpart1[0](inputs['input_ids'])\n",
    "        # Pass through the first encoder layer (Layer 0)\n",
    "        hidden_states = modelpart1[1](hidden_states)[0]  # Only Layer 0\n",
    "\n",
    "    # Serialize the output\n",
    "    data = pickle.dumps(hidden_states)\n",
    "\n",
    "    try:\n",
    "        # Establish a connection to the server for each input\n",
    "        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        client_socket.connect(('172.16.19.89', 10300))  # Use the server's IP and port\n",
    "        client_socket.sendall(data)\n",
    "        print(\"Intermediate output sent to server.\")\n",
    "\n",
    "        # Receive the response from the server\n",
    "        result_data = b\"\"\n",
    "        while True:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                break\n",
    "            result_data += packet\n",
    "\n",
    "        # Deserialize and display the result from the server\n",
    "        result = pickle.loads(result_data)\n",
    "        print(f\"Server Response - Predictions: {result['predictions']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with the server: {e}\")\n",
    "    finally:\n",
    "        client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b01111-fea5-4226-9576-ced525bc761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence (or type 'done' to finish):  manya is good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Define modelpart1 (Embeddings and first encoder layer)\n",
    "modelpart1 = nn.ModuleList([\n",
    "    tinybert.bert.embeddings,\n",
    "    tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "])\n",
    "\n",
    "# Set up the socket connection to the server\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.89', 10300))  # Replace with your server IP and port\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get input from the user\n",
    "        text = input(\"Enter a sentence (or type 'done' to finish): \")\n",
    "        if text.lower() == 'done':\n",
    "            break  # Exit loop if user finishes input\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer([text], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # Pass through modelpart1\n",
    "        with torch.no_grad():\n",
    "            hidden_states = modelpart1[0](inputs['input_ids'])  # Embeddings\n",
    "            hidden_states = modelpart1[1](hidden_states)[0]     # First encoder layer (Layer 0)\n",
    "\n",
    "        # Serialize the output and send it to the server\n",
    "        data = pickle.dumps(hidden_states)\n",
    "        client_socket.sendall(data)\n",
    "        print(\"Intermediate output sent to server.\")\n",
    "        \n",
    "        # Wait for server response (predictions or any feedback)\n",
    "        result_data = b\"\"\n",
    "        while True:\n",
    "            packet = client_socket.recv(4096)\n",
    "            if not packet:\n",
    "                break\n",
    "            result_data += packet\n",
    "\n",
    "        # Deserialize received result\n",
    "        result = pickle.loads(result_data)\n",
    "        print(f\"Server Response: {result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during communication: {e}\")\n",
    "finally:\n",
    "    # Close the connection when done\n",
    "    client_socket.close()\n",
    "    print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd9ad9b-e891-4725-b7b3-f0a14d991bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence (or type 'done' to finish):  Movie is good\n",
      "Enter a sentence (or type 'done' to finish):  Movie was amazing\n",
      "Enter a sentence (or type 'done' to finish):  Movie was terrible\n",
      "Enter a sentence (or type 'done' to finish):  done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tinybert = BertForSequenceClassification.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./PSO+Pruning saved model\")\n",
    "\n",
    "# Get input from the user\n",
    "texts = []\n",
    "while True:\n",
    "    text = input(\"Enter a sentence (or type 'done' to finish): \")\n",
    "    if text.lower() == 'done':\n",
    "        break\n",
    "    texts.append(text)\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Define modelpart1 (Embeddings and first encoder layer)\n",
    "modelpart1 = nn.ModuleList([\n",
    "    tinybert.bert.embeddings,\n",
    "    tinybert.bert.encoder.layer[0]  # First encoder layer (Layer 0)\n",
    "])\n",
    "\n",
    "# Pass through modelpart1\n",
    "with torch.no_grad():\n",
    "    # Get embeddings\n",
    "    hidden_states = modelpart1[0](inputs['input_ids'])\n",
    "    # Pass through the first encoder layer (Layer 0)\n",
    "    hidden_states = modelpart1[1](hidden_states)[0]  # Only Layer 0\n",
    "\n",
    "# Serialize the output and send it to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "\n",
    "try:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('172.16.19.89', 10300))  # Use the server's IP and port\n",
    "    client_socket.sendall(data)\n",
    "    print(\"Intermediate output sent to server.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending data: {e}\")\n",
    "finally:\n",
    "    client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31c7b4-5818-436b-a40d-a4b31f4dd857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
