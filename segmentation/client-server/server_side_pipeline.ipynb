{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4779c31-01fc-4e88-8c3e-1d9035b9c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.19.49', 63432)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Model Part 2 forward pass completed. Output shape: torch.Size([1, 312])\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Load TinyBERT part 2\n",
    "tinybert = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "class TinyBERTPart2(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart2, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.encoder.layer[2:])\n",
    "        self.pooler = original_model.pooler\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)[0]  # First element of the output tuple\n",
    "        x = self.pooler(x)\n",
    "        return x\n",
    "\n",
    "model_part2 = TinyBERTPart2(tinybert)\n",
    "\n",
    "# Perform forward pass through part 2\n",
    "with torch.no_grad():\n",
    "    output = model_part2(intermediate_output)\n",
    "print(f\"Model Part 2 forward pass completed. Output shape: {output.shape}\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6b880-1be1-4265-8310-40f91451fcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening on 0.0.0.0:10300...\n",
      "Connected to ('172.16.19.49', 64415)\n",
      "Client: hi\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Server (you):  Hello Client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: careless you are\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Server (you):  Thank you\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# Create a socket object\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Define the host and port\n",
    "host = '0.0.0.0'  # This will allow connections from any network interface\n",
    "port = 10300       # Choose a port number that is not being used\n",
    "\n",
    "# Bind the socket to the host and port\n",
    "server_socket.bind((host, port))\n",
    "\n",
    "# Enable the server to listen for incoming connections (max 1 connection)\n",
    "server_socket.listen(1)\n",
    "print(f\"Server is listening on {host}:{port}...\")\n",
    "\n",
    "# Accept a connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to {client_address}\")\n",
    "\n",
    "# Communication loop\n",
    "while True:\n",
    "    # Receive data from the client (max 1024 bytes)\n",
    "    data = client_socket.recv(1024).decode()\n",
    "    if not data:\n",
    "        break\n",
    "    print(f\"Client: {data}\")\n",
    "\n",
    "    # Send a reply to the client\n",
    "    message = input(\"Server (you): \")\n",
    "    client_socket.send(message.encode())\n",
    "\n",
    "# Close the connection\n",
    "client_socket.close()\n",
    "server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a863812-4696-402e-8dcd-77e1b3fd7c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to calculate the total number of parameters in a model\n",
    "def get_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Create the split models\n",
    "class TinyBERTPart1(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart1, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.encoder.layer[:2])  # First 2 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)[0]  # First element of the output tuple\n",
    "        return x\n",
    "\n",
    "class TinyBERTPart2(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart2, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.encoder.layer[2:])  # Remaining layers\n",
    "        self.pooler = original_model.pooler\n",
    "        self.classifier = nn.Linear(original_model.config.hidden_size, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)[0]\n",
    "        x = self.pooler(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model_part1 = TinyBERTPart1(tinybert)\n",
    "model_part2 = TinyBERTPart2(tinybert)\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    output = model_part2(intermediate_output)\n",
    "\n",
    "print(f\"Model Part 2 forward pass completed. Output shape: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "# Replace this with your actual true labels for the evaluation\n",
    "true_labels = torch.tensor([1, 0, 1, 1, 0])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66f5074-12e2-491d-9dcf-f36f2baab464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Exam\\.conda\\envs\\Rakuten\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on 0.0.0.0:10300...\n",
      "Connected to client at ('172.16.19.49', 64244)\n",
      "Intermediate output received from client.\n",
      "Deserialized the received data.\n",
      "Model Part 2 forward pass completed. Output shape: torch.Size([5, 2])\n",
      "Accuracy: 60.00%\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Server setup\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = '0.0.0.0'\n",
    "port = 10300\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(1)\n",
    "print(f\"Server listening on {host}:{port}...\")\n",
    "\n",
    "# Load the original TinyBERT model\n",
    "tinybert = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "class TinyBERTPart1(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart1, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.encoder.layer[:2])  # First 2 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)[0]  # Process through each layer\n",
    "        return x\n",
    "\n",
    "class TinyBERTPart2(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart2, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(original_model.encoder.layer[2:])  # Remaining layers\n",
    "        self.pooler = original_model.pooler\n",
    "        self.classifier = nn.Linear(original_model.config.hidden_size, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)[0]  # Process through each layer\n",
    "        x = self.pooler(x)  # Apply the pooler\n",
    "        x = self.classifier(x)  # Classification layer\n",
    "        return x\n",
    "\n",
    "model_part1 = TinyBERTPart1(tinybert)\n",
    "model_part2 = TinyBERTPart2(tinybert)\n",
    "\n",
    "# Accept connection\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connected to client at {client_address}\")\n",
    "\n",
    "# Receive the intermediate tensor from client\n",
    "data = b''\n",
    "while True:\n",
    "    packet = client_socket.recv(4096)\n",
    "    if not packet:\n",
    "        break\n",
    "    data += packet\n",
    "\n",
    "print(\"Intermediate output received from client.\")\n",
    "\n",
    "# Deserialize the received data\n",
    "intermediate_output = pickle.loads(data)\n",
    "print(\"Deserialized the received data.\")\n",
    "\n",
    "# Perform forward pass through Part 2\n",
    "with torch.no_grad():\n",
    "    output = model_part2(intermediate_output)\n",
    "\n",
    "print(f\"Model Part 2 forward pass completed. Output shape: {output.shape}\")\n",
    "\n",
    "# Step 1: Convert the output to logits\n",
    "predictions = torch.argmax(output, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Step 2: For evaluation, assume you have a list of true labels\n",
    "true_labels = torch.tensor([1, 0, 1, 0, 1])  # Example true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n",
    "server_socket.close()\n",
    "print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6df467-140f-4929-9de3-beb6703881ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28f4a7a-3570-470b-8275-71b096928214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc89885-0f93-4428-a2db-6d24450e61ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyBERTPart2(\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSdpaSelfAttention(\n",
       "          (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf10cb-9ad0-4ef8-af8e-f1db8d71ea33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
