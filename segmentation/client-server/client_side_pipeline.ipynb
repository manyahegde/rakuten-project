{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d889a31-7580-414f-b27a-d0ffbdd4a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server at 172.16.19.59:10300\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "# Client setup\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_ip = '172.16.19.59'  # Replace with the server's (Laptop B's) IP address\n",
    "server_port = 10300\n",
    "client_socket.connect((server_ip, server_port))\n",
    "print(f\"Connected to server at {server_ip}:{server_port}\")\n",
    "\n",
    "# Load TinyBERT part 1\n",
    "from transformers import AutoModel\n",
    "tinybert = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "class TinyBERTPart1(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(TinyBERTPart1, self).__init__()\n",
    "        self.embeddings = original_model.embeddings\n",
    "        self.encoder_layers = nn.ModuleList(original_model.encoder.layer[:2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)[0]  # First element of the output tuple\n",
    "        return x\n",
    "\n",
    "model_part1 = TinyBERTPart1(tinybert).to('cpu')  # Keep it on CPU for simplicity in sending over network\n",
    "\n",
    "# Example input\n",
    "input_data = torch.randint(0, 30522, (1, 16))  # Simulated input for TinyBERT\n",
    "\n",
    "# Perform forward pass through part 1\n",
    "intermediate_output = model_part1(input_data)\n",
    "\n",
    "# Serialize the intermediate tensor to send over network\n",
    "data = pickle.dumps(intermediate_output)\n",
    "\n",
    "# Send the intermediate tensor to server\n",
    "client_socket.sendall(data)\n",
    "\n",
    "# Close the socket connection\n",
    "client_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65a9d9-c999-4ce8-b21f-6109198aab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server at 172.16.19.59:10300\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Client (you):  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server: Hello Client\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Client (you):  careless you are\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server: Thank you\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# Create a socket object\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Define the server's IP address and port number\n",
    "server_ip = '172.16.19.59'  # Replace with the server's IP address\n",
    "server_port = 10300              # Port must match the one used by the server\n",
    "\n",
    "# Connect to the server\n",
    "client_socket.connect((server_ip, server_port))\n",
    "print(f\"Connected to server at {server_ip}:{server_port}\")\n",
    "\n",
    "# Communication loop\n",
    "while True:\n",
    "    # Send a message to the server\n",
    "    message = input(\"Client (you): \")\n",
    "    client_socket.send(message.encode())\n",
    "\n",
    "    # Receive a reply from the server (max 1024 bytes)\n",
    "    data = client_socket.recv(1024).decode()\n",
    "    if not data:\n",
    "        break\n",
    "    print(f\"Server: {data}\")\n",
    "\n",
    "# Close the connection\n",
    "client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eae141b-027d-4e60-a523-7ac22f405fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Setup\n",
    "host = '172.16.19.59'  # Server address\n",
    "port = 10300\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Prepare input data\n",
    "text = \"This is an example sentence for sentiment analysis.\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)  # Forward pass\n",
    "    intermediate_output = output.last_hidden_state  # Access the last hidden state\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(intermediate_output)\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect((host, port))\n",
    "client_socket.sendall(data)\n",
    "client_socket.close()\n",
    "print(\"Intermediate output sent to server.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e3965a-70b5-4b10-8258-5b52a515b718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n",
      "Tokenizer and model loaded successfully.\n",
      "Tokenizing input text: 'This is an example sentence for sentiment analysis.'\n",
      "Input tokenized successfully.\n",
      "Performing forward pass through the model...\n",
      "Output shape: torch.Size([1, 11, 312])\n",
      "Intermediate output serialized.\n",
      "Summary of the intermediate output tensor:\n",
      "tensor([[[-0.1936,  0.2849,  0.2053,  ..., -0.1179,  0.1213,  0.1366],\n",
      "         [-0.0972,  0.3074,  0.0534,  ..., -0.5702,  0.5079,  0.0612],\n",
      "         [-0.1953,  0.4485, -0.0143,  ..., -0.8655,  0.1390,  0.5142],\n",
      "         ...,\n",
      "         [ 0.1187,  0.3189,  0.3829,  ...,  0.3394,  0.4038,  0.2010],\n",
      "         [-0.0102, -0.1025,  0.0466,  ..., -0.2004, -0.8425,  0.2412],\n",
      "         [-0.0360, -0.2039,  0.0244,  ..., -0.1843, -0.4815,  0.2304]]])\n",
      "Connecting to the server...\n",
      "Connected to the server.\n",
      "Sending intermediate output to server...\n",
      "Data sent successfully.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Setup\n",
    "host = '172.16.19.59'  # Server address\n",
    "port = 10300\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "print(\"Tokenizer and model loaded successfully.\")\n",
    "\n",
    "# Prepare input data\n",
    "text = \"This is an example sentence for sentiment analysis.\"\n",
    "print(f\"Tokenizing input text: '{text}'\")\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "print(\"Input tokenized successfully.\")\n",
    "\n",
    "# Forward pass through the model\n",
    "print(\"Performing forward pass through the model...\")\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)  # Forward pass\n",
    "    intermediate_output = output.last_hidden_state  # Access the last hidden state\n",
    "    print(f\"Output shape: {intermediate_output.shape}\")\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(intermediate_output)\n",
    "print(\"Intermediate output serialized.\")\n",
    "\n",
    "# Print summary of the tensor data\n",
    "print(\"Summary of the intermediate output tensor:\")\n",
    "print(intermediate_output)\n",
    "\n",
    "try:\n",
    "    print(\"Connecting to the server...\")\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect((host, port))\n",
    "    print(\"Connected to the server.\")\n",
    "\n",
    "    print(\"Sending intermediate output to server...\")\n",
    "    client_socket.sendall(data)\n",
    "    print(\"Data sent successfully.\")\n",
    "\n",
    "except ConnectionRefusedError:\n",
    "    print(f\"Connection refused. Ensure the server is running at {host}:{port}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    client_socket.close()\n",
    "    print(\"Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5477669c-2936-45b0-83d8-34e79569938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Part 1: 45.652771 MB\n",
      "Size of Part 2: 9.089081 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Assume you want to split the model in half (for pipeline parallelism)\n",
    "half = len(list(model.parameters())) // 2\n",
    "\n",
    "# Calculate size of each part\n",
    "part1_params = list(model.parameters())[:half]\n",
    "part2_params = list(model.parameters())[half:]\n",
    "\n",
    "def calculate_model_size(params):\n",
    "    return sum(param.numel() for param in params) * 4 / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "part1_size_mb = calculate_model_size(part1_params)\n",
    "part2_size_mb = calculate_model_size(part2_params)\n",
    "\n",
    "print(f\"Size of Part 1: {part1_size_mb:.6f} MB\")\n",
    "print(f\"Size of Part 2: {part2_size_mb:.6f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39004bdf-e683-4754-a965-d6b357431bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client is serializing the data and sending to server. it is deserializing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dfad62b-ddcd-4316-9b16-ec09f39a37e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate output sent to server.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "\n",
    "# Load TinyBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "tinybert = AutoModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Example input texts (batch)\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The service was fantastic!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"I would recommend this!\"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass through the first two layers\n",
    "model_part1 = tinybert.encoder.layer[:2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the initial hidden states from the embedding layer\n",
    "    hidden_states = tinybert.embeddings(inputs['input_ids'])\n",
    "    \n",
    "    # Pass through the first two layers\n",
    "    for layer in model_part1:\n",
    "        hidden_states = layer(hidden_states)[0]  # Get the output from each layer\n",
    "\n",
    "# Serialize and send the intermediate tensor to the server\n",
    "data = pickle.dumps(hidden_states)\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "client_socket.connect(('172.16.19.59', 10300))  # Adjust host and port as needed\n",
    "client_socket.sendall(data)\n",
    "client_socket.close()\n",
    "print(\"Intermediate output sent to server.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdb6ac3b-8262-4e2a-9c90-827ecde474ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSdpaSelfAttention(\n",
       "        (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5595d-aa24-4593-bc8a-55d371d8a9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
